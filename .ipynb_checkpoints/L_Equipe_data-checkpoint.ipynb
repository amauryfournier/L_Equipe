{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage données L'équipe\n",
    "\n",
    "### Amaury Fournier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Xiti  /!\\ sep=';' OK : \n",
    "#Clients 569 fichiers OK\n",
    "#Pages 570 fichiers OK\n",
    "#Sessions 566 fichiers OK\n",
    "\n",
    "\n",
    "#Ventes en ligne  /!\\ encoding utf-16 OK :\n",
    "#Commande 89 fichiers OK\n",
    "#Compte 90 fichiers OK\n",
    "#Souscription 90 fichiers OK\n",
    "\n",
    "#PROD42 : 262 fichiers, csv normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join,getsize,isdir\n",
    "import csv\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "#import msgpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fonctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#n'affiche pas les warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#affiche la totalité des colonnes du dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "#retourne la liste des csv du répertoire Y:\\L_Equipe\\tablesbrutes\\root\\namedir \n",
    "#root : {'Ventes en ligne','Xiti'}\n",
    "#namedir : {'Clients', 'Pages', 'Sessions', 'Souscription', 'Commandes', 'Compte', 'PROD42'}\n",
    "def makelistcsv(namedir, root):\n",
    "    path = r'Y:\\L_Equipe\\tablesbrutes' + \"\\\\\" + root +\"\\\\\"+ namedir \n",
    "    if root == \"Ventes en ligne\":\n",
    "        if namedir == 'PROD42' : #cas particulier, il faut aller chercher les différents csv qui ne sont pas tous au même endroit\n",
    "            onlyfiles = []\n",
    "            path = r'Y:\\L_Equipe\\tablesbrutes\\Ventes en ligne\\PROD_VL\\PROD'\n",
    "            onlyfiles.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                         if f.endswith('.csv') if \"SubscriptionsReport\" in f if getsize(join(path,f))>0])\n",
    "            for d in listdir(path) : \n",
    "                if d.startswith('20') and not d.startswith('20141209') and not d.startswith('20141212') and not d.startswith('20141217'):\n",
    "                    path = r'Y:\\L_Equipe\\tablesbrutes\\Ventes en ligne\\PROD_VL\\PROD'+\"\\\\\"+d\n",
    "                    onlyfiles.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                         if f.endswith('.csv') if 'SubscriptionsReport' in f if getsize(join(path,f))>0])\n",
    "                    \n",
    "        else : #cas Ventes en ligne, not PROD42\n",
    "            onlyfiles = [join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                         if f.endswith('.csv') if f.startswith('EQP') if getsize(join(path,f))>0]\n",
    "    else : #cas Xiti\n",
    "        onlyfiles = [join(path,f) for f in listdir(path) if isfile(join(path, f)) if f.endswith('.csv') if getsize(join(path,f))>0]\n",
    "    print \"liste des csv finie\" \n",
    "    return onlyfiles\n",
    "\n",
    "#construit le dataframe du fichier csv\n",
    "def makedataframe(csv):\n",
    "    if 'Ventes en ligne' in csv :\n",
    "        if 'PROD' in csv :\n",
    "            df = pd.read_csv(csv, encoding = 'utf-8')\n",
    "        else : #cas Ventes en ligne, not PROD\n",
    "            df = pd.read_csv(csv, encoding= 'utf-16')\n",
    "    else : #cas Xiti\n",
    "        df = pd.read_csv(csv, sep=';', encoding = 'utf-16')\n",
    "    return df\n",
    "\n",
    "#retourne la liste privée des doublons de la liste seq \n",
    "def makeunique(seq):\n",
    "    set = {}\n",
    "    map(set.__setitem__, seq, [])\n",
    "    return set.keys()\n",
    "\n",
    "#retourne le dataframe du fichier namedir du directory root\n",
    "#root : {'Ventes en ligne','Xiti'}\n",
    "#namedir : {'clients', 'pages', 'sessions', 'souscription', 'commandes', 'compte', 'PROD42'}\n",
    "#/!\\ attention à la RAM\n",
    "#une fois la fonction executée le dataframe est sauvegardé dans le fichier r\"W:\\L_equipe\\s_dataframe_general_root_directory.pkl\"\n",
    "#pensez à reset le kernel après une exécution de cette fonction pour libérer la RAM.\n",
    "def makegeneraldataframe(namedir, root):\n",
    "    if namedir == 'pages' : #cas pages : on ne charge qu'un certain nombre de fichiers pour ne pas exploser la RAM\n",
    "        liste_page = makelistcsv(namedir, root)\n",
    "        #liste = liste_page[-38:-8] #pour le dernier mois (juin 2015)\n",
    "        liste = liste_page[-69:-38] #pour mai 2015\n",
    "        #liste = liste_page[-99 : -69] #avril 2015\n",
    "        #liste = liste_page[-130  : -99] #mars 2015\n",
    "    elif namedir == 'sessions' : #cas sessions :\n",
    "        liste_session = makelistcsv(namedir, root)\n",
    "        liste = liste_session[-30:] #pour le dernier mois (juin 2015)\n",
    "        #liste = liste_session[-61:-30] #pour mai 2015\n",
    "        #liste = liste_session[-91:-61]# avril 2015\n",
    "        #liste = liste_sessions[-122:91] #mars 2015\n",
    "        #liste = liste_session[-122:] #mars-juin 2015\n",
    "    else :\n",
    "        liste = makelistcsv(namedir,root)\n",
    "    df_g = pd.concat((makedataframe(f) for f in liste))\n",
    "    #tentative de découpage du dataframe en deux parties:\n",
    "\n",
    "    \n",
    "    if root == \"Ventes en ligne\" :\n",
    "        #df_g.to_csv(r\"W:\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".csv\", index=False, encoding='utf-16')\n",
    "        #df_g.to_json(r\"W:\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".pkl\")\n",
    "        #msgpack.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".pkl\", \"wb\" ))\n",
    "        df_g.to_pickle(r\"C:\\Users\\Data Science 5\\Desktop\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".pkl\")\n",
    "        #pickle.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".txt\", \"wb\" ))\n",
    "        #json.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+directory+\".json\", \"wb\" ))\n",
    "    elif namedir == 'pages'  :\n",
    "        if (df_g.shape[0])%2 >0 :\n",
    "            part1 = df_g.shape[0]/2\n",
    "            part2 = df_g.shape[0]/2 +1\n",
    "        else : \n",
    "            part1 = df_g.shape[0]/2\n",
    "            part2 = df_g.shape[0]/2\n",
    "        df_g_p1 = df_g.head(part1)\n",
    "        df_g_p2 = df_g.tail(part2)\n",
    "        #return df_g_p1,df_g_p2\n",
    "        #df_g.to_json(u\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_\"+str(len(liste))+\".json\")\n",
    "        #msgpack.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_\"+str(len(liste))+\".pkl\", \"wb\" ))\n",
    "        #df_g.to_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_juin2015_partie1.pkl\")\n",
    "        \n",
    "        #df_g_p1.to_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_mai2015_partie1.pkl\")\n",
    "        df_g_p1.to_pickle(r\"C:\\Users\\Data Science 5\\Desktop\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_mai2015_partie1.pkl\")\n",
    "        \n",
    "        #df_g_p2.to_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_mai2015_partie2.pkl\")\n",
    "        df_g_p2.to_pickle(r\"C:\\Users\\Data Science 5\\Desktop\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_mai2015_partie2.pkl\")\n",
    "        \n",
    "        #df_g.to_csv(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_avril15.csv\",index=False,encoding='utf-16')\n",
    "        #pickle.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\".txt\", \"wb\" ))\n",
    "        #json.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+directory+\".json\", \"wb\" ))\n",
    "    else : \n",
    "        #msgpack.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\".pkl\", \"wb\" ))\n",
    "        #df_g.to_json(u\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\".json\")\n",
    "        df_g.to_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_juin2015.pkl\")\n",
    "        #df_g.to_csv(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\".csv\", index=False,encoding='utf-16')\n",
    "    print \"pickle pret\"\n",
    "\n",
    "#pour un dataframe donné retourne la liste privée de doublons du champ\n",
    "#dataframe : {\"df_g_client\",\"df_g_page\", \"df_g_session\", \"df_g_souscription\", \"df_g_commande\", \"df_g_compte\", \"df_g_prod\"}\n",
    "#champs : voir dictionnaire des données\n",
    "def makelist(dataframe, champ) : \n",
    "    l =[]\n",
    "    if dataframe in globals() :\n",
    "        df = eval(dataframe)\n",
    "        l.extend(df[champ].unique())\n",
    "    else : \n",
    "        print 'il faut créer le dataframe'\n",
    "    return l\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liste des csv finie\n"
     ]
    },
    {
     "ename": "CParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCParserError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1069d6f63ad5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#makegeneraldataframe(\"PROD42\", \"Ventes en ligne\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmakegeneraldataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sessions\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Xiti'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#df_g_client = makegeneraldataframe(\"clients\", \"Xiti\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#df_g_commande = makegeneraldataframe(\"commande\", \"Ventes en ligne\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-31f3d969bc69>\u001b[0m in \u001b[0;36mmakegeneraldataframe\u001b[1;34m(namedir, root)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mliste\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmakelistcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamedir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mdf_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmakedataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mliste\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;31m#tentative de découpage du dataframe en deux parties:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\tools\\merge.pyc\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    832\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 834\u001b[1;33m                        copy=copy)\n\u001b[0m\u001b[0;32m    835\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\tools\\merge.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[0mobjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m             \u001b[0mobjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-31f3d969bc69>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m((f,))\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mliste\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmakelistcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamedir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mdf_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmakedataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mliste\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;31m#tentative de découpage du dataframe en deux parties:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-31f3d969bc69>\u001b[0m in \u001b[0;36mmakedataframe\u001b[1;34m(csv)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'utf-16'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;31m#cas Xiti\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-16'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    527\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    745\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    748\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:4948)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._get_header (pandas\\parser.c:6493)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._tokenize_rows (pandas\\parser.c:8838)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.raise_parser_error (pandas\\parser.c:22649)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "#une seule exec nécessaire pour créer les pickles\n",
    "start_time = time.clock()\n",
    "#makegeneraldataframe(\"PROD42\", \"Ventes en ligne\")\n",
    "\n",
    "makegeneraldataframe(\"sessions\", 'Xiti')\n",
    "#df_g_client = makegeneraldataframe(\"clients\", \"Xiti\")\n",
    "#df_g_commande = makegeneraldataframe(\"commande\", \"Ventes en ligne\")\n",
    "#df_g_souscription = makegeneraldataframe(\"souscription\",  \"Ventes en ligne\")\n",
    "#df_g_prod42 = makegeneraldataframe(\"PROD42\",  \"Ventes en ligne\")\n",
    "#df_g_compte = makegeneraldataframe(\"compte\", 'Ventes en ligne')\n",
    "print time.clock() - start_time, \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loading pickle\n",
    "start_time = time.clock()\n",
    "#df_g_session = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_sessions_30.pkl\") \n",
    "#df_g_client = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_clients.pkl\")\n",
    "#df_g_page = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_pages_30juin15.pkl\")\n",
    "#df_g_commande = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_vel_commande.pkl\")\n",
    "df_g_commande = pd.read_csv(r\"W:\\L_equipe\\s_dataframe_general_vel_commande.csv\", encoding='utf-16')\n",
    "#df_g_compte = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_vel_compte.pkl\")\n",
    "#df_g_souscription = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_vel_souscription.pkl\")\n",
    "#df_g_prod = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_vel_PROD42.pkl\")\n",
    "\n",
    "#fichiers result de copy /!\\ totalement bugman les fichiers sont mal concaténés maybe gérables :\n",
    "#df_g_client = pd.read_csv(r\"W:\\L_equipe\\all_clients.csv\", sep=';')\n",
    "#df_g_page =  pd.read_csv(r\"W:\\L_equipe\\pages_avril2015_clean.csv\")\n",
    "#df_g_commande = pd.read_csv(r\"W:\\L_equipe\\all_commandes2.csv\", encoding = 'utf-16', error_bad_lines=False)\n",
    "#df_g_compte = pd.read_csv(r\"W:\\L_equipe\\all_comptes.csv\", encoding = 'utf-16')\n",
    "    \n",
    "#fichiers camille\n",
    "#df_g_page = makedataframe(u'Y:\\L_Equipe\\Projet2_072015\\Données\\Table SAS\\PAGEHISTO2.csv')\n",
    "#df_g_pages = pd.read_csv(u'Y:\\L_Equipe\\Projet2_072015\\Données\\Table SAS\\PAGEHISTO2.csv', sep=';')\n",
    "print time.clock() - start_time, \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creation du datarame pour apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_client_sub = set(l_liste_xiti[0]) & set(l_liste_vel[0])\n",
    "l_not_sub = set(l_liste_xiti[0]) - set(l_liste_vel[0])\n",
    "#random undersampling pour les clients non subs\n",
    "l_not_sub = random.sample(l_not_sub, 16554)\n",
    "\n",
    "dftrain = df_g_client.copy()\n",
    "\n",
    "#on rajoute une colone sub \n",
    "dftrain_sub = dftrain.loc[dftrain['id_client'].isin(l_client_sub)]\n",
    "dftrain_sub_sum = dftrain_sub.groupby(['id_client', 'id_site'], sort=False).sum()\n",
    "dftrain_sub['sub'] = 1\n",
    "\n",
    "\n",
    "dftrain_notsub= dftrain.loc[dftrain['id_client'].isin(l_not_sub)]\n",
    "dftrain_notsub_sum = dftrain_notsub.groupby(['id_client', 'id_site'], sort=False).sum()\n",
    "dftrain_notsub['sub'] = 0\n",
    "\n",
    "#dftrain_sub_sum = pd.DataFrame({'visites_sum' : dftrain_sub.groupby(['id_client', 'id_site'])['visites'].sum()}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftrain_tot = pd.concat([dftrain_sub, dftrain_notsub])\n",
    "dftrain_sum = pd.concat([dftrain_notsub_sum, dftrain_sub_sum])\n",
    "\n",
    "#dftrain_sum = dftrain_sum.set_index(['id_client'])\n",
    "#dftrain_sum.rename(columns={'visites': 'visites_sum', 'pages_vues': 'pages_vues_sum'}, inplace=True)\n",
    "dftraintot = dftrain_tot.join(dftrain_sum, on=['id_client', 'id_site'])\n",
    "\n",
    "\n",
    "# dataframe contenant les sub et non sub (équilibrés) avec l'id du site le nombre de visites et le nombre de pages vues\n",
    "dftraintot = dftraintot.drop_duplicates(subset=['id_client','id_site'])\n",
    "dftraintot.drop(['visites', 'pages_vues'], axis=1, inplace=True)\n",
    "\n",
    "#len(df[(df['A']>0) & (df['B']>0) & (df['C']>0)])\n",
    "print 'sub' , len(dftraintot[dftraintot['sub'] == 1])\n",
    "print 'non sub' , len(dftraintot[dftraintot['sub'] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code pour l'audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dftraintot.sort(columns='id_client' )\n",
    "#ajout de la colonne durée\n",
    "#ajout du temps passé par session\n",
    "#df_g_client.info\n",
    "#print df_g_commande['fileid'].value_counts(dropna=False)\n",
    "#print df_g_souscription.SubscriptionCreated.value_counts(dropna=False)\n",
    "#print df_g_commande.District.value_counts(dropna=False)\n",
    "#print ('-------------------------------------------------------')\n",
    "#print (df_g_pages.value_counts(dropna=False) / len(df_g_page))*100\n",
    "#print (df_g_page.id_site.value_counts(dropna=False) / len(df_g_page))*100\n",
    "#print df_g_client.id_site.value_counts(dropna=False)\n",
    "#print ('-------------------------------------------------------')\n",
    "#print (df_g_client.id_site.value_counts(dropna=False) / len(df_g_client))*100 \n",
    "\n",
    "#print (df_g_souscription.SubscriptionCreated[1028])\n",
    "#df_page = df_g_page.drop(df_g_page.index[df_g_page['id_site'] == 'id_site'])\n",
    "#df_page[df_page['url'] == 'url']\n",
    "#df_g_client.premiere_visite.count()\n",
    "df_page.to_csv(r\"W:\\L_equipe\\pages_avril2015_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_g_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# à garder et à mettre en automatique soon ! (penser aussi aux autres csv qui n'ont pas de format datetime propre)\n",
    "#df_g_prod['SubscriptionCreated'] = pd.to_datetime(df_g_prod['SubscriptionCreated'])\n",
    "#df_g_prod['SubscriptionLastUpdated'] = pd.to_datetime(df_g_prod['SubscriptionLastUpdated'])\n",
    "#df_g_prod['ServiceExpiry'] = pd.to_datetime(df_g_prod['ServiceExpiry'])\n",
    "df_g_pages['date'] = pd.to_datetime(df_g_pages['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code partie analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visiteurs qui ne sont pas sub en avril, mai 2015 et qui ont souscrit en juin 2015\n",
    "df_g_pages_avril.id_client\n",
    "df_g_pages_mai\n",
    "df_g_pages_juin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes à conserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#items à save :\n",
    "#df Xiti (fichier s_dataframe_xiti) : l_dataframe_xiti = [l_df_client, l_df_page, l_df_session] \n",
    "#listes Xiti (fichiers s_liste_xiti): l_liste_xiti = [l_client_unique, l_page_unique, l_session_unique]\n",
    "#df Vente en ligne (fichier s_dataframe_vel) : l_dataframe_vel = [l_df_sub, l_df_commande, l_df_compte] \n",
    "#listes Vente en ligne (fichiers s_liste_vel): l_liste_vel = [l_sub_unique, l_commande_unique] (compte à rajouter s'il le faut)\n",
    "\n",
    "#----------------------------1er jet test du pickleling OK-----------------------\n",
    "\n",
    "#l_dataframe = [l_df_sub, l_df_client, l_df_page, l_df_session]\n",
    "#l_liste = [l_sub_unique, l_client_unique, l_page_unique, l_session_unique]\n",
    "#\n",
    "#outfile_df = r'W:\\L_equipe\\s_dataframe.txt'\n",
    "#outfile_liste = r'W:\\L_equipe\\s_liste.txt'\n",
    "#\n",
    "#pickle.dump(l_dataframe,  open( r\"W:\\L_equipe\\s_dataframe.txt\", \"wb\" ))\n",
    "#pickle.dump(l_liste,  open( r\"W:\\L_equipe\\s_liste.txt\", \"wb\" ))\n",
    "\n",
    "#----------------------------pickle propre OK ----------------------------\n",
    "\n",
    "l_dataframe_xiti = [l_df_client, l_df_page, l_df_session]\n",
    "l_liste_xiti = [l_client_unique, l_page_unique, l_session_unique]\n",
    "\n",
    "outfile_df_xiti = r'W:\\L_equipe\\s_dataframe_xiti.txt'\n",
    "outfile_liste_xiti = r'W:\\L_equipe\\s_liste_xiti.txt'\n",
    "\n",
    "pickle.dump(l_dataframe_xiti,  open( r\"W:\\L_equipe\\s_dataframe_xiti.txt\", \"wb\" ))\n",
    "pickle.dump(l_liste_xiti,  open( r\"W:\\L_equipe\\s_liste_xiti.txt\", \"wb\" ))\n",
    "\n",
    "l_dataframe_vel = [l_df_sub, l_df_commande, l_df_compte] \n",
    "l_liste_vel = [l_sub_unique, l_commande_unique]\n",
    "\n",
    "outfile_df_vel = r'W:\\L_equipe\\s_dataframe_vel.txt'\n",
    "outfile_liste_vel = r'W:\\L_equipe\\s_liste_vel.txt'\n",
    "\n",
    "pickle.dump(l_dataframe_vel,  open( r\"W:\\L_equipe\\s_dataframe_vel.txt\", \"wb\" ))\n",
    "pickle.dump(l_liste_vel,  open( r\"W:\\L_equipe\\s_liste_vel.txt\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
