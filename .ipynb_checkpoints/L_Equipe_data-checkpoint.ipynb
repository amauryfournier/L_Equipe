{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage données L'équipe\n",
    "\n",
    "### Amaury Fournier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Xiti  /!\\ sep=';' OK : \n",
    "#Clients 569 fichiers OK\n",
    "#Pages 570 fichiers OK\n",
    "#Sessions 566 fichiers OK\n",
    "\n",
    "\n",
    "#Ventes en ligne  /!\\ encoding utf-16 OK :\n",
    "#Commande 89 fichiers OK\n",
    "#Compte 90 fichiers OK\n",
    "#Souscription 90 fichiers OK\n",
    "\n",
    "#PROD42 : 262 fichiers, csv normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join,getsize,isdir\n",
    "import csv\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "#import msgpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fonctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#n'affiche pas les warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#affiche la totalité des colonnes du dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "#retourne la liste des csv du répertoire Y:\\L_Equipe\\tablesbrutes\\root\\namedir \n",
    "#root : {'Ventes en ligne','Xiti'}\n",
    "#namedir : {'Clients', 'Pages', 'Sessions', 'Souscription', 'Commandes', 'Compte', 'PROD42'}\n",
    "def makelistcsv(namedir, root):\n",
    "    path = r'Y:\\L_Equipe\\tablesbrutes' + \"\\\\\" + root +\"\\\\\"+ namedir \n",
    "    if root == \"Ventes en ligne\":\n",
    "        if namedir == 'PROD42' : #cas particulier, il faut aller chercher les différents csv qui ne sont pas tous au même endroit\n",
    "            onlyfiles = []\n",
    "            path = r'Y:\\L_Equipe\\tablesbrutes\\Ventes en ligne\\PROD_VL\\PROD'\n",
    "            onlyfiles.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                         if f.endswith('.csv') if \"SubscriptionsReport\" in f if getsize(join(path,f))>0])\n",
    "            for d in listdir(path) : \n",
    "                if d.startswith('20') and not d.startswith('20141209') and not d.startswith('20141212') and not d.startswith('20141217'):\n",
    "                    path = r'Y:\\L_Equipe\\tablesbrutes\\Ventes en ligne\\PROD_VL\\PROD'+\"\\\\\"+d\n",
    "                    onlyfiles.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                         if f.endswith('.csv') if 'SubscriptionsReport' in f if getsize(join(path,f))>0])\n",
    "                    \n",
    "        else : #cas Ventes en ligne, not PROD42\n",
    "            onlyfiles = [join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                         if f.endswith('.csv') if f.startswith('EQP') if getsize(join(path,f))>0]\n",
    "    else : #cas Xiti\n",
    "        onlyfiles = [join(path,f) for f in listdir(path) if isfile(join(path, f)) if f.endswith('.csv') if getsize(join(path,f))>0]\n",
    "    print \"liste des csv finie\" \n",
    "    return onlyfiles\n",
    "\n",
    "#construit le dataframe du fichier csv\n",
    "def makedataframe(csv):\n",
    "    if 'Ventes en ligne' in csv :\n",
    "        if 'PROD' in csv :\n",
    "            df = pd.read_csv(csv, encoding = 'utf-8')\n",
    "        else : #cas Ventes en ligne, not PROD\n",
    "            df = pd.read_csv(csv, encoding= 'utf-16')\n",
    "    else : #cas Xiti\n",
    "        df = pd.read_csv(csv, sep=';', encoding = 'utf-8')\n",
    "    return df\n",
    "\n",
    "#retourne la liste privée des doublons de la liste seq \n",
    "def makeunique(seq):\n",
    "    set = {}\n",
    "    map(set.__setitem__, seq, [])\n",
    "    return set.keys()\n",
    "\n",
    "#retourne le dataframe du fichier namedir du directory root\n",
    "#root : {'Ventes en ligne','Xiti'}\n",
    "#namedir : {'clients', 'pages', 'sessions', 'souscription', 'commandes', 'compte', 'PROD42'}\n",
    "#/!\\ attention à la RAM\n",
    "#une fois la fonction executée le dataframe est sauvegardé dans le fichier r\"W:\\L_equipe\\s_dataframe_general_root_directory.pkl\"\n",
    "#pensez à reset le kernel après une exécution de cette fonction pour libérer la RAM.\n",
    "def makegeneraldataframe(namedir, root):\n",
    "    if namedir == 'pages' : #cas pages : on ne charge qu'un certain nombre de fichiers pour ne pas exploser la RAM\n",
    "        liste_page = makelistcsv(namedir, root)\n",
    "        #liste = liste_page[-38:-8] #pour le dernier mois (juin 2015)\n",
    "        liste = liste_page[-69:-38] #pour mai 2015\n",
    "        #liste = liste_page[-99 : -69] #avril 2015\n",
    "        #liste = liste_page[-130  : -99] #mars 2015\n",
    "    elif namedir == 'sessions' : #cas sessions :\n",
    "        liste_session = makelistcsv(namedir, root)\n",
    "        #liste = liste_session[-30:] #pour le dernier mois (juin 2015)\n",
    "        #liste = liste_session[-61:-30] #pour mai 2015\n",
    "        #liste = liste_session[-91:-61]# avril 2015\n",
    "        liste = liste_session[-122:-91] #mars 2015\n",
    "        #liste = liste_session[-122:] #mars-juin 2015\n",
    "    else :\n",
    "        liste = makelistcsv(namedir,root)\n",
    "    df_g = pd.concat((makedataframe(f) for f in liste))\n",
    "    #tentative de découpage du dataframe en deux parties:\n",
    "\n",
    "    \n",
    "    if root == \"Ventes en ligne\" :\n",
    "        #df_g.to_csv(r\"W:\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".csv\", index=False, encoding='utf-16')\n",
    "        #df_g.to_json(r\"W:\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".pkl\")\n",
    "        #msgpack.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".pkl\", \"wb\" ))\n",
    "        df_g.to_pickle(r\"C:\\Users\\Data Science 5\\Desktop\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".pkl\")\n",
    "        #pickle.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".txt\", \"wb\" ))\n",
    "        #json.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+directory+\".json\", \"wb\" ))\n",
    "    elif namedir == 'pages'  :\n",
    "        if (df_g.shape[0])%2 >0 :\n",
    "            part1 = df_g.shape[0]/2\n",
    "            part2 = df_g.shape[0]/2 +1\n",
    "        else : \n",
    "            part1 = df_g.shape[0]/2\n",
    "            part2 = df_g.shape[0]/2\n",
    "        df_g_p1 = df_g.head(part1)\n",
    "        df_g_p2 = df_g.tail(part2)\n",
    "        #return df_g_p1,df_g_p2\n",
    "        #df_g.to_json(u\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_\"+str(len(liste))+\".json\")\n",
    "        #msgpack.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_\"+str(len(liste))+\".pkl\", \"wb\" ))\n",
    "        #df_g.to_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_juin2015_partie1.pkl\")\n",
    "        \n",
    "        #df_g_p1.to_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_mai2015_partie1.pkl\")\n",
    "        df_g_p1.to_pickle(r\"C:\\Users\\Data Science 5\\Desktop\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_mai2015_partie1.pkl\")\n",
    "        \n",
    "        #df_g_p2.to_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_mai2015_partie2.pkl\")\n",
    "        df_g_p2.to_pickle(r\"C:\\Users\\Data Science 5\\Desktop\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_mai2015_partie2.pkl\")\n",
    "        \n",
    "        #df_g.to_csv(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_avril15.csv\",index=False,encoding='utf-16')\n",
    "        #pickle.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\".txt\", \"wb\" ))\n",
    "        #json.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+directory+\".json\", \"wb\" ))\n",
    "    else : \n",
    "        #msgpack.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\".pkl\", \"wb\" ))\n",
    "        #df_g.to_json(u\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\".json\")\n",
    "        df_g.to_pickle(r\"W:\\L_equipe\\dataframes\\s_dataframe_general_xiti_\"+namedir+\"_mars2015.pkl\")\n",
    "        #df_g.to_csv(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\".csv\", index=False,encoding='utf-16')\n",
    "    print \"pickle pret\"\n",
    "\n",
    "#pour un dataframe donné retourne la liste privée de doublons du champ\n",
    "#dataframe : {\"df_g_client\",\"df_g_page\", \"df_g_session\", \"df_g_souscription\", \"df_g_commande\", \"df_g_compte\", \"df_g_prod\"}\n",
    "#champs : voir dictionnaire des données\n",
    "def makelist(dataframe, champ) : \n",
    "    l =[]\n",
    "    if dataframe in globals() :\n",
    "        df = eval(dataframe)\n",
    "        l.extend(df[champ].unique())\n",
    "    else : \n",
    "        print 'il faut créer le dataframe'\n",
    "    return l\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liste des csv finie\n",
      "pickle pret\n",
      "496.15388611 seconds\n"
     ]
    }
   ],
   "source": [
    "#une seule exec nécessaire pour créer les pickles\n",
    "start_time = time.clock()\n",
    "#makegeneraldataframe(\"PROD42\", \"Ventes en ligne\")\n",
    "\n",
    "makegeneraldataframe(\"sessions\", 'Xiti')\n",
    "#df_g_client = makegeneraldataframe(\"clients\", \"Xiti\")\n",
    "#df_g_commande = makegeneraldataframe(\"commande\", \"Ventes en ligne\")\n",
    "#df_g_souscription = makegeneraldataframe(\"souscription\",  \"Ventes en ligne\")\n",
    "#df_g_prod42 = makegeneraldataframe(\"PROD42\",  \"Ventes en ligne\")\n",
    "#df_g_compte = makegeneraldataframe(\"compte\", 'Ventes en ligne')\n",
    "print time.clock() - start_time, \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loading pickle\n",
    "start_time = time.clock()\n",
    "#df_g_session = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_sessions_30.pkl\") \n",
    "#df_g_client = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_clients.pkl\")\n",
    "#df_g_page = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_pages_30juin15.pkl\")\n",
    "#df_g_commande = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_vel_commande.pkl\")\n",
    "df_g_commande = pd.read_csv(r\"W:\\L_equipe\\s_dataframe_general_vel_commande.csv\", encoding='utf-16')\n",
    "#df_g_compte = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_vel_compte.pkl\")\n",
    "#df_g_souscription = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_vel_souscription.pkl\")\n",
    "#df_g_prod = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_vel_PROD42.pkl\")\n",
    "\n",
    "#fichiers result de copy /!\\ totalement bugman les fichiers sont mal concaténés maybe gérables :\n",
    "#df_g_client = pd.read_csv(r\"W:\\L_equipe\\all_clients.csv\", sep=';')\n",
    "#df_g_page =  pd.read_csv(r\"W:\\L_equipe\\pages_avril2015_clean.csv\")\n",
    "#df_g_commande = pd.read_csv(r\"W:\\L_equipe\\all_commandes2.csv\", encoding = 'utf-16', error_bad_lines=False)\n",
    "#df_g_compte = pd.read_csv(r\"W:\\L_equipe\\all_comptes.csv\", encoding = 'utf-16')\n",
    "    \n",
    "#fichiers camille\n",
    "#df_g_page = makedataframe(u'Y:\\L_Equipe\\Projet2_072015\\Données\\Table SAS\\PAGEHISTO2.csv')\n",
    "#df_g_pages = pd.read_csv(u'Y:\\L_Equipe\\Projet2_072015\\Données\\Table SAS\\PAGEHISTO2.csv', sep=';')\n",
    "print time.clock() - start_time, \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creation du datarame pour apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_client_sub = set(l_liste_xiti[0]) & set(l_liste_vel[0])\n",
    "l_not_sub = set(l_liste_xiti[0]) - set(l_liste_vel[0])\n",
    "#random undersampling pour les clients non subs\n",
    "l_not_sub = random.sample(l_not_sub, 16554)\n",
    "\n",
    "dftrain = df_g_client.copy()\n",
    "\n",
    "#on rajoute une colone sub \n",
    "dftrain_sub = dftrain.loc[dftrain['id_client'].isin(l_client_sub)]\n",
    "dftrain_sub_sum = dftrain_sub.groupby(['id_client', 'id_site'], sort=False).sum()\n",
    "dftrain_sub['sub'] = 1\n",
    "\n",
    "\n",
    "dftrain_notsub= dftrain.loc[dftrain['id_client'].isin(l_not_sub)]\n",
    "dftrain_notsub_sum = dftrain_notsub.groupby(['id_client', 'id_site'], sort=False).sum()\n",
    "dftrain_notsub['sub'] = 0\n",
    "\n",
    "#dftrain_sub_sum = pd.DataFrame({'visites_sum' : dftrain_sub.groupby(['id_client', 'id_site'])['visites'].sum()}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dftrain_tot = pd.concat([dftrain_sub, dftrain_notsub])\n",
    "dftrain_sum = pd.concat([dftrain_notsub_sum, dftrain_sub_sum])\n",
    "\n",
    "#dftrain_sum = dftrain_sum.set_index(['id_client'])\n",
    "#dftrain_sum.rename(columns={'visites': 'visites_sum', 'pages_vues': 'pages_vues_sum'}, inplace=True)\n",
    "dftraintot = dftrain_tot.join(dftrain_sum, on=['id_client', 'id_site'])\n",
    "\n",
    "\n",
    "# dataframe contenant les sub et non sub (équilibrés) avec l'id du site le nombre de visites et le nombre de pages vues\n",
    "dftraintot = dftraintot.drop_duplicates(subset=['id_client','id_site'])\n",
    "dftraintot.drop(['visites', 'pages_vues'], axis=1, inplace=True)\n",
    "\n",
    "#len(df[(df['A']>0) & (df['B']>0) & (df['C']>0)])\n",
    "print 'sub' , len(dftraintot[dftraintot['sub'] == 1])\n",
    "print 'non sub' , len(dftraintot[dftraintot['sub'] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code pour l'audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dftraintot.sort(columns='id_client' )\n",
    "#ajout de la colonne durée\n",
    "#ajout du temps passé par session\n",
    "#df_g_client.info\n",
    "#print df_g_commande['fileid'].value_counts(dropna=False)\n",
    "#print df_g_souscription.SubscriptionCreated.value_counts(dropna=False)\n",
    "#print df_g_commande.District.value_counts(dropna=False)\n",
    "#print ('-------------------------------------------------------')\n",
    "#print (df_g_pages.value_counts(dropna=False) / len(df_g_page))*100\n",
    "#print (df_g_page.id_site.value_counts(dropna=False) / len(df_g_page))*100\n",
    "#print df_g_client.id_site.value_counts(dropna=False)\n",
    "#print ('-------------------------------------------------------')\n",
    "#print (df_g_client.id_site.value_counts(dropna=False) / len(df_g_client))*100 \n",
    "\n",
    "#print (df_g_souscription.SubscriptionCreated[1028])\n",
    "#df_page = df_g_page.drop(df_g_page.index[df_g_page['id_site'] == 'id_site'])\n",
    "#df_page[df_page['url'] == 'url']\n",
    "#df_g_client.premiere_visite.count()\n",
    "df_page.to_csv(r\"W:\\L_equipe\\pages_avril2015_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_g_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# à garder et à mettre en automatique soon ! (penser aussi aux autres csv qui n'ont pas de format datetime propre)\n",
    "#df_g_prod['SubscriptionCreated'] = pd.to_datetime(df_g_prod['SubscriptionCreated'])\n",
    "#df_g_prod['SubscriptionLastUpdated'] = pd.to_datetime(df_g_prod['SubscriptionLastUpdated'])\n",
    "#df_g_prod['ServiceExpiry'] = pd.to_datetime(df_g_prod['ServiceExpiry'])\n",
    "df_g_pages['date'] = pd.to_datetime(df_g_pages['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code partie analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visiteurs qui ne sont pas sub en avril, mai 2015 et qui ont souscrit en juin 2015\n",
    "df_g_pages_avril.id_client\n",
    "df_g_pages_mai\n",
    "df_g_pages_juin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes à conserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#items à save :\n",
    "#df Xiti (fichier s_dataframe_xiti) : l_dataframe_xiti = [l_df_client, l_df_page, l_df_session] \n",
    "#listes Xiti (fichiers s_liste_xiti): l_liste_xiti = [l_client_unique, l_page_unique, l_session_unique]\n",
    "#df Vente en ligne (fichier s_dataframe_vel) : l_dataframe_vel = [l_df_sub, l_df_commande, l_df_compte] \n",
    "#listes Vente en ligne (fichiers s_liste_vel): l_liste_vel = [l_sub_unique, l_commande_unique] (compte à rajouter s'il le faut)\n",
    "\n",
    "#----------------------------1er jet test du pickleling OK-----------------------\n",
    "\n",
    "#l_dataframe = [l_df_sub, l_df_client, l_df_page, l_df_session]\n",
    "#l_liste = [l_sub_unique, l_client_unique, l_page_unique, l_session_unique]\n",
    "#\n",
    "#outfile_df = r'W:\\L_equipe\\s_dataframe.txt'\n",
    "#outfile_liste = r'W:\\L_equipe\\s_liste.txt'\n",
    "#\n",
    "#pickle.dump(l_dataframe,  open( r\"W:\\L_equipe\\s_dataframe.txt\", \"wb\" ))\n",
    "#pickle.dump(l_liste,  open( r\"W:\\L_equipe\\s_liste.txt\", \"wb\" ))\n",
    "\n",
    "#----------------------------pickle propre OK ----------------------------\n",
    "\n",
    "l_dataframe_xiti = [l_df_client, l_df_page, l_df_session]\n",
    "l_liste_xiti = [l_client_unique, l_page_unique, l_session_unique]\n",
    "\n",
    "outfile_df_xiti = r'W:\\L_equipe\\s_dataframe_xiti.txt'\n",
    "outfile_liste_xiti = r'W:\\L_equipe\\s_liste_xiti.txt'\n",
    "\n",
    "pickle.dump(l_dataframe_xiti,  open( r\"W:\\L_equipe\\s_dataframe_xiti.txt\", \"wb\" ))\n",
    "pickle.dump(l_liste_xiti,  open( r\"W:\\L_equipe\\s_liste_xiti.txt\", \"wb\" ))\n",
    "\n",
    "l_dataframe_vel = [l_df_sub, l_df_commande, l_df_compte] \n",
    "l_liste_vel = [l_sub_unique, l_commande_unique]\n",
    "\n",
    "outfile_df_vel = r'W:\\L_equipe\\s_dataframe_vel.txt'\n",
    "outfile_liste_vel = r'W:\\L_equipe\\s_liste_vel.txt'\n",
    "\n",
    "pickle.dump(l_dataframe_vel,  open( r\"W:\\L_equipe\\s_dataframe_vel.txt\", \"wb\" ))\n",
    "pickle.dump(l_liste_vel,  open( r\"W:\\L_equipe\\s_liste_vel.txt\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
