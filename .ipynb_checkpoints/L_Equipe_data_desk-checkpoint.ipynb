{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage données L'équipe\n",
    "\n",
    "### Amaury Fournier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Xiti  /!\\ sep=';' OK : \n",
    "#Clients 569 fichiers OK\n",
    "#Pages 570 fichiers OK\n",
    "#Sessions 566 fichiers OK\n",
    "\n",
    "\n",
    "#Ventes en ligne  /!\\ encoding utf-16 OK :\n",
    "#Commande 89 fichiers OK\n",
    "#Compte 90 fichiers OK\n",
    "#Souscription 90 fichiers OK\n",
    "\n",
    "#PROD42 : 262 fichiers, csv normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join,getsize,isdir\n",
    "import csv\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fonctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#n'affiche pas les warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#affiche la totalité des colonnes du dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#retourne la liste des csv du répertoire Y:\\L_Equipe\\tablesbrutes\\root\\namedir \n",
    "#root : {'Ventes en ligne','Xiti'}\n",
    "#namedir : {'Clients', 'Pages', 'Sessions', 'Souscription', 'Commandes', 'Compte', 'PROD42'}\n",
    "def makelistcsv(namedir, root):\n",
    "    path = r'Y:\\L_Equipe\\tablesbrutes' + \"\\\\\" + root +\"\\\\\"+ namedir\n",
    "    if (root == 'churn') : \n",
    "        path = r\"V:\\L_Equipe\\Projet_01042016\\tables_brutes\"+\"\\\\\"+namedir \n",
    "    if root == \"Ventes en ligne\":\n",
    "        if namedir == 'PROD42' : #cas particulier, il faut aller chercher les différents csv qui ne sont pas tous au même endroit\n",
    "            onlyfiles = []\n",
    "            path = r'Y:\\L_Equipe\\tablesbrutes\\Ventes en ligne\\PROD_VL\\PROD'\n",
    "            onlyfiles.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                         if f.endswith('.csv') if \"SubscriptionsReport\" in f if getsize(join(path,f))>0])\n",
    "            for d in listdir(path) : \n",
    "                if d.startswith('20') and not d.startswith('20141209') and not d.startswith('20141212') and not d.startswith('20141217'):\n",
    "                    path = r'Y:\\L_Equipe\\tablesbrutes\\Ventes en ligne\\PROD_VL\\PROD'+\"\\\\\"+d\n",
    "                    onlyfiles.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                         if f.endswith('.csv') if 'SubscriptionsReport' in f if getsize(join(path,f))>0])\n",
    "                    \n",
    "        else : #cas Ventes en ligne, not PROD42\n",
    "            onlyfiles = [join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                         if f.endswith('.csv') if f.startswith('EQP') if getsize(join(path,f))>0]\n",
    "    else : #cas Xiti\n",
    "        onlyfiles = [join(path,f) for f in listdir(path) if isfile(join(path, f)) if f.endswith('.csv') if getsize(join(path,f))>0]\n",
    "    print \"liste des csv finie\"\n",
    "    return onlyfiles\n",
    "\n",
    "#construit le dataframe du fichier csv\n",
    "def makedataframe(csv, root):\n",
    "    if root == 'churn' :\n",
    "        df = pd.read_csv(csv, sep =\";\", encoding='utf-8')\n",
    "    elif root =='Ventes en ligne' :\n",
    "        if 'PROD' in csv :\n",
    "            df = pd.read_csv(csv, encoding = 'utf-8')\n",
    "        else : #cas Ventes en ligne, not PROD\n",
    "            df = pd.read_csv(csv, encoding= 'utf-16')\n",
    "    else : #cas Xiti\n",
    "        df = pd.read_csv(csv, sep=';', encoding = 'utf-16')\n",
    "    return df\n",
    "\n",
    "#retourne la liste privée des doublons de la liste seq \n",
    "def makeunique(seq):\n",
    "    set = {}\n",
    "    map(set.__setitem__, seq, [])\n",
    "    return set.keys()\n",
    "\n",
    "#retourne le dataframe du fichier namedir du directory root\n",
    "#root : {'Ventes en ligne','Xiti'}\n",
    "#namedir : {'clients', 'pages', 'sessions', 'souscription', 'commandes', 'compte', 'PROD42'}\n",
    "#/!\\ attention à la RAM\n",
    "#une fois la fonction executée le dataframe est sauvegardé dans le fichier r\"W:\\L_equipe\\s_dataframe_general_root_directory.pkl\"\n",
    "#pensez à reset le kernel après une exécution de cette fonction pour libérer la RAM.\n",
    "def makegeneraldataframe(namedir, root):\n",
    "    if namedir == 'pages' : #cas pages : on ne charge qu'un certain nombre de fichiers pour ne pas exploser la RAM\n",
    "        liste_page = makelistcsv(namedir, root)\n",
    "        #liste = liste_page[-38:-8] #pour le dernier mois (juin 2015)\n",
    "        liste = liste_page[-69:-38] #pour mai 2015\n",
    "        #liste = liste_page[-99 : -69] #avril 2015\n",
    "        #liste = liste_page[-130  : -99] #mars 2015\n",
    "    elif namedir == 'sessions' : #cas sessions :\n",
    "        liste_session = makelistcsv(namedir, root)\n",
    "        #liste = liste_session[-30:] #pour le dernier mois (juin 2015)\n",
    "        #liste = liste_session[-61:-30] #pour mai 2015\n",
    "        liste = liste_session[-91:-61]# avril 2015\n",
    "    else :\n",
    "        liste = makelistcsv(namedir,root)\n",
    "    df_g = pd.concat((makedataframe(f, root) for f in liste))\n",
    "    #tentative de découpage du dataframe en deux parties:\n",
    "\n",
    "    \n",
    "    if root == \"Ventes en ligne\" :\n",
    "        #df_g.to_csv(r\"W:\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".csv\", index=False, encoding='utf-16')\n",
    "        #df_g.to_json(r\"W:\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".pkl\")\n",
    "        #msgpack.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".pkl\", \"wb\" ))\n",
    "        df_g.to_pickle(r\"C:\\Users\\Data Science 5\\Desktop\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".pkl\")\n",
    "        #pickle.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_vel_\"+namedir+\".txt\", \"wb\" ))\n",
    "        #json.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+directory+\".json\", \"wb\" ))\n",
    "    elif namedir == 'pages'  :\n",
    "        if (df_g.shape[0])%2 >0 :\n",
    "            part1 = df_g.shape[0]/2\n",
    "            part2 = df_g.shape[0]/2 +1\n",
    "        else : \n",
    "            part1 = df_g.shape[0]/2\n",
    "            part2 = df_g.shape[0]/2\n",
    "        df_g_p1 = df_g.head(part1)\n",
    "        df_g_p2 = df_g.tail(part2)\n",
    "        #return df_g_p1,df_g_p2\n",
    "        #df_g.to_json(u\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_\"+str(len(liste))+\".json\")\n",
    "        #msgpack.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_\"+str(len(liste))+\".pkl\", \"wb\" ))\n",
    "        #df_g.to_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_juin2015_partie1.pkl\")\n",
    "        \n",
    "        #df_g_p1.to_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_mai2015_partie1.pkl\")\n",
    "        df_g_p1.to_pickle(r\"C:\\Users\\Data Science 5\\Desktop\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_mai2015_partie1.pkl\")\n",
    "        \n",
    "        #df_g_p2.to_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_mai2015_partie2.pkl\")\n",
    "        df_g_p2.to_pickle(r\"C:\\Users\\Data Science 5\\Desktop\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_mai2015_partie2.pkl\")\n",
    "        \n",
    "        #df_g.to_csv(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_avril15.csv\",index=False,encoding='utf-16')\n",
    "        #pickle.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\".txt\", \"wb\" ))\n",
    "        #json.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+directory+\".json\", \"wb\" ))\n",
    "    else : \n",
    "        #msgpack.dump(df_g, open( r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\".pkl\", \"wb\" ))\n",
    "        #df_g.to_json(u\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\".json\")\n",
    "        df_g.to_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\"_mai2015.pkl\")\n",
    "        #df_g.to_csv(r\"W:\\L_equipe\\s_dataframe_general_xiti_\"+namedir+\".csv\", index=False,encoding='utf-16')\n",
    "    print \"pickle pret\"\n",
    "\n",
    "#pour un dataframe donné retourne la liste privée de doublons du champ\n",
    "#dataframe : {\"df_g_client\",\"df_g_page\", \"df_g_session\", \"df_g_souscription\", \"df_g_commande\", \"df_g_compte\", \"df_g_prod\"}\n",
    "#champs : voir dictionnaire des données\n",
    "def makelist(dataframe, champ) : \n",
    "    l =[]\n",
    "    if dataframe in globals() :\n",
    "        df = eval(dataframe)\n",
    "        l.extend(df[champ].unique())\n",
    "    else : \n",
    "        print 'il faut créer le dataframe'\n",
    "    return l\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#une seule exec nécessaire pour créer les pickles\n",
    "start_time = time.clock()\n",
    "makegeneraldataframe(\"PROD42\", \"Ventes en ligne\")\n",
    "\n",
    "#df_g_session = makegeneraldataframe(\"sessions\", 'Xiti')\n",
    "#df_g_client = makegeneraldataframe(\"clients\", \"Xiti\")\n",
    "#df_g_commande = makegeneraldataframe(\"commande\", \"Ventes en ligne\")\n",
    "#df_g_souscription = makegeneraldataframe(\"souscription\",  \"Ventes en ligne\")\n",
    "#df_g_prod42 = makegeneraldataframe(\"PROD42\",  \"Ventes en ligne\")\n",
    "#df_g_compte = makegeneraldataframe(\"compte\", 'Ventes en ligne')\n",
    "print time.clock() - start_time, \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Churn (création des csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liste des csv finie\n"
     ]
    }
   ],
   "source": [
    "l = makelistcsv(\"Clients\",\"churn\")\n",
    "df_g = pd.concat((makedataframe(f) for f in l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_client =df_g.id_client.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_g.to_csv(r\"V:\\L_Equipe\\Projet_01042016\\tables_brutes\"+\"\\\\\"+\"clients_2015_1.csv\", encoding='utf-8', index_label=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liste des csv finie\n"
     ]
    }
   ],
   "source": [
    "l = makelistcsv(\"Commandes\",\"churn\")\n",
    "df_com = pd.concat((makedataframe(f) for f in l))\n",
    "df_com = df_com.loc[df_com.clientuserid.isin(l_client)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_com.to_csv(r\"V:\\L_Equipe\\Projet_01042016\\tables_brutes\\commandes_2015.csv\", encoding='utf-8', index_label=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liste des csv finie\n"
     ]
    }
   ],
   "source": [
    "l = makelistcsv(\"Comptes\",\"churn\")\n",
    "df_com = pd.concat((makedataframe(f) for f in l))\n",
    "df_com = df_com.loc[df_com.clientuserid.isin(l_client)]\n",
    "df_com.to_csv(r\"V:\\L_Equipe\\Projet_01042016\\tables_brutes\\comptes_2015.csv\", encoding='utf-8', index_label=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liste des csv finie\n"
     ]
    }
   ],
   "source": [
    "l = makelistcsv(\"Milibris\",\"churn\")\n",
    "df_com = pd.concat((makedataframe(f) for f in l))\n",
    "#df_com = df_com.loc[df_com.clientuserid.isin(l_client)]\n",
    "df_com.to_csv(r\"V:\\L_Equipe\\Projet_01042016\\tables_brutes\\milibris_2015.csv\", encoding='utf-8', index_label=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#premium\n",
    "#df_sub = pd.read_csv(r\"V:\\L_Equipe\\Projet_01042016\\tables_brutes\\souscriptions_2015.csv\", encoding='utf-8')\n",
    "l_sub = df_sub.clientuserid.unique()\n",
    "df_g = df_g.loc[df_g.id_client.isin(l_sub)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_g.to_csv(r\"V:\\L_Equipe\\Projet_01042016\\tables_brutes\\clients_premium_2015.csv\", encoding='utf-8', index_label=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liste des csv finie\n"
     ]
    }
   ],
   "source": [
    "l = makelistcsv(\"Comptes\",\"churn\")\n",
    "df_com = pd.concat((makedataframe(f) for f in l))\n",
    "df_com = df_com.loc[df_com.clientuserid.isin(l_sub)]\n",
    "df_com.to_csv(r\"V:\\L_Equipe\\Projet_01042016\\tables_brutes\\comptes_premium_2015.csv\", encoding='utf-8', index_label=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes à conserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#items à save :\n",
    "#df Xiti (fichier s_dataframe_xiti) : l_dataframe_xiti = [l_df_client, l_df_page, l_df_session] \n",
    "#listes Xiti (fichiers s_liste_xiti): l_liste_xiti = [l_client_unique, l_page_unique, l_session_unique]\n",
    "#df Vente en ligne (fichier s_dataframe_vel) : l_dataframe_vel = [l_df_sub, l_df_commande, l_df_compte] \n",
    "#listes Vente en ligne (fichiers s_liste_vel): l_liste_vel = [l_sub_unique, l_commande_unique] (compte à rajouter s'il le faut)\n",
    "\n",
    "#----------------------------1er jet test du pickleling OK-----------------------\n",
    "\n",
    "#l_dataframe = [l_df_sub, l_df_client, l_df_page, l_df_session]\n",
    "#l_liste = [l_sub_unique, l_client_unique, l_page_unique, l_session_unique]\n",
    "#\n",
    "#outfile_df = r'W:\\L_equipe\\s_dataframe.txt'\n",
    "#outfile_liste = r'W:\\L_equipe\\s_liste.txt'\n",
    "#\n",
    "#pickle.dump(l_dataframe,  open( r\"W:\\L_equipe\\s_dataframe.txt\", \"wb\" ))\n",
    "#pickle.dump(l_liste,  open( r\"W:\\L_equipe\\s_liste.txt\", \"wb\" ))\n",
    "\n",
    "#----------------------------pickle propre OK ----------------------------\n",
    "\n",
    "l_dataframe_xiti = [l_df_client, l_df_page, l_df_session]\n",
    "l_liste_xiti = [l_client_unique, l_page_unique, l_session_unique]\n",
    "\n",
    "outfile_df_xiti = r'W:\\L_equipe\\s_dataframe_xiti.txt'\n",
    "outfile_liste_xiti = r'W:\\L_equipe\\s_liste_xiti.txt'\n",
    "\n",
    "pickle.dump(l_dataframe_xiti,  open( r\"W:\\L_equipe\\s_dataframe_xiti.txt\", \"wb\" ))\n",
    "pickle.dump(l_liste_xiti,  open( r\"W:\\L_equipe\\s_liste_xiti.txt\", \"wb\" ))\n",
    "\n",
    "l_dataframe_vel = [l_df_sub, l_df_commande, l_df_compte] \n",
    "l_liste_vel = [l_sub_unique, l_commande_unique]\n",
    "\n",
    "outfile_df_vel = r'W:\\L_equipe\\s_dataframe_vel.txt'\n",
    "outfile_liste_vel = r'W:\\L_equipe\\s_liste_vel.txt'\n",
    "\n",
    "pickle.dump(l_dataframe_vel,  open( r\"W:\\L_equipe\\s_dataframe_vel.txt\", \"wb\" ))\n",
    "pickle.dump(l_liste_vel,  open( r\"W:\\L_equipe\\s_liste_vel.txt\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
