{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join,getsize,isdir\n",
    "import csv\n",
    "import numpy\n",
    "import cPickle as pickle\n",
    "import json\n",
    "from random import shuffle\n",
    "import warnings\n",
    "import time\n",
    "#import msgpack\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "import math\n",
    "import pylab\n",
    "import pandas_profiling\n",
    "import unicodedata\n",
    "import dateutil.relativedelta\n",
    "\n",
    "#n'affiche pas les warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#affiche la totalité des colonnes du dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "#path = \"C:\\Users\\Data Science 5\\Desktop\\L_equipe\"\n",
    "path = \"W:\\L_equipe\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chargement des dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_client = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_clients.pkl\")\n",
    "#df_prod42 = pd.read_pickle(path+\"\\s_dataframe_general_vel_PROD42.pkl\")\n",
    "#df_client_clean = pd.read_pickle(path+\"\\s_dataframe_client_clean_final.pkl\")\n",
    "df_client = pd.read_pickle(r\"W:\\L_equipe\\df_client_true_site.pkl\")\n",
    "#dataframe = pd.read_pickle(r\"C:\\Users\\Data Science 5\\Desktop\\L_equipe\\df_client_true_site.pkl\")\n",
    "\n",
    "#df_compte = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_vel_compte.pkl\")\n",
    "#df_commande = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_vel_commande.pkl\")\n",
    "#df_prod42_clean = pd.read_pickle(path+\"\\s_dataframe_prod42_clean.pkl\")\n",
    "#df_compte_clean = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_compte_clean.pkl\")\n",
    "#df_commande_clean = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_commande_clean.pkl\")\n",
    "#df_pages_juin_p1 = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_pages_juin2015_partie1.pkl\")\n",
    "#df_pages_juin_p1 = pd.read_pickle(\"s_dataframe_pages_juin_partie1_clean.pkl\")\n",
    "#df_session_juin = pd.read_pickle(path+\"\\s_dataframe_general_xiti_sessions_juin2015.pkl\")\n",
    "#df_session_juin_clean = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_session_juin2015_clean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#on ne garde que les sites de l'équipe et les client qui sont dans la base Client\n",
    "#----------------------------------Sites de l'equipe ----------------------------------\n",
    "df_client = pd.read_pickle(r\"W:\\L_equipe\\df_client_true_site.pkl\")\n",
    "\n",
    "def nettoyage_site(dataframe) :\n",
    "    dataframe = dataframe.loc[dataframe['id_site'].isin([492987,496306,496307,496838,502193,509043,539121,548647])]\n",
    "    liste_client = dataframe['id_client'].tolist()\n",
    "    return dataframe, liste_client \n",
    "\n",
    "df_client, liste_client = nettoyage_site(df_client)\n",
    "#save\n",
    "#df_client.to_pickle(r\"C:\\Users\\Data Science 5\\Desktop\\L_equipe\\df_client_true_site.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#--------------------nettoyage nouveaux abonnés--------------------------\n",
    "path = \"W:\\L_Equipe\"\n",
    "df_prod42 = pd.read_pickle(path+\"\\dataframes\\s_dataframe_general_vel_PROD42.pkl\")\n",
    "\n",
    "def nettoyage_sub(dataframe, liste_client) :\n",
    "    #selection des abonnés de l'equipe\n",
    "    dataframe = dataframe.loc[dataframe['ClientUserId'].isin(liste_client)]\n",
    "    #formatage datetime \n",
    "    dataframe['SubscriptionCreated'] = pd.to_datetime(dataframe['SubscriptionCreated'], format='%d/%m/%Y %H:%M:%S')\n",
    "    dataframe['SubscriptionLastUpdated'] = pd.to_datetime(dataframe['SubscriptionLastUpdated'], format='%d/%m/%Y %H:%M:%S')\n",
    "    dataframe['ServiceExpiry'] = pd.to_datetime(dataframe['ServiceExpiry'], format='%d/%m/%Y %H:%M:%S')\n",
    "    \n",
    "    #on ne garde que les abonnés de l'année 2015\n",
    "    dataframe['anciennete_souscripteur'] = dataframe.groupby('ClientUserId').SubscriptionCreated.transform('min')\n",
    "    dataframe = dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,1,1)) & (dataframe.anciennete_souscripteur < date(2015,7,1)) ]\n",
    "    #de plus on ne prends pas en compte les mises a jour de juillet 2015\n",
    "    dataframe = dataframe.loc[(dataframe.SubscriptionLastUpdated > date(2015,1,1)) & (dataframe.SubscriptionLastUpdated < date(2015,7,1))]\n",
    "    \n",
    "    #correction fautes de frapes et autres modifs\n",
    "    dataframe.loc[(dataframe.ServiceTitle == 'SFR_Full_Extra_Suplementaire'), 'ServiceTitle']= 'SFR_Full_Extra_Supplementaire'\n",
    "    dataframe.loc[(dataframe.ServiceTitle == 'Fete_des_peres_2015_v2'), 'ServiceTitle']= 'Fete_des_peres_2015'\n",
    "    dataframe.loc[(dataframe.ServiceTitle == 'Running_heroes_1M_gratuit'), 'ServiceTitle']= 'Running_Heroes_1M_gratuit'\n",
    "    dataframe.loc[(dataframe.ServiceTitle == 'TEST_ROLAND_GARROS_'), 'ServiceTitle']= 'RG2015_1€_15jrs_puis_11.99€'\n",
    "    dataframe.loc[(dataframe.ServiceTitle == 'TEMPLATE_ROLAND_GARROS'), 'ServiceTitle']= 'RG2015_1€_15jrs_puis_11.99€'\n",
    "    dataframe.loc[(dataframe.ServiceTitle == 'CocaCola_Total_1mois_essai '), 'ServiceTitle']= 'CocaCola_Total_1mois_essai'\n",
    "    \n",
    "    #fusion du nettoyage des souscriptions avec les abonnés\n",
    "    df_sous = pd.read_excel(path+'\\souscription_detail.xlsx')\n",
    "    df_sous.drop(['ServiceTitle'], axis=1, inplace=True)\n",
    "    dataframe = pd.merge(dataframe, df_sous, how='inner', on='ServiceID')\n",
    "    \n",
    "    #group by\n",
    "    #dataframe['somme_prix'] = dataframe.groupby(['ClientUserId', 'SubscriptionId'])['ExplicitPrice'].transform('sum')\n",
    "    dataframe['SubscriptionCreated'] = dataframe.groupby(['ClientUserId','SubscriptionId'])[\"SubscriptionCreated\"].transform('min')\n",
    "    dataframe['SubscriptionLastUpdated'] = dataframe.groupby(['ClientUserId', 'SubscriptionId'])[\"SubscriptionLastUpdated\"].transform('max')\n",
    "    dataframe['ServiceExpiry'] = dataframe.groupby(['ClientUserId', 'SubscriptionId'])['ServiceExpiry'].transform('max')\n",
    "    \n",
    "    #feature generation\n",
    "    dataframe = dataframe.loc[((dataframe.ServiceGroupTitle == 'Abo EQP') | (dataframe.ServiceGroupTitle == \"Abo EQP via Tiers\"))]\n",
    "    dataframe.loc[((dataframe['ServiceGroupTitle'] == 'Abo EQP') & (dataframe.prix_mensuel != 0)), 'service'] = 'Abo_Equipe_payant' \n",
    "    dataframe.loc[((dataframe['ServiceGroupTitle'] == 'Abo EQP') & (dataframe.prix_mensuel == 0)), 'service'] = 'Abo_Equipe_gratuit'\n",
    "    dataframe.loc[((dataframe['ServiceGroupTitle'] == 'Abo EQP via Tiers') & (dataframe.prix_mensuel != 0)), 'service'] = 'Abo_via_tiers_payant' \n",
    "    dataframe.loc[((dataframe['ServiceGroupTitle'] == 'Abo EQP via Tiers') & (dataframe.prix_mensuel == 0)), 'service'] = 'Abo_via_tiers_gratuit' \n",
    "          \n",
    "    dataframe['provider'] = dataframe['UKI'].str.extract(('.*@(.+)\\..*'))\n",
    "    \n",
    "    \n",
    "    dataframe[\"sub_month\"] = numpy.nan\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,1,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,2,1)), 'sub_month'] = '1'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,2,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,3,1)), 'sub_month'] = '2'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,3,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,4,1)), 'sub_month'] = '3'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,4,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,5,1)), 'sub_month'] = '4'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,5,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,6,1)), 'sub_month'] = '5'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,6,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,7,1)), 'sub_month'] = '6'\n",
    "        \n",
    "    dataframe[\"anciennete_souscripteur\"] = (date(2015, 7, 1) - dataframe[\"anciennete_souscripteur\"])\n",
    "    dataframe['duree_abonnement_reelle'] = dataframe['ServiceExpiry'] - dataframe['SubscriptionCreated']\n",
    "    dataframe[\"derniere_maj\"] = (date(2015, 7, 1) - dataframe[\"SubscriptionLastUpdated\"])\n",
    "    dataframe.anciennete_souscripteur= pd.to_numeric(dataframe.anciennete_souscripteur)/(1000000000 * 60 * 60 * 24)\n",
    "    dataframe.derniere_maj = pd.to_numeric(dataframe.derniere_maj)/(1000000000 * 60 * 60 * 24)\n",
    "    dataframe.duree_abonnement_reelle = pd.to_numeric(dataframe.duree_abonnement_reelle)/(1000000000 * 60 * 60 * 24)\n",
    "    \n",
    "    dataframe['somme_mensualites'] = ((np.maximum(0,dataframe['duree_abonnement_reelle']\n",
    "                                                  -dataframe[\"duree_cadeau\"])/30.416)*dataframe[\"prix_mensuel\"]\n",
    "                                     + (dataframe[\"duree_cadeau\"]/30.416)*dataframe[\"prix_mensuel_cadeau\"])\n",
    "    \n",
    "    #sauvegarde d'un temp pour la data vis\n",
    "    dataframe.to_pickle(path+\"\\dataframes\\s_dataframe_temp_sub_2015.pkl\")\n",
    "    \n",
    "    #dummies \n",
    "    dataframe = pd.get_dummies(dataframe, columns = ['SubscriptionStatus'], prefix ='SubStatus' )\n",
    "    dataframe = pd.get_dummies(dataframe, columns = ['AutoRenew'], prefix ='AutoRenew' )\n",
    "    dataframe = pd.get_dummies(dataframe, columns = ['service'], prefix = 'service')\n",
    "    dataframe = pd.get_dummies(dataframe , columns=['sub_month'], prefix = 'sub_month')\n",
    "    \n",
    "    #group by pour traiter certaines dummies\n",
    "    dataframe.rename(columns=lambda x: x.replace(\" \", \"_\"), inplace=True)\n",
    "    dataframe['SubStatus_Active_Subscription'] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"SubStatus_Active_Subscription\"].transform('min')\n",
    "    dataframe['SubStatus_Cancelled_By_AutoRenew_Process'] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"SubStatus_Cancelled_By_AutoRenew_Process\"].transform('max')\n",
    "    dataframe['SubStatus_Cancelled_By_Customer_Support_Agent'] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"SubStatus_Cancelled_By_Customer_Support_Agent\"].transform('max')\n",
    "    dataframe['SubStatus_Cancelled_By_User'] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"SubStatus_Cancelled_By_User\"].transform('max')\n",
    "    dataframe['SubStatus_Expired_Subscription'] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"SubStatus_Expired_Subscription\"].transform('max')\n",
    "    dataframe['SubStatus_Failure_Retry_Mode'] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"SubStatus_Failure_Retry_Mode\"].transform('max')\n",
    "    dataframe['duree_abonnement_reelle'] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"duree_abonnement_reelle\"].transform('max')\n",
    "    dataframe[\"AutoRenew_0.0\"] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"AutoRenew_0.0\"].transform('max')\n",
    "    dataframe[\"AutoRenew_1.0\"] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"AutoRenew_1.0\"].transform('max')\n",
    "    dataframe[\"AutoRenew_2.0\"] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"AutoRenew_2.0\"].transform('max')\n",
    "    dataframe[\"AutoRenew_4.0\"] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"AutoRenew_4.0\"].transform('max')\n",
    "    \n",
    "    #drop\n",
    "    dataframe.drop(['ExplicitCurrency', 'PriceBandPaymentType', 'PriceBandAmount', 'PriceBandCurrency', 'IsTrial'\n",
    "                  ,'ExplicitPaymentType',\"ServiceGroup\", 'ServiceGroupDescription',\"UKI\"\n",
    "                      ,\"SubscriptionStatusID\",\"ServiceDescription\",\"ServiceExpiry\",'ServiceGroupTitle',\"ExplicitPrice\"\n",
    "                   ,\"SubscriptionLastUpdated\", 'ServiceTitle', 'cadeau',], axis=1, inplace=True)#\"SubscriptionCreated\" ,\n",
    "    \n",
    "    dataframe = dataframe.drop_duplicates()\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "df_prod42_test = nettoyage_sub(df_prod42, liste_client)\n",
    "#print df_prod42_test.somme_mensualites.sum()\n",
    "#df_prod42_test.sort_values(by=['ClientUserId'])\n",
    "#sauvegarde \n",
    "df_prod42_test.to_pickle(path+\"\\dataframes\\s_dataframe_final_sub_2015.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection de la population des subscribers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------nouveaux abonnés juin------------------------\n",
    "df_prod42_test = pd.read_pickle(path+\"\\s_dataframe_final_sub_2015.pkl\")\n",
    "#l_prod42 = df_prod42_test.ClientUserId.unique()\n",
    "\n",
    "#sub juin \n",
    "nouveaux_sub_juin = df_prod42_test.loc[df_prod42_test.sub_month_6 == 1]\n",
    "nouveaux_sub_juin = nouveaux_sub_juin.loc[nouveaux_sub_juin.somme_mensualites >0]\n",
    "nouveaux_sub_juin.drop(['sub_month_1', 'sub_month_2', 'sub_month_3'\n",
    "            , 'sub_month_4','sub_month_5',\"sub_month_6\"], axis=1, inplace=True)\n",
    "\n",
    "nouveaux_sub_juin.to_pickle(path+\"\\s_dataframe_final_sub_payant_juin_2015.pkl\")\n",
    "#\n",
    "#print nouveaux_sub_juin.shape #-> (2021, 28)\n",
    "#print len (nouveaux_sub_juin.ClientUserId.unique()) #-> 2011\n",
    "#\n",
    "#test1 = nouveaux_sub_juin.loc[((nouveaux_sub_juin.service_Abo_Equipe_payant ==1 ) | (nouveaux_sub_juin.service_Abo_via_tiers_payant == 1))]\n",
    "#print test1.shape #-> (946, 28)\n",
    "#print len(test1.ClientUserId.unique()) #-> 945\n",
    "#\n",
    "#test2 = nouveaux_sub_juin.loc[nouveaux_sub_juin.somme_mensualites >0]\n",
    "#print test2.shape #-> (439, 28)\n",
    "#print len(test2.ClientUserId.unique()) #-> 439\n",
    "## ***.to_pickle(path+\"\\s_final_groupby_dataframe_sub_juin_2015.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Clients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------------------nettoyage clients--------------------------\n",
    "#dataframe = pd.read_pickle(r\"W:\\L_equipe\\df_client_true_site.pkl\")\n",
    "dataframe = pd.read_csv(r\"V:\\L_Equipe\\Projet_01042016\\tables_brutes\\clients_2015.csv\")\n",
    "\n",
    "#dataframe = dataframe.loc[dataframe['id_client'].isin(liste_client)]\n",
    "\n",
    "#formatage datetime \n",
    "dataframe['premiere_visite'] = pd.to_datetime(dataframe['premiere_visite'], format='%Y-%m-%d %H:%M:%S')\n",
    "dataframe['derniere_visite'] = pd.to_datetime(dataframe['derniere_visite'], format='%Y-%m-%d %H:%M:%S')\n",
    "#group by\n",
    "dataframe['somme_visites_totale'] = dataframe.groupby(['id_client']).visites.transform('sum')\n",
    "dataframe['somme_pages_vues_totale'] = dataframe.groupby([\"id_client\"])[\"pages_vues\"].transform('sum')\n",
    "dataframe['anciennete'] = dataframe.groupby(['id_client'])[\"premiere_visite\"].transform('min')\n",
    "dataframe['recence_de_visite'] = dataframe.groupby(['id_client'])[\"derniere_visite\"].transform('max')\n",
    "    \n",
    "#on choisit les clients qui on eu une acitivité entre  mars et mai 2015\n",
    "#dataframe = dataframe.loc[(dataframe.anciennete < date(2015,6,1)) & (dataframe.recence_de_visite > date(2015,2,28))]\n",
    "#dataframe = dataframe.loc[dataframe.recence_de_visite > date(2015,2,28)]\n",
    "\n",
    "\n",
    "dataframe['recence_de_visite'] =(date(2015, 7, 1) - dataframe.recence_de_visite)\n",
    "dataframe.recence_de_visite = pd.to_numeric(dataframe.recence_de_visite)/(1000000000 * 60 * 60 * 24)\n",
    "dataframe['anciennete'] =(date(2015, 7, 1) - dataframe.anciennete)\n",
    "dataframe.anciennete = pd.to_numeric(dataframe.anciennete)/(1000000000 * 60 * 60 * 24)\n",
    "\n",
    "dataframe['somme_visites_par_site'] = dataframe.groupby([\"id_client\",\"id_site\"])[\"visites\"].transform('sum')\n",
    "dataframe['pourcentage_visites_par_site'] = dataframe[\"somme_visites_par_site\"]/dataframe[\"somme_visites_totale\"]*100\n",
    "\n",
    "dataframe['somme_pages_vues_par_site'] = dataframe.groupby([\"id_client\",\"id_site\"])[\"pages_vues\"].transform('sum')\n",
    "dataframe['pourcentage_pages_vues_par_site'] = dataframe[\"somme_pages_vues_par_site\"]/dataframe[\"somme_pages_vues_totale\"]*100\n",
    "\n",
    "#dummies\n",
    "dataframe[\"id_site2\"] = dataframe.id_site\n",
    "dataframe = pd.get_dummies(dataframe, columns =['id_site'], prefix ='%visite_site')\n",
    "dataframe.drop([\"%visite_site_555493\",\"%visite_site_40086\",\"%visite_site_498703\",\"%visite_site_413580\",\n",
    "                '%visite_site_499304'],axis=1, inplace=True, errors='ingore')\n",
    "\n",
    "dataframe = pd.get_dummies(dataframe, columns =['id_site2'], prefix ='%pages_vues_site')\n",
    "dataframe.drop([\"%pages_vues_site_555493\",\"%pages_vues_site_40086\",\"%pages_vues_site_498703\",\"%pages_vues_site_413580\",\n",
    "                '%pages_vues_site_499304'],axis=1, inplace=True, errors='ingore')\n",
    "\n",
    "\n",
    "#mutliplication des dummies pour optenir le pourcentage de visites par site\n",
    "dataframe[['%visite_site_492987','%visite_site_496306','%visite_site_496307','%visite_site_496838','%visite_site_502193'\n",
    ",'%visite_site_509043','%visite_site_539121','%visite_site_548647']] = dataframe[['%visite_site_492987','%visite_site_496306'\n",
    ",'%visite_site_496307','%visite_site_496838','%visite_site_502193','%visite_site_509043','%visite_site_539121'\n",
    ",'%visite_site_548647']].multiply(dataframe[\"pourcentage_visites_par_site\"], axis=\"index\")\n",
    "\n",
    "#mutliplication des dummies pour optenir le pourcentage de pages vues par site\n",
    "dataframe[['%pages_vues_site_492987','%pages_vues_site_496306','%pages_vues_site_496307','%pages_vues_site_496838'\n",
    ",'%pages_vues_site_502193','%pages_vues_site_509043','%pages_vues_site_539121'\n",
    ",'%pages_vues_site_548647']] = dataframe[['%pages_vues_site_492987','%pages_vues_site_496306','%pages_vues_site_496307'\n",
    ",'%pages_vues_site_496838','%pages_vues_site_502193','%pages_vues_site_509043','%pages_vues_site_539121'\n",
    ",'%pages_vues_site_548647']].multiply(dataframe[\"pourcentage_pages_vues_par_site\"], axis=\"index\")\n",
    "\n",
    "dataframe[['%visite_site_492987','%visite_site_496306','%visite_site_496307','%visite_site_496838','%visite_site_502193'\n",
    ",'%visite_site_509043','%visite_site_539121','%visite_site_548647']] = dataframe.groupby([\"id_client\"])['%visite_site_492987'\n",
    ",'%visite_site_496306','%visite_site_496307','%visite_site_496838','%visite_site_502193','%visite_site_509043'\n",
    ",'%visite_site_539121','%visite_site_548647'].transform('max')\n",
    "\n",
    "dataframe[['%pages_vues_site_492987','%pages_vues_site_496306','%pages_vues_site_496307','%pages_vues_site_496838'\n",
    ",'%pages_vues_site_502193','%pages_vues_site_509043','%pages_vues_site_539121'\n",
    ",'%pages_vues_site_548647']] = dataframe.groupby([\"id_client\"])['%pages_vues_site_492987','%pages_vues_site_496306'\n",
    ",'%pages_vues_site_496307','%pages_vues_site_496838','%pages_vues_site_502193','%pages_vues_site_509043'\n",
    ",'%pages_vues_site_539121','%pages_vues_site_548647'].transform(\"max\")\n",
    "\n",
    "\n",
    "dataframe.drop(['somme_visites_par_site', 'somme_pages_vues_par_site','premiere_visite', 'derniere_visite'\n",
    "                , 'fileid', 'visites', 'pages_vues', 'pourcentage_pages_vues_par_site','pourcentage_visites_par_site']\n",
    "               , axis=1, inplace=True)\n",
    "dataframe = dataframe.drop_duplicates()\n",
    "\n",
    "dataframe.to_pickle(path+\"\\s_dataframe_final_clients_mam_pas_forcmnt_juin.pkl\")\n",
    "\n",
    "#client ayant fait au moins une visite entre mars et mai 2015 et au moins une en juin 2015\n",
    "dataframe = dataframe.loc[dataframe.recence_de_visite < 31]\n",
    "dataframe.to_pickle(path+\"\\s_dataframe_final_clients.pkl\")\n",
    "\n",
    "#df_client_clean = pd.read_pickle(path+\"\\dataframes\\s_dataframes_final_clients.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean commmandes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    6540\n",
      "1.0     135\n",
      "Name: prod_type_Service, dtype: int64\n",
      "1.0    2230\n",
      "Name: prod_type_Service, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "path = 'W:\\L_equipe\\\\'\n",
    "dataframe= pd.read_pickle(path+\"\\dataframes\\s_dataframe_general_vel_commande.pkl\")\n",
    "\n",
    "def nettoyage_commande(dataframe, liste_client) :\n",
    "    #selection des abonnés de l'equipe\n",
    "    dataframe = dataframe.loc[dataframe['ClientUserId'].isin(liste_client)]\n",
    "    #formatage datetime \n",
    "    dataframe['OrderDate'] = pd.to_datetime(dataframe['OrderDate'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    #On ne considère pas les achats et les souscriptions post juin 2015\n",
    "    dataframe = dataframe.loc[dataframe.OrderDate < date(2015,7,1)]\n",
    "       \n",
    "    dataframe = dataframe.loc[dataframe.OrderStatus == 'Completed']\n",
    "    #Pour bien faire le lien avec la table souscription on réccupère les nouveaux abonnés 2015\n",
    "    datasub = dataframe.loc[dataframe.ProductType == 'Service']\n",
    "    datasub['anciennete_souscripteur'] = datasub.groupby(['ClientUserId']).OrderDate.transform('min')\n",
    "    datasub = datasub.loc[((datasub.anciennete_souscripteur > date(2015,1,1)) & (datasub.OrderDate < date(2015,7,1))) ]\n",
    "    l_sub = datasub.ClientUserId.unique()\n",
    "    datasub = datasub[[\"ClientUserId\", \"anciennete_souscripteur\"]]\n",
    "    datasub = datasub.drop_duplicates()\n",
    "    \n",
    "    dataframe = dataframe.merge(datasub,how='left', left_on='ClientUserId', right_on='ClientUserId')\n",
    "    \n",
    "    \n",
    "    #fusion du nettoyage des souscriptions avec les commandes\n",
    "    dataframe.rename(columns={'ServiceId' : \"ServiceID\"}, inplace=True)\n",
    "    df_sous = pd.read_excel(path+'\\souscription_detail.xlsx')\n",
    "    df_sous.drop(['ServiceTitle'], axis=1, inplace=True)\n",
    "    dataframe = pd.merge(dataframe, df_sous, how='left', on=\"ServiceID\")\n",
    "        \n",
    "    dataframe[\"sub_month\"] = numpy.nan\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,1,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,2,1)), 'sub_month'] = '1'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,2,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,3,1)), 'sub_month'] = '2'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,3,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,4,1)), 'sub_month'] = '3'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,4,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,5,1)), 'sub_month'] = '4'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,5,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,6,1)), 'sub_month'] = '5'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,6,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,7,1)), 'sub_month'] = '6' \n",
    "    \n",
    "    #group by\n",
    "    \n",
    "    dataframe['recence_souscripteur'] = 0\n",
    "    dataframe['recence_souscripteur'].loc[dataframe.ProductType == 'Service'] =\\\n",
    "    dataframe.loc[dataframe.ProductType == 'Service'].groupby(['ClientUserId'])[\"OrderDate\"].transform('max')\n",
    "    dataframe[\"recence_souscripteur\"].loc[dataframe.ProductType == 'Service'] = (date(2015, 7, 1) - dataframe.recence_souscripteur)\n",
    "    dataframe.recence_souscripteur = pd.to_numeric(dataframe.recence_souscripteur)/(1000000000 * 60 * 60 * 24)\n",
    "    \n",
    "    dataframe.anciennete_souscripteur = (date(2015, 7, 1) - dataframe.anciennete_souscripteur)\n",
    "    dataframe.anciennete_souscripteur = pd.to_numeric(dataframe.anciennete_souscripteur)/(1000000000 * 60 * 60 * 24)\n",
    "    \n",
    "    #On ne prend en compte que les commande pré souscription\n",
    "    dataframe[\"article_OK\"] = 0\n",
    "    dataframe.loc[dataframe.anciennete_souscripteur == 0 , \"article_OK\"] = 1\n",
    "    dataframe[\"numeric_orderdate\"] = (date(2015, 7, 1) - dataframe.OrderDate)\n",
    "    dataframe.numeric_orderdate = pd.to_numeric(dataframe.numeric_orderdate)/(1000000000 * 60 * 60 * 24)\n",
    "    dataframe.loc[((dataframe.ProductType == \"Digital\") & (dataframe.numeric_orderdate >dataframe.anciennete_souscripteur)), \"article_OK\"] = 1\n",
    "                                                           \n",
    "    dataframe.loc[((dataframe.ProductType == \"Service\") | (dataframe.article_OK == 1))]\n",
    "    \n",
    "\n",
    "    dataframe[\"recence_article\"] = 0\n",
    "    dataframe['recence_article'].loc[dataframe.ProductType == 'Digital'] =\\\n",
    "    dataframe.loc[dataframe.ProductType == 'Digital'].groupby(['ClientUserId'])[\"OrderDate\"].transform('max')\n",
    "    dataframe[\"recence_article\"].loc[dataframe.ProductType == 'Digital'] = (date(2015, 7, 1) - dataframe.recence_article)\n",
    "    dataframe.recence_article = pd.to_numeric(dataframe.recence_article)/(1000000000 * 60 * 60 * 24)\n",
    "\n",
    "    dataframe[\"anciennete_article\"] = 0\n",
    "    dataframe['anciennete_article'].loc[dataframe.ProductType == \"Digital\"] =\\\n",
    "    dataframe.loc[dataframe.ProductType == \"Digital\"].groupby(['ClientUserId'])[\"OrderDate\"].transform('min')\n",
    "    dataframe.anciennete_article.loc[dataframe.ProductType == \"Digital\"] = (date(2015, 7, 1) - dataframe.anciennete_article)\n",
    "    dataframe.anciennete_article = pd.to_numeric(dataframe.anciennete_article)/(1000000000 * 60 * 60 * 24)\n",
    "    \n",
    "    dataframe[\"somme_paiement_par_abonnement\"] =0\n",
    "    dataframe[\"somme_paiement_par_abonnement\"].loc[dataframe.ProductType == 'Service'] = \\\n",
    "    dataframe.loc[dataframe.ProductType == 'Service'].groupby([\"ClientUserId\",\"ServiceID\"]).GrossAmount.transform('sum')\n",
    "    \n",
    "    dataframe['nb_mensualites'] = 0\n",
    "    dataframe['nb_mensualites'].loc[((dataframe.ProductType == 'Service') & (dataframe.duree_abonnement_theorique ==0 ))] =\\\n",
    "    dataframe.loc[dataframe.ProductType == 'Service'].groupby([\"ClientUserId\",\"ServiceID\"]).BasketNumber.transform('count')\n",
    "     \n",
    "\n",
    "    dataframe[\"somme_mensualites\"] = 0\n",
    "    dataframe[\"somme_mensualites\"].loc[((dataframe.ProductType == 'Service') & (dataframe.duree_abonnement_theorique ==0 ))] =\\\n",
    "    dataframe.loc[((dataframe.ProductType == 'Service') & (dataframe.duree_abonnement_theorique ==0 ))]\\\n",
    "    .groupby([\"ClientUserId\"]).BasketNumber.transform('count')\n",
    "    \n",
    "    \n",
    "    dataframe[\"mensualite_moyenne\"] = 0\n",
    "    dataframe[\"mensualite_moyenne\"].loc[((dataframe.ProductType == 'Service') & (dataframe.duree_abonnement_theorique ==0 ))] =\\\n",
    "    dataframe.loc[((dataframe.ProductType == 'Service') & (dataframe.duree_abonnement_theorique ==0 ))]\\\n",
    "    .groupby(['ClientUserId']).GrossAmount.transform(\"sum\")\n",
    "    dataframe[\"mensualite_moyenne\"] = dataframe.mensualite_moyenne / dataframe.somme_mensualites\n",
    "   \n",
    "    \n",
    "    #pour les abos fermes :\n",
    "    dataframe.loc[((dataframe.somme_mensualites == 0) & (dataframe.duree_abonnement_theorique > 0)), \"somme_mensualites\"] =\\\n",
    "    dataframe.loc[((dataframe.somme_mensualites == 0) & (dataframe.duree_abonnement_theorique > 0))].duree_abonnement_theorique/30\n",
    "    \n",
    "    dataframe.loc[((dataframe.somme_mensualites == 0) & (dataframe.duree_abonnement_theorique > 0)), \"mensualite_moyenne\"] =\\\n",
    "    dataframe.loc[((dataframe.somme_mensualites == 0) & (dataframe.duree_abonnement_theorique > 0))].GrossAmount/\\\n",
    "    dataframe.loc[((dataframe.somme_mensualites == 0) & (dataframe.duree_abonnement_theorique > 0))].somme_mensualites\n",
    "   \n",
    "    dataframe['somme_paiement_totale'] = dataframe.groupby([\"ClientUserId\"]).GrossAmount.transform('sum')\n",
    "    \n",
    "    dataframe[\"somme_paiement_article\"] =0\n",
    "    dataframe[\"somme_paiement_article\"].loc[dataframe.ProductType == 'Digital'] = \\\n",
    "    dataframe.loc[dataframe.ProductType == 'Digital'].groupby([\"ClientUserId\"]).GrossAmount.transform('sum')\n",
    "        \n",
    "    #features\n",
    "    dataframe.ServiceID.fillna(0, inplace=True)\n",
    "    #dataframe['theme_article'] = dataframe['ARTICLE_URL'].str.extract(('.*www.lequipe.fr/([a-zA-Z]+)/.*'))\n",
    "    #dataframe.loc[dataframe.somme_paiement_par_abonnement == 0, 'commande_payante']=1\n",
    "    #dataframe.loc[dataframe.somme_paiement_par_abonnement != 0, \"commande_payante\"]=0\n",
    "    \n",
    "    \n",
    "    #dummies\n",
    "    dataframe = pd.get_dummies(dataframe, columns = ['ProductType'], prefix ='prod_type')\n",
    "    dataframe = pd.get_dummies(dataframe , columns=['sub_month'], prefix = 'sub_month')\n",
    "    \n",
    "    #dataframe['theme_article'] = dataframe['ARTICLE_URL'].str.extract(('.*www.lequipe.fr/([a-zA-Z]+)/.*'))\n",
    "    #dataframe = pd.get_dummies(dataframe, columns=['theme_article'], prefix='theme_article')\n",
    "    \n",
    "    #duplication des infos utiles pour split abonnements et articles\n",
    "    \n",
    "    \n",
    "    dataframe['nb_articles'] = dataframe.prod_type_Digital\n",
    "    dataframe['nb_articles'] = dataframe.groupby(['ClientUserId']).nb_articles.transform(\"sum\")\n",
    "   \n",
    "    #drop\n",
    "    dataframe.drop([\"AffiliateId\",\"BasketNumber\",\"Country\",\"County\",\"Currency\",\"District\",\"NetAmount\",\"HouseFlatNumber\",\n",
    "        \"HouseName\",\"Message\",\"Name\",\"OfferId\",\"OrderStatus\",\"ParameterOrderId\",\"PaymentMethod\",\"PostCode\",\"ProductDataSource\",\n",
    "        \"Street\",\"SupplierID\",\"ThirdPartyRef\",\"Titre\",\"TotalTaxPercentage\",\"TownCity\",\"VATAmount\",\"detail_mode_paiement\",\"gift_id\",\n",
    "        \"identifiant_commercial\",\"identifiant_salon\",\"lien\",\"provenance\",\"type\",'ActivationCode',\n",
    "        'DeliveryComments', 'subscriptionId','ContentItemId', \"ARTICLE_TITLE\", \"ARTICLE_URL\"\n",
    "       ,'GrossAmount', 'OrderID', 'AccountID', \"MPPGiftCode\",\"Description\", 'MPPGiftID', 'OrderDate', 'somme_paiement_par_abonnement',\n",
    "         'ServiceID', 'article_OK',\"numeric_orderdate\"], axis=1, inplace=True)\n",
    "    \n",
    "    #on ne garde pas les souscription de avril/mai\n",
    "    dataframe = dataframe.loc[((dataframe.sub_month_4==0) & (dataframe.sub_month_5==0))]\n",
    "    dataframe.drop(['sub_month_4','sub_month_5','sub_month_6'], axis=1, inplace=True)\n",
    "    dataframe = dataframe.drop_duplicates()\n",
    "  \n",
    "    dataframe[[\"duree_abonnement_theorique\",\"prix_mensuel\",\"occasion\",\"cadeau\",\"prix_mensuel_cadeau\",\"duree_cadeau\",\n",
    "              \"prod_type_Digital\",\"prod_type_Service\",\"anciennete_souscripteur\"\n",
    "              ,\"anciennete_article\",\"mensualite_moyenne\", \"nb_mensualites\"]] = dataframe.groupby([\"ClientUserId\"])[\"duree_abonnement_theorique\"\n",
    "            ,\"prix_mensuel\",\"occasion\",\"cadeau\",\"prix_mensuel_cadeau\",\"duree_cadeau\",\"prod_type_Digital\",\"prod_type_Service\"\n",
    "            ,\"anciennete_souscripteur\",\"anciennete_article\",\"mensualite_moyenne\",\"nb_mensualites\"].transform(max)\n",
    "    #print dataframe.loc[dataframe.prod_type_Digital == 1].prod_type_Service.value_counts()\n",
    "    #print dataframe.loc[dataframe.prod_type_Digital == 0].prod_type_Service.value_counts()\n",
    "    #return\n",
    "\n",
    "    dataframe[['recence_article', 'recence_souscripteur']] = dataframe.groupby(['ClientUserId'])['recence_article'\n",
    "                    , 'recence_souscripteur'].transform('min')\n",
    "    \n",
    "    dataframe = dataframe.drop_duplicates()\n",
    "    \n",
    "    dataframe[\"nb_abonnements\"] = dataframe.groupby([\"ClientUserId\"]).prod_type_Service.transform(\"sum\")\n",
    "    dataframe[[\"somme_mensualites\", \"somme_paiement_article\"]] = dataframe.groupby([\"ClientUserId\"])[\"somme_mensualites\"\n",
    "    ,\"somme_paiement_article\"].transform(sum)\n",
    "\n",
    "    dataframe = dataframe.drop_duplicates()\n",
    "    dataframe.fillna(0, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "df_commande = nettoyage_commande(dataframe, liste_client)\n",
    "#df_commande.to_pickle(path+\"\\dataframes\\s_dataframe_final_commandes.pkl\")\n",
    "\n",
    "\n",
    "#df_commande.sort_values(by='ClientUserId')\n",
    "#df_commande.nb_abonnements.value_counts(dropna=False)\n",
    "#pandas_profiling.ProfileReport(df_commande)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature pour indiquer à partir de quelle date on regarde pour les sub \n",
    "path = \"W:\\L_equipe\\dataframes\\\\\"\n",
    "sub = pd.read_pickle(path+\"\\s_dataframe_final_sub_payant_juin_2015.pkl\")\n",
    "sub[\"date_observation_min\"] = sub.SubscriptionCreated - pd.DateOffset(months=3)\n",
    "#dateutil.relativedelta.relativedelta(months=3)\n",
    "sub[\"abo_juin\"] = 1\n",
    "sub.to_pickle(path+\"\\s_dataframe_final_sub_payant_juin_2015.pkl\")\n",
    "\n",
    "abo_2015 = pd.read_pickle(path+\"\\s_dataframe_final_sub_2015.pkl\")\n",
    "abo_2015[\"subscriber\"] = 1\n",
    "abo_2015.to_pickle(path+\"\\s_dataframe_final_sub_2015.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dataframe= pd.read_pickle(path+\"\\dataframes\\s_dataframe_general_xiti_sessions_mars_juin2015.pkl\")\n",
    "path = \"W:\\L_equipe\\dataframes\\\\\"\n",
    "files  = []\n",
    "files.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                        if f.startswith('s_dataframe_general_xiti_sessions') if getsize(join(path,f))>0])\n",
    "\n",
    "for f in files :\n",
    "    dataframe = pd.read_pickle(f)\n",
    "    reg = re.match(('.*s_dataframe_general_xiti_sessions_(.*)\\.pkl'), f)\n",
    "    \n",
    "    #merge avec  les sub juin\n",
    "    sub = pd.read_pickle(path+\"\\s_dataframe_final_sub_payant_juin_2015.pkl\")\n",
    "    sub2 = sub[[\"ClientUserId\", \"date_observation_min\", \"abo_juin\"]]\n",
    "    dataframe = pd.merge(dataframe, sub2, how='left', left_on='id_client', right_on=\"ClientUserId\")\n",
    "    dataframe.abo_juin.fillna(0, inplace=True)\n",
    "    date_min_visiteur = date(2015,3,1)\n",
    "    dataframe.date_observation_min.fillna(date_min_visiteur,inplace=True)\n",
    "    dataframe[\"date_observation_max\"] = dataframe.date_observation_min + pd.DateOffset(months = 3) - pd.DateOffset(days = 1)\n",
    "    dataframe.loc[((dataframe.debut_de_session > dataframe.date_observation_min) & (dataframe.debut_de_session < dataframe.date_observation_max))]\n",
    "    \n",
    "    #merge avec les sub 2015 pour ne garder que les non subs ou les sub juin 2015\n",
    "    abo_2015 = pd.read_pickle(path+\"\\s_dataframe_final_sub_2015.pkl\")\n",
    "    abo2 = abo_2015[[\"ClientUserId\", 'subscriber']]\n",
    "    dataframe = pd.merge(dataframe, abo2, how='left', left_on='id_client', right_on=\"ClientUserId\")\n",
    "    dataframe.subscriber.fillna(0, inplace=True)\n",
    "    dataframe = dataframe.loc[((dataframe.subscriber == 0) | (dataframe.abo_juin==1))]\n",
    "\n",
    "    #def clean_session(dataframe, liste_client) :\n",
    "    #selection des abonnés de l'equipe\n",
    "    dataframe = dataframe.loc[dataframe['id_client'].isin(liste_client)]\n",
    "    #suppression des lignes ou fin de session = u'0001-01-01 00:00:00'\n",
    "    dataframe = dataframe.loc[dataframe.fin_de_session != u'0001-01-01 00:00:00']\n",
    "    #formatage datetime \n",
    "    dataframe['debut_de_session'] = pd.to_datetime(dataframe['debut_de_session'], format='%Y-%m-%d %H:%M:%S')\n",
    "    dataframe['fin_de_session'] = pd.to_datetime(dataframe['fin_de_session'], format='%Y-%m-%d %H:%M:%S')\n",
    "    dataframe['duree_session'] = dataframe.fin_de_session - dataframe.debut_de_session\n",
    "    \n",
    "    dataframe.loc[((dataframe.debut_de_session.dt.hour >= 0) & (dataframe.debut_de_session.dt.hour <= 5)), \"periode\" ] = \"nuit\"\n",
    "    dataframe.loc[((dataframe.debut_de_session.dt.hour >= 6) & (dataframe.debut_de_session.dt.hour <=8)), \"periode\" ] = \"aube\"\n",
    "    dataframe.loc[((dataframe.debut_de_session.dt.hour >= 9) & (dataframe.debut_de_session.dt.hour <12)), \"periode\" ] = \"matinee\"\n",
    "    dataframe.loc[((dataframe.debut_de_session.dt.hour >= 12) & (dataframe.debut_de_session.dt.hour < 14)), \"periode\" ] = \"midi\"\n",
    "    dataframe.loc[((dataframe.debut_de_session.dt.hour >= 14) & (dataframe.debut_de_session.dt.hour < 17)), \"periode\"] = \"apres_midi\"\n",
    "    dataframe.loc[((dataframe.debut_de_session.dt.hour >=17 )& (dataframe.debut_de_session.dt.hour <20)), 'periode'] = 'fin_apres_midi'\n",
    "    dataframe.loc[((dataframe.debut_de_session.dt.hour >= 20 ) & (dataframe.debut_de_session.dt.hour < 24)), \"periode\"] = \"soiree\"\n",
    "    \n",
    "    \n",
    "    dataframe.loc[dataframe['localisation_3eme_niveau'] == 'Paris', 'Paris'] = 1\n",
    "    dataframe.loc[dataframe['localisation_3eme_niveau'] != 'Paris', 'Paris'] = 0\n",
    "    \n",
    "    dataframe.loc[dataframe['localisation_2eme_niveau'] == 'Ile-de-France', 'RP'] = 1\n",
    "    dataframe.loc[dataframe['localisation_2eme_niveau'] != 'Ile-de-France', 'RP'] = 0\n",
    "    \n",
    "    dataframe.loc[dataframe['localisation_1er_niveau'] == 'France', 'France'] = 1\n",
    "    dataframe.loc[dataframe['localisation_1er_niveau'] != 'France', 'France'] = 0\n",
    "    \n",
    "    liste_semaines = [\"semaine1\",\"semaine2\",\"semaine3\",\"semaine4\",\"semaine5\",\"semaine6\",\"semaine7\",\"semaine8\",\"semaine9\",\"semaine10\",\n",
    "             \"semaine11\",\"semaine12\"]\n",
    "    liste_mois = [\"mois1\",\"mois2\",\"mois3\"]\n",
    "    \n",
    "    dataframe.loc[(dataframe.debut_de_session >= dataframe.date_observation_max - pd.DateOffset(months = 1)), \"mois3\"] = 1\n",
    "    dataframe.loc[((dataframe.debut_de_session >= dataframe.date_observation_max - pd.DateOffset(months = 2)) &\\\n",
    "                   (dataframe.debut_de_session < dataframe.date_observation_max -pd.DateOffset(months =1))) , \"mois2\"] = 1\n",
    "    dataframe.loc[((dataframe.debut_de_session >= dataframe.date_observation_max - pd.DateOffset(months = 3)) &\\\n",
    "                   (dataframe.debut_de_session < dataframe.date_observation_max -pd.DateOffset(months =2))), \"mois1\"] = 1\n",
    "    \n",
    "    dataframe.loc[(dataframe['debut_de_session'] >=dataframe.date_observation_max - pd.DateOffset(weeks = 1)), \"semaine12\" ] = 1\n",
    "    dataframe.loc[((dataframe['debut_de_session'] >=dataframe.date_observation_max - pd.DateOffset(weeks = 2)) &\\\n",
    "                   (dataframe['debut_de_session'] < dataframe.date_observation_max - pd.DateOffset(weeks = 1))) , \"semaine11\" ] = 1\n",
    "    dataframe.loc[((dataframe['debut_de_session'] >=dataframe.date_observation_max - pd.DateOffset(weeks = 3)) &\\\n",
    "                   (dataframe['debut_de_session'] < dataframe.date_observation_max - pd.DateOffset(weeks = 2))) , \"semaine10\" ] = 1\n",
    "    dataframe.loc[((dataframe['debut_de_session'] >=dataframe.date_observation_max - pd.DateOffset(weeks = 4)) &\\\n",
    "                   (dataframe['debut_de_session'] < dataframe.date_observation_max - pd.DateOffset(weeks = 3))) , \"semaine9\" ] = 1\n",
    "    dataframe.loc[((dataframe['debut_de_session'] >=dataframe.date_observation_max - pd.DateOffset(weeks = 5)) &\\\n",
    "                   (dataframe['debut_de_session'] < dataframe.date_observation_max - pd.DateOffset(weeks = 4))) , \"semaine8\" ] = 1\n",
    "    dataframe.loc[((dataframe['debut_de_session'] >=dataframe.date_observation_max - pd.DateOffset(weeks = 6)) &\\\n",
    "                   (dataframe['debut_de_session'] < dataframe.date_observation_max - pd.DateOffset(weeks = 5))) , \"semaine7\" ] = 1\n",
    "    dataframe.loc[((dataframe['debut_de_session'] >=dataframe.date_observation_max - pd.DateOffset(weeks = 7)) &\\\n",
    "                   (dataframe['debut_de_session'] < dataframe.date_observation_max - pd.DateOffset(weeks = 6))) , \"semaine6\" ] = 1\n",
    "    dataframe.loc[((dataframe['debut_de_session'] >=dataframe.date_observation_max - pd.DateOffset(weeks = 8)) &\\\n",
    "                   (dataframe['debut_de_session'] < dataframe.date_observation_max - pd.DateOffset(weeks = 7))) , \"semaine5\" ] = 1\n",
    "    dataframe.loc[((dataframe['debut_de_session'] >=dataframe.date_observation_max - pd.DateOffset(weeks = 9)) &\\\n",
    "                   (dataframe['debut_de_session'] < dataframe.date_observation_max - pd.DateOffset(weeks = 8))) , \"semaine4\" ] = 1\n",
    "    dataframe.loc[((dataframe['debut_de_session'] >=dataframe.date_observation_max - pd.DateOffset(weeks = 10)) &\\\n",
    "                   (dataframe['debut_de_session'] < dataframe.date_observation_max - pd.DateOffset(weeks = 9))) , \"semaine3\" ] = 1\n",
    "    dataframe.loc[((dataframe['debut_de_session'] >=dataframe.date_observation_max - pd.DateOffset(weeks = 11)) &\\\n",
    "                   (dataframe['debut_de_session'] < dataframe.date_observation_max - pd.DateOffset(weeks = 10))) , \"semaine2\" ] = 1\n",
    "    dataframe.loc[((dataframe['debut_de_session'] >=dataframe.date_observation_max - pd.DateOffset(weeks = 12)) &\\\n",
    "                   (dataframe['debut_de_session'] < dataframe.date_observation_max - pd.DateOffset(weeks = 11))) , \"semaine1\" ] = 1\n",
    "\n",
    "    #dataframe.loc[((dataframe['debut_de_session'] <= date(2015, 6, 7)) &\n",
    "    #              (dataframe.debut_de_session > date(2015,5,31))), 'juin_semaine_1'] = 1\n",
    "    #dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 6, 14))& \n",
    "    #              (dataframe['debut_de_session'] > date(2015, 6, 7)), 'juin_semaine_2'] = 1\n",
    "    #dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 6, 21) ) & \n",
    "    #              (dataframe['debut_de_session'] > date(2015, 6, 14)), 'juin_semaine_3'] = 1\n",
    "    #dataframe.loc[(dataframe['debut_de_session'] > date(2015, 6, 21)) & \n",
    "    #              (dataframe.debut_de_session < date(2015,7,1)), 'juin_semaine_4'] = 1\n",
    "    #\n",
    "    #dataframe.loc[((dataframe['debut_de_session'] <= date(2015, 5,10)) &\n",
    "    #              (dataframe.debut_de_session > date(2015,4,30))), 'mai_semaine_1'] = 1\n",
    "    #dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 5, 17))& \n",
    "    #              (dataframe['debut_de_session'] > date(2015, 5, 10)), 'mai_semaine_2'] = 1\n",
    "    #dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 5, 24) ) & \n",
    "    #              (dataframe['debut_de_session'] > date(2015, 5, 17)), 'mai_semaine_3'] = 1\n",
    "    #dataframe.loc[(dataframe['debut_de_session'] > date(2015, 5, 24)) & \n",
    "    #              (dataframe.debut_de_session < date(2015,6,1)), 'mai_semaine_4'] = 1\n",
    "    #                  \n",
    "    #dataframe.loc[((dataframe['debut_de_session'] <= date(2015, 4, 5)) &\n",
    "    #              (dataframe.debut_de_session > date(2015,3,31))), 'avril_semaine_1'] = 1\n",
    "    #dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 4, 12))& \n",
    "    #              (dataframe['debut_de_session'] > date(2015, 4, 5)), 'avril_semaine_2'] = 1\n",
    "    #dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 4, 19) ) & \n",
    "    #              (dataframe['debut_de_session'] > date(2015, 4, 12)), 'avril_semaine_3'] = 1\n",
    "    #dataframe.loc[(dataframe['debut_de_session'] > date(2015, 4, 19)) & \n",
    "    #              (dataframe.debut_de_session < date(2015,5,1)), 'avril_semaine_4'] = 1\n",
    "    #               \n",
    "    #dataframe.loc[((dataframe['debut_de_session'] <= date(2015, 3, 8)) &\n",
    "    #              (dataframe.debut_de_session > date(2015,2,28))), 'mars_semaine_1'] = 1\n",
    "    #dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 3, 15))& \n",
    "    #              (dataframe['debut_de_session'] > date(2015, 3, 8)), 'mars_semaine_2'] = 1\n",
    "    #dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 3, 22) ) & \n",
    "    #              (dataframe['debut_de_session'] > date(2015, 3, 15)), 'mars_semaine_3'] = 1\n",
    "    #dataframe.loc[(dataframe['debut_de_session'] > date(2015, 3, 22)) & \n",
    "    #              (dataframe.debut_de_session < date(2015,4,1)), 'mars_semaine_4'] = 1\n",
    "    \n",
    "    liste_weekend = [date(2015,3,1),date(2015,3,7),date(2015,3,8),date(2015,3,14),date(2015,3,15), date(2015,3,21),date(2015,3,22),date(2015,3,28),\n",
    "                     date(2015,3,29),date(2015,4,4),date(2015,4,5),date(2015,4,11),date(2015,4,12),date(2015,4,18),date(2015,4,19),date(2015,4,25),\n",
    "                     date(2015,4,26),date(2015,5,2),date(2015,5,3),date(2015,5,9),date(2015,5,10),date(2015,5,16),date(2015,5,17),date(2015,5,23),\n",
    "                     date(2015,5,24),date(2015,5,30),date(2015,5,31),date(2015,6,6),date(2015,6,7),date(2015,6,13),date(2015,6,14),date(2015,6,20),\n",
    "                     date(2015,6,21),date(2015,6,27),date(2015,6,28)]\n",
    "    \n",
    "    dataframe[\"type_jour\"] = \"semaine\"\n",
    "    dataframe.loc[dataframe.debut_de_session.dt.date.isin(liste_weekend), \"type_jour\"] = \"weekend\"\n",
    "    \n",
    "    #liste_semaines = [col for col in list(dataframe) if col.find(\"semaine\")]\n",
    "\n",
    "    dataframe[liste_semaines] = dataframe.groupby([\"id_client\"])[liste_semaines].transform(max)\n",
    "    dataframe[liste_mois] = dataframe.groupby([\"id_client\"])[liste_mois].transform(max)\n",
    "    \n",
    "    #dataframe[\"mars_semaine_1\"] = dataframe.groupby([\"id_client\"])[\"mars_semaine_1\"].transform(\"max\")\n",
    "    #dataframe[\"mars_semaine_2\"] = dataframe.groupby([\"id_client\"])[\"mars_semaine_2\"].transform(\"max\")\n",
    "    #dataframe[\"mars_semaine_3\"] = dataframe.groupby([\"id_client\"])[\"mars_semaine_3\"].transform(\"max\")\n",
    "    #dataframe[\"mars_semaine_4\"] = dataframe.groupby([\"id_client\"])[\"mars_semaine_4\"].transform(\"max\")\n",
    "    #dataframe[\"avril_semaine_1\"] = dataframe.groupby([\"id_client\"])[\"avril_semaine_1\"].transform(\"max\")\n",
    "    #dataframe[\"avril_semaine_2\"] = dataframe.groupby([\"id_client\"])[\"avril_semaine_2\"].transform(\"max\")\n",
    "    #dataframe[\"avril_semaine_3\"] = dataframe.groupby([\"id_client\"])[\"avril_semaine_3\"].transform(\"max\")\n",
    "    #dataframe[\"avril_semaine_4\"] = dataframe.groupby([\"id_client\"])[\"avril_semaine_4\"].transform(\"max\")\n",
    "    #dataframe[\"mai_semaine_1\"] = dataframe.groupby([\"id_client\"])[\"mai_semaine_1\"].transform(\"max\")\n",
    "    #dataframe[\"mai_semaine_2\"] = dataframe.groupby([\"id_client\"])[\"mai_semaine_2\"].transform(\"max\")\n",
    "    #dataframe[\"mai_semaine_3\"] = dataframe.groupby([\"id_client\"])[\"mai_semaine_3\"].transform(\"max\")\n",
    "    #dataframe[\"mai_semaine_4\"] = dataframe.groupby([\"id_client\"])[\"mai_semaine_4\"].transform(\"max\")\n",
    "    #dataframe[\"juin_semaine_1\"] = dataframe.groupby([\"id_client\"])[\"juin_semaine_1\"].transform(\"max\")\n",
    "    #dataframe[\"juin_semaine_2\"] = dataframe.groupby([\"id_client\"])[\"juin_semaine_2\"].transform(\"max\")\n",
    "    #dataframe[\"juin_semaine_3\"] = dataframe.groupby([\"id_client\"])[\"juin_semaine_3\"].transform(\"max\")\n",
    "    #dataframe[\"juin_semaine_4\"] = dataframe.groupby([\"id_client\"])[\"juin_semaine_4\"].transform(\"max\")\n",
    "    \n",
    "    \n",
    "    dataframe[\"somme_session\"] = dataframe.groupby([\"id_client\"]).id_session.transform('count')\n",
    "    dataframe = pd.get_dummies(dataframe, columns=[\"type_jour\"])\n",
    "    dataframe[\"somme_session_semaine\"] = dataframe.groupby([\"id_client\"]).type_jour_semaine.transform('sum')\n",
    "    dataframe[\"somme_session_weekend\"] = dataframe.groupby([\"id_client\"]).type_jour_weekend.transform('sum')\n",
    "    #dataframe[\"duree_session_moyenne\"] = dataframe.groupby([\"id_client\"]).duree_session.transform(\"mean\")\n",
    "    dataframe[\"duree_session_min\"] = dataframe.groupby([\"id_client\"]).duree_session.transform(\"min\")\n",
    "    dataframe[\"duree_session_max\"] = dataframe.groupby([\"id_client\"]).duree_session.transform(\"max\")\n",
    "    dataframe[\"duree_session_totale\"] = dataframe.groupby([\"id_client\"]).duree_session.transform(\"sum\")\n",
    "    \n",
    "    dataframe = pd.get_dummies(dataframe, columns=[\"id_site\"], prefix = \"site\")\n",
    "    \n",
    "    #liste_site = [\"site_492987\",\"site_496306\",\"site_496307\",\"site_496838\",\"site_502193\",\"site_509043\",\"site_539121\",\"site_548647\"]\n",
    "    \n",
    "    dataframe[\"site_492987\"] = dataframe.groupby([\"id_client\"])[\"site_492987\"].transform(\"max\")\n",
    "    dataframe[\"site_496306\"] = dataframe.groupby([\"id_client\"])[\"site_496306\"].transform(\"max\")\n",
    "    dataframe[\"site_496307\"] = dataframe.groupby([\"id_client\"])[\"site_496307\"].transform(\"max\")\n",
    "    dataframe[\"site_496838\"] = dataframe.groupby([\"id_client\"])[\"site_496838\"].transform(\"max\")\n",
    "    dataframe[\"site_502193\"] = dataframe.groupby([\"id_client\"])[\"site_502193\"].transform(\"max\")\n",
    "    dataframe[\"site_509043\"] = dataframe.groupby([\"id_client\"])[\"site_509043\"].transform(\"max\")\n",
    "    dataframe[\"site_539121\"] = dataframe.groupby([\"id_client\"])[\"site_539121\"].transform(\"max\")\n",
    "    dataframe[\"site_548647\"] = dataframe.groupby([\"id_client\"])[\"site_548647\"].transform(\"max\")\n",
    "    \n",
    "    \n",
    "    dataframe[\"somme_pages_vues\"] = dataframe.groupby([\"id_client\"]).pages_vues.transform(\"sum\")\n",
    "    #OS managing\n",
    "    dataframe[\"os2\"] = \"autres\"\n",
    "    dataframe.os = dataframe.os.str.lower()\n",
    "    dataframe.loc[dataframe.os.str.match(\".*ios.*\"), \"os2\"] = \"iphone\"\n",
    "    dataframe.loc[dataframe.os.str.match(\".*ipad.*\"), \"os2\"] = \"ipad\"\n",
    "    dataframe.loc[((dataframe.os.str.match(\".*mac os.*\")) | (dataframe.os2.str.match(\".*os x.*\")))  , \"os2\"] = \"mac\"\n",
    "    dataframe.loc[dataframe.os.str.match(\".*windows.*\"), \"os2\"] = \"windows\"\n",
    "    dataframe.loc[dataframe.os.str.match(\".*windows phone.*\"), \"os2\"] = \"windows_phone\"\n",
    "    dataframe.loc[dataframe.os.str.match(\".*android.*\"), \"os2\"] = \"android\"\n",
    "    dataframe.loc[dataframe.os.str.match(\".*linux.*\"), \"os2\"] = \"linux\"\n",
    "    dataframe.loc[dataframe.os.str.match(\".*chrome.*\"), \"os2\"] = \"chrome_os\"\n",
    "    \n",
    "    dataframe = pd.get_dummies(dataframe, columns=[\"os2\"])\n",
    "    dataframe = pd.get_dummies(dataframe, columns= [\"periode\"])\n",
    "    \n",
    "    dataframe[[\"os2_iphone\",\"os2_ipad\",\"os2_mac\",\"os2_windows\",\"os2_windows_phone\",\"os2_android\",\"os2_linux\",\n",
    "              \"os2_chrome_os\", \"os2_autres\"]] = dataframe.groupby([\"id_client\"])[\"os2_iphone\",\"os2_ipad\",\"os2_mac\",\"os2_windows\"\n",
    "                                ,\"os2_windows_phone\",\"os2_android\",\"os2_linux\",\"os2_chrome_os\",\"os2_autres\"].transform(\"max\")\n",
    "    \n",
    "    dataframe[[\"periode_nuit\",\"periode_aube\",\"periode_matinee\",\"periode_midi\",\"periode_apres_midi\",\n",
    "               \"periode_fin_apres_midi\",\"periode_soiree\"]] = dataframe.groupby(['id_client'])[\"periode_nuit\",\n",
    "            \"periode_aube\",\"periode_matinee\",\"periode_midi\",\"periode_apres_midi\",\"periode_fin_apres_midi\",\"periode_soiree\"].transform('max')\n",
    "    \n",
    "    dataframe.drop([\"fileid\",\"id_campagne\",\"id_session\", \"debut_de_session\",\"fin_de_session\", \"pages_vues\", 'localisation_1er_niveau',\n",
    "                    'localisation_2eme_niveau', 'localisation_3eme_niveau', \"terminal\", \"os\", \"duree_session\",\"ClientUserId_y\",\"ClientUserId_x\"],axis=1, inplace=True)\n",
    "    \n",
    "    dataframe = dataframe.drop_duplicates()\n",
    "    #return dataframe\n",
    "    \n",
    "    #df_session = clean_session(dataframe, liste_client)\n",
    "    dataframe.to_pickle(path+\"\\s_dataframe_final_sessions_\"+reg.group(1)+\".pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on rassemble les différents fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataframe= pd.read_pickle(path+\"\\dataframes\\s_dataframe_general_xiti_sessions_mars_juin2015.pkl\")\n",
    "path = \"W:\\L_equipe\\dataframes\\\\\"\n",
    "final = pd.DataFrame()\n",
    "files  = []\n",
    "files.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                        if f.startswith('s_dataframe_final_sessions_') if getsize(join(path,f))>0])\n",
    "\n",
    "liste_semaines = [\"semaine1\",\"semaine2\",\"semaine3\",\"semaine4\",\"semaine5\",\"semaine6\",\"semaine7\",\"semaine8\",\"semaine9\",\"semaine10\",\n",
    "             \"semaine11\",\"semaine12\"]\n",
    "liste_mois = [\"mois1\",\"mois2\",\"mois3\"]\n",
    "\n",
    "final= pd.concat(( pd.read_pickle(f) for f in files))\n",
    "\n",
    "#final.drop([\"site_555493\",\"site_40086\",\"site_498703\",\"site_413580\",'site_499304'],axis=1, inplace=True)\n",
    "final.os2_autres = final.groupby(['id_client']).os2_autres.transform(max)\n",
    "\n",
    "final[[\"Paris\", \"RP\", \"France\"]] = final.groupby([\"id_client\"])[\"Paris\",\"RP\",'France'].transform(\"max\")\n",
    "final[[\"os2_iphone\",\"os2_ipad\",\"os2_mac\",\"os2_windows\",\"os2_windows_phone\",\"os2_android\",\"os2_linux\",\n",
    "              \"os2_chrome_os\", \"os2_autres\"]] = final.groupby([\"id_client\"])[\"os2_iphone\",\"os2_ipad\",\"os2_mac\",\"os2_windows\"\n",
    "                                ,\"os2_windows_phone\",\"os2_android\",\"os2_linux\",\"os2_chrome_os\",\"os2_autres\"].transform(\"max\")\n",
    "final[[\"periode_nuit\",\"periode_aube\",\"periode_matinee\",\"periode_midi\",\"periode_apres_midi\",\n",
    "               \"periode_fin_apres_midi\",\"periode_soiree\"]] = final.groupby(['id_client'])[\"periode_nuit\",\n",
    "            \"periode_aube\",\"periode_matinee\",\"periode_midi\",\"periode_apres_midi\",\"periode_fin_apres_midi\",\"periode_soiree\"].transform('max')\n",
    "\n",
    "final[\"site_492987\"] = final.groupby([\"id_client\"])[\"site_492987\"].transform(\"max\")\n",
    "final[\"site_496306\"] = final.groupby([\"id_client\"])[\"site_496306\"].transform(\"max\")\n",
    "final[\"site_496307\"] = final.groupby([\"id_client\"])[\"site_496307\"].transform(\"max\")\n",
    "final[\"site_496838\"] = final.groupby([\"id_client\"])[\"site_496838\"].transform(\"max\")\n",
    "final[\"site_502193\"] = final.groupby([\"id_client\"])[\"site_502193\"].transform(\"max\")\n",
    "final[\"site_509043\"] = final.groupby([\"id_client\"])[\"site_509043\"].transform(\"max\")\n",
    "final[\"site_539121\"] = final.groupby([\"id_client\"])[\"site_539121\"].transform(\"max\")\n",
    "final[\"site_548647\"] = final.groupby([\"id_client\"])[\"site_548647\"].transform(\"max\")\n",
    " \n",
    "#final[\"site_40086\"] = final.groupby(['id_client'])[\"site_40086\"].transform('max')\n",
    "#final[\"site_498703\"] = final.groupby(['id_client'])[\"site_498703\"].transform('max')\n",
    "#final[\"site_413580\"] = final.groupby(['id_client'])[\"site_413580\"].transform('max')\n",
    "#final['site_499304'] = final.groupby(['id_client'])[\"site_499304\"].transform('max')\n",
    "\n",
    "final[\"duree_session_max\"] = final.groupby([\"id_client\"]).duree_session_max.transform(\"max\")\n",
    "final[liste_semaines] = final.groupby([\"id_client\"])[liste_semaines].transform(max)\n",
    "final[liste_mois] = final.groupby([\"id_client\"])[liste_mois].transform(max)\n",
    "\n",
    "final[['type_jour_semaine','type_jour_weekend']] = final.groupby(['id_client'])['type_jour_semaine','type_jour_weekend'].transform(max)\n",
    "final[\"duree_session_min\"] = final.groupby([\"id_client\"]).duree_session_min.transform(\"min\")\n",
    "final = final.drop_duplicates()\n",
    "final['somme_pages_vues'] = final.groupby(['id_client'])['somme_pages_vues'].transform(sum)\n",
    "final['somme_session'] = final.groupby(['id_client'])['somme_session'].transform(sum)\n",
    "final['somme_session_semaine'] = final.groupby(['id_client'])['somme_session_semaine'].transform(sum)\n",
    "final['somme_session_weekend'] = final.groupby(['id_client'])['somme_session_weekend'].transform(sum)\n",
    "final['duree_session_totale'] =final.groupby(['id_client'])['duree_session_totale'].transform(sum)\n",
    "\n",
    "final = final.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final.to_pickle(path+\"\\s_dataframe_final_sessions.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#création de la liste de la popolation a sélectionner dans page et sessions\n",
    "df = pd.read_pickle(path+\"\\s_dataframe_final_clients.pkl\")\n",
    "df1 = pd.read_pickle(path+\"\\s_dataframe_final_sub_payant_juin_2015.pkl\")\n",
    "liste_pop = df.id_client.unique().tolist()\n",
    "liste_sub_juin = df1.ClientUserId.unique().tolist()\n",
    "set_pop=set(liste_pop)\n",
    "set_sub_juin = set(liste_sub_juin)\n",
    "set_pop.update(set_sub_juin)  #liste d'id unique contenant notre population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#premiere étape clean chaque fichiers page et fichier session avec le cut sur notre population \n",
    "path = \"W:\\L_equipe\\dataframes\\\\\"\n",
    "dataframe = pd.DataFrame()\n",
    "files  = []\n",
    "files.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                        if f.startswith('s_dataframe_general_xiti_pages') if getsize(join(path,f))>0])\n",
    "\n",
    "for f in files :\n",
    "    reg = re.match(('.*pages_(.*)'), f)\n",
    "    dataframe = pd.read_pickle(f)\n",
    "    dataframe = dataframe.loc[dataframe.id_site.isin([492987,496306,496307,496838,502193,509043,539121,548647])]\n",
    "    dataframe = dataframe.loc[dataframe.id_client.isin(set_pop)]\n",
    "    dataframe.to_pickle(path+\"s_dataframe_temp_pages_payant_\"+reg.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deuxieme étape (à faire car le cut n'est pas suffisant) on group by et on refera une étape de groupby par la suite\n",
    "path = \"W:\\L_equipe\\dataframes\\\\\"\n",
    "files  = []\n",
    "files.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                        if f.startswith('s_dataframe_temp_pages_population') if getsize(join(path,f))>0])\n",
    "\n",
    "for f in files :\n",
    "    reg = re.match(('.*pages_population_(.*)\\.pkl'), f)\n",
    "    dataframe = pd.read_pickle(f)\n",
    "    \n",
    "    dataframe = pd.get_dummies(dataframe, columns=[\"id_site\"], prefix=\"id_site\")\n",
    "    #normalisation de niveau_2\n",
    "    dataframe.niveau_2 = dataframe.niveau_2.str.lower()\n",
    "    dataframe.niveau_2 = dataframe.niveau_2.str.normalize('NFKD')\n",
    "    dataframe.niveau_2 = dataframe.niveau_2.str.encode('ASCII', 'ignore')\n",
    "    type_sports = pd.read_csv(r\"W:\\L_Equipe\\types_sports.csv\",sep=\";\")\n",
    "    dataframe = dataframe.merge(type_sports,how='left', left_on='niveau_2', right_on='niveau_2')\n",
    "    #dummies\n",
    "    dataframe = pd.get_dummies(dataframe, columns= [\"theme\"], prefix='theme')\n",
    "    #groupby\n",
    "    dataframe[\"nb_session\"] = dataframe.groupby([\"id_client\"]).id_session.transform(\"count\")\n",
    "    \n",
    "    dataframe[\"nb_pages_par_session\"] = dataframe.groupby([\"fileid\", \"id_client\",\"id_session\"]).position_de_la_page.transform(\"max\")\n",
    "    dataframe['nb_pages_moyen'] = dataframe.groupby([\"id_client\"]).nb_pages_par_session.transform('mean')\n",
    "    dataframe['nb_pages_min'] = dataframe.groupby([\"id_client\"]).nb_pages_par_session.transform('min')\n",
    "    dataframe['nb_pages_max'] = dataframe.groupby([\"id_client\"]).nb_pages_par_session.transform('mean')\n",
    "    dataframe['nb_pages_total'] = dataframe.groupby([\"id_client\"]).nb_pages_par_session.transform('sum')\n",
    "    \n",
    "    dataframe[\"duree_session\"] = dataframe.groupby([\"fileid\", \"id_client\",\"id_session\"]).temps_passe_sur_la_page.transform(\"sum\")\n",
    "    dataframe[\"duree_session_moyenne\"] = dataframe.groupby([\"id_client\"]).duree_session.transform('mean')\n",
    "    dataframe[\"duree_session_max\"] = dataframe.groupby([\"id_client\"]).duree_session.transform('max')\n",
    "    dataframe[\"duree_session_min\"] = dataframe.groupby([\"id_client\"]).duree_session.transform('min')\n",
    "    \n",
    "    sites = [col for col in list(dataframe) if col.startswith(\"id_site\")]\n",
    "\n",
    "    themes = [col for col in list(dataframe) if col.startswith('theme')]\n",
    "    \n",
    "    dataframe[sites] = dataframe.groupby([\"id_client\"])[sites].transform(\"max\")\n",
    "    dataframe[themes] = dataframe.groupby([\"id_client\"])[themes].transform(\"max\")\n",
    "    \n",
    "    dataframe.drop([\"url\", \"fileid\",\"position_de_la_page\",\"temps_passe_sur_la_page\", \"niveau_2\",\"id_session\"\n",
    "                    ,\"nb_pages_par_session\",\"duree_session\",\"index\"],axis=1, inplace=True)\n",
    "    \n",
    "    dataframe = dataframe.drop_duplicates()\n",
    "    #dataframe = dataframe.rename(columns = lambda x : reg.group(1) +'_'+ x)\n",
    "    dataframe.to_pickle(path+\"\\s_dataframe_temp2_pages_population_\"+reg.group(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final groupby pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"W:\\L_equipe\\dataframes\\\\\"\n",
    "files  = []\n",
    "dataframe = pd.DataFrame()\n",
    "files.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                        if f.startswith('s_dataframe_temp2_pages_population') if getsize(join(path,f))>0])\n",
    "\n",
    "dataframe = pd.concat((pd.read_pickle(f) for f in files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe.sort_values(by='id_client')\n",
    "listemax = [\"duree_session_max\",\"id_site_492987\",\"id_site_492987.0\",\"id_site_496306\",\"id_site_496306.0\",\n",
    "            \"id_site_496307\",\"id_site_496307.0\",\"id_site_496838\",\"id_site_496838.0\",\"id_site_502193\",\n",
    "            \"id_site_502193.0\",\"id_site_509043\",\"id_site_509043.0\",\"id_site_539121\",\"id_site_539121.0\",\n",
    "            \"id_site_548647\",\"id_site_548647.0\",\"nb_pages_max\",\"theme_athletisme\",\"theme_auto-moto\",\n",
    "            \"theme_autres\",\"theme_basket\",\"theme_blogs\",\"theme_cyclisme\",\"theme_e-boutiques\",\"theme_football\",\n",
    "            \"theme_general\",\"theme_golf\",\"theme_handball\",\"theme_hippisme\",\"theme_natation\",\"theme_rugby\",\n",
    "            \"theme_ski-glace\",\"theme_souscription\",\"theme_tennis\",\"theme_vente en ligne\",\"theme_video\",\"theme_voile\"]\n",
    "\n",
    "listemin = [\"nb_pages_min\",\"duree_session_min\"]\n",
    "dataframe[listemin] = dataframe.groupby(['id_client'])[listemin].transform(min)\n",
    "dataframe[listemax] = dataframe.groupby([\"id_client\"])[listemax].transform(max)\n",
    "\n",
    "dataframe = dataframe.drop_duplicates()\n",
    "\n",
    "dataframe.drop([\"duree_session_moyenne\",\"nb_pages_moyen\"], axis=1, inplace=True)\n",
    "dataframe.nb_pages_total = dataframe.groupby(['id_client']).nb_pages_total.transform(sum)\n",
    "dataframe.nb_session = dataframe.groupby(['id_client']).nb_session.transform(sum)\n",
    "dataframe = dataframe.drop_duplicates()\n",
    "dataframe.to_pickle(path+\"\\s_dataframe_final_page.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame()\n",
    "\n",
    "path = \"W:\\L_equipe\\dataframes\\\\\"\n",
    "client =pd.read_pickle(path+\"\\s_dataframe_final_clients_mam_pas_forcmnt_juin.pkl\")\n",
    "pages = pd.read_pickle(path+\"\\s_dataframe_final_page.pkl\")\n",
    "sessions = pd.read_pickle(path+\"\\s_dataframe_final_sessions.pkl\")\n",
    "#commandes = pd.read_pickle(path+\"\\s_dataframe_final_commandes.pkl\")\n",
    "sub_juin = pd.read_pickle(path+\"\\s_dataframe_final_sub_payant_juin_2015.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe = pages.merge(client,how='inner', left_on='id_client', right_on='id_client')\n",
    "dataframe = dataframe.merge(sessions, how=\"inner\", left_on='id_client', right_on='id_client')\n",
    "#dataframe = dataframe.merge(commandes, how=\"inner\", left_on='id_client', right_on='ClientUserId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "liste_client = client.id_client.unique()\n",
    "liste_pages = pages.id_client.unique()\n",
    "liste_sessions = sessions.id_client.unique()\n",
    "#liste_commandes = commandes.ClientUserId.unique()\n",
    "liste_sub_juin = sub_juin.ClientUserId.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille de l'intersection sub_juin sessions :  399\n",
      "taille de l'intersection sub_juin client :  402\n",
      "taille de l'intersection sub_juin pages :  402\n"
     ]
    }
   ],
   "source": [
    "s=set(liste_sub_juin)\n",
    "t=set(liste_pages)\n",
    "u=set(liste_sessions)\n",
    "#v=set(liste_commandes)\n",
    "w=set(liste_client)\n",
    "print \"taille de l'intersection sub_juin sessions : \", len(s.intersection(u))\n",
    "#print \"taille de l'intersection sub_juin commandes : \", len(s.intersection(v))\n",
    "print \"taille de l'intersection sub_juin client : \", len(s.intersection(w))\n",
    "print \"taille de l'intersection sub_juin pages : \", len(s.intersection(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#derniers drops et ajustements\n",
    "dataframe.drop([\"date_observation_max\",\"date_observation_min\"], axis=1, inplace=True)\n",
    "dataframe.duree_session_totale = dataframe.duree_session_totale.dt.seconds\n",
    "\n",
    "#drop for too high corelation\n",
    "#dataframe.drop([\"%pages_vues_site_492987\",\"%pages_vues_site_496306\",\"%pages_vues_site_496307\",\"%pages_vues_site_496838\",\n",
    "#                \"%pages_vues_site_502193\",\"%pages_vues_site_509043\",\"%pages_vues_site_539121\",\"%pages_vues_site_548647\",\n",
    "#                \"ClientUserId\",\"cadeau\",\"duree_session_max_y\",\"duree_session_min_y\",\"site_496306\",\"site_496307\",\n",
    "#                \"site_496838\",\"site_502193\",\"site_509043\",\"site_539121\",\"site_492987\",\"site_548647\",\"site_40086\",\n",
    "#                \"site_413580\",\"site_498703\",\"site_499304\",\"site_555493\",\"somme_pages_vues\",\n",
    "#                \"id_site_492987.0\",\"id_site_496306.0\",\"id_site_496307.0\",\"id_site_496838.0\",\"id_site_502193.0\",\n",
    "#                \"id_site_509043.0\",\"id_site_539121.0\",\"id_site_548647.0\"], axis=1, inplace=True)\n",
    "#\n",
    "dataframe.drop([\"%pages_vues_site_492987\",\"%pages_vues_site_496306\",\"%pages_vues_site_496307\",\"%pages_vues_site_496838\",\n",
    "                \"%pages_vues_site_502193\",\"%pages_vues_site_509043\",\"%pages_vues_site_539121\",\"%pages_vues_site_548647\",\n",
    "                \"duree_session_max_y\",\"duree_session_min_y\",\"site_496306\",\"site_496307\",\n",
    "                \"site_496838\",\"site_502193\",\"site_509043\",\"site_539121\",\"site_492987\",\"site_548647\",\"site_40086\",\n",
    "                \"site_413580\",\"site_498703\",\"site_499304\",\"site_555493\",\"somme_pages_vues\",\n",
    "                \"id_site_492987.0\",\"id_site_496306.0\",\"id_site_496307.0\",\"id_site_496838.0\",\"id_site_502193.0\",\n",
    "                \"id_site_509043.0\",\"id_site_539121.0\",\"id_site_548647.0\"], axis=1, inplace=True)\n",
    "\n",
    "#high corelation pas sur \n",
    "#dataframe.drop([\"somme_pages_vues_totale\",\"somme_session\",\"recence_souscripteur\",\"somme_mensualites\",\n",
    "#                \"nb_articles\",\"nb_abonnements\"], axis=1, inplace=True)\n",
    "#\n",
    "#drop \n",
    "#dataframe.drop([\"anciennete_souscripteur\",\"duree_abonnement_theorique\",\"duree_cadeau\",\"mensualite_moyenne\",\n",
    "#               \"nb_mensualites\",\"occasion\",\"prix_mensuel\",\"prix_mensuel_cadeau\",\"prod_type_Service\",\n",
    "#                \"somme_paiement_totale\"], axis=1, inplace=True)\n",
    "\n",
    "dataframe[\"multidevice\"] = dataframe.id_site_492987 + dataframe.id_site_496306 + dataframe.id_site_496307\\\n",
    "                + dataframe.id_site_496838 + dataframe.id_site_502193 + dataframe.id_site_509043 + dataframe.id_site_539121\\\n",
    "                + dataframe.id_site_548647\n",
    "#juste pour les semaines\n",
    "dataframe.fillna(0, inplace=True)\n",
    "dataframe = dataframe.drop_duplicates()\n",
    "#dataframe.drop([\"id_client\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    123084\n",
       "1.0       399\n",
       "Name: abo_juin, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.abo_juin.value_counts()\n",
    "#dataframe.loc[dataframe.id_client == 178626.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe.to_pickle(path+\"\\dataframe_final_sans_commande.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123383, 90)\n"
     ]
    }
   ],
   "source": [
    "path = \"W:\\L_equipe\\dataframes\\\\\"\n",
    "dataframe = pd.read_pickle(path+\"\\dataframe_final_sans_commande.pkl\")\n",
    "#dataframe = pd.pd.read_pickle(path+\"\\dataframe_final.pkl\")\n",
    "non_abo = dataframe.loc[dataframe.abo_juin == 0].sample(n = 50)\n",
    "abo = dataframe.loc[dataframe.abo_juin == 1].sample(n=50)\n",
    "reste = dataframe.loc[(~dataframe.id_client.isin(non_abo.id_client)& ~dataframe.id_client.isin(abo.id_client))]\n",
    "reste_nonabo = reste.loc[dataframe.abo_juin == 0]\n",
    "reste_abo = reste.loc[dataframe.abo_juin ==1]\n",
    "framestest = [reste_nonabo, reste_abo]\n",
    "frames = [non_abo,abo]\n",
    "dataframe_final = pd.concat(frames)\n",
    "dataframe_final_test = pd.concat(framestest)\n",
    "dataframe_final.to_pickle(path+\"\\dataframe_sans_commande_ready_to_use_50_50.pkl\")\n",
    "print reste.shape\n",
    "dataframe_final_test.to_pickle(path+\"\\dataframe_sans_commande_test_50_50_full_non_abo.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = dataframe.loc[dataframe.abo_juin == 0]\n",
    "test = test.sample(n = 10000)\n",
    "test.to_pickle(path+\"test_pour_voir.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes, tests, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean datetime\n",
    "#df_commande_clean['OrderDate'] = pd.to_datetime(df_commande_clean['OrderDate'])\n",
    "#df_compte_clean['LastUpdated'] = pd.to_datetime(df_compte_clean['LastUpdated'])\n",
    "#df_compte_clean['CreateDate'] = pd.to_datetime(df_compte_clean['CreateDate'])\n",
    "\n",
    "\n",
    "df_session_juin['debut_de_session'] = pd.to_datetime(df_session_juin['debut_de_session'])\n",
    "df_session_juin['fin_de_session'] = pd.to_datetime(df_session_juin['fin_de_session'])\n",
    "\n",
    "\n",
    "#df_client_clean.premiere_v = pd.to_datetime(df_client_clean['premiere_v'])\n",
    "#df_client_clean.derniere_v = pd.to_datetime(df_client_clean['derniere_v'])\n",
    "\n",
    "#\n",
    "df_session_juin[\"temps_session\"] =  df_session_juin[\"fin_de_session\"] - df_session_juin[\"debut_de_session\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#création de différents indicateurs\n",
    "df_session_juin.loc[df_session_juin['localisation_3eme_niveau'] == 'Paris', 'Paris'] = 1\n",
    "df_session_juin.loc[df_session_juin['localisation_3eme_niveau'] != 'Paris', 'Paris'] = 0\n",
    "\n",
    "df_session_juin.loc[df_session_juin['localisation_2eme_niveau'] == 'Ile-de-France', 'RP'] = 1\n",
    "df_session_juin.loc[df_session_juin['localisation_2eme_niveau'] != 'Ile-de-France', 'RP'] = 0\n",
    "\n",
    "df_session_juin.loc[df_session_juin['localisation_1er_niveau'] == 'France', 'France'] = 1\n",
    "df_session_juin.loc[df_session_juin['localisation_1er_niveau'] != 'France', 'France'] = 0\n",
    "\n",
    "\n",
    "df_session_juin.loc[(df_session_juin['debut_de_session'] < date(2015, 6, 7)), 'juin_semaine_1'] = 1\n",
    "#df_session_juin_clean.loc[(df_session_juin_clean['debut_de_session'] > date(2015, 6, 7)), 'juin_semaine_1'] = 0\n",
    "\n",
    "df_session_juin.loc[(df_session_juin['debut_de_session'] < date(2015, 6, 14) ) \n",
    "                          & (df_session_juin['debut_de_session'] > date(2015, 6, 6)), 'juin_semaine_2'] = 1\n",
    "#df_session_juin_clean.loc[(df_session_juin_clean['debut_de_session'] < date(2015, 6, 7) ) \n",
    "#                          & (df_session_juin_clean['debut_de_session'] > date(2015, 5, 31)), 'juin_semaine_1'] = 1\n",
    "\n",
    "df_session_juin.loc[(df_session_juin['debut_de_session'] < date(2015, 6, 21) ) \n",
    "                          & (df_session_juin['debut_de_session'] > date(2015, 6, 13)), 'juin_semaine_3'] = 1\n",
    "\n",
    "df_session_juin.loc[(df_session_juin['debut_de_session'] > date(2015, 6, 20) ), 'juin_semaine_4'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_session_juin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#travail sur les dates des clients : \n",
    "df_client_clean[\"anciennete\"] = (date(2015, 6, 30) - df_client_clean[\"premiere_v\"])\n",
    "df_client_clean[\"derniere_visite\"] = (date(2015, 6, 30) - df_client_clean[\"derniere_v\"])\n",
    "df_client_clean.anciennete = pd.to_numeric(df_client_clean.anciennete)/(1000000000 * 60 * 60 * 24)\n",
    "df_client_clean.derniere_visite = pd.to_numeric(df_client_clean.derniere_visite)/(1000000000 * 60 * 60 * 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "#df_session_juin[\"debut_de_session\"].groupby(df_session_juin[\"debut_de_session\"].dt.hour).count().plot(kind=\"bar\")\n",
    "df_session_juin[\"debut_de_session\"].groupby(df_session_juin[\"debut_de_session\"].dt.hour).count().plot(kind=\"bar\")\n",
    "plt.savefig(\"debut_de_session_juin2015.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_commande.rename(columns = {'Somme_NetAmmount':'Somme_NetAmount'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (df_pages_juin_p1.theme_page.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différence des abonnés dans comande et dans sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s=set(liste_nouveaux_sub_commande_2015)\n",
    "t=set(liste_nouveaux_sub_2015)\n",
    "print \"taille de sub commande\" , len(s)\n",
    "print \"taille de sub\", len(t)\n",
    "print \"commande est il subset de sub ?\", s.issubset(t)#\ts <= t\ttest whether every element in s is in t\n",
    "print \"commande est il superset de sub ?\", s.issuperset(t)#\ts >= t\ttest whether every element in t is in s\n",
    "print \"taille de l'intersection\", len(s.intersection(t))#\ts & t\tnew set with elements common to s and t\n",
    "print \"elements dans commande mais pas dans sub\" , len(s.difference(t))# \ts - t\tnew set with elements in s but not in t\n",
    "print \"elements dans sub mais pas dans commande\" , len(t.difference(s))\n",
    "#s.symmetric_difference(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liste_nouveaux_sub_commande_2015_2 = df_commande.ClientUserId.unique()\n",
    "liste_nouveaux_sub_2015_2 = df_prod42_test.ClientUserId.unique()\n",
    "s=set(liste_nouveaux_sub_commande_2015_2)\n",
    "t=set(liste_nouveaux_sub_2015_2)\n",
    "print \"taille de sub commande : \" , len(s)\n",
    "print \"taille de sub : \", len(t)\n",
    "print \"commande est il subset de sub ?\", s.issubset(t)#\ts <= t\ttest whether every element in s is in t\n",
    "print \"commande est il superset de sub ?\", s.issuperset(t)#\ts >= t\ttest whether every element in t is in s\n",
    "print \"taille de l'intersection : \", len(s.intersection(t))#\ts & t\tnew set with elements common to s and t\n",
    "print \"elements dans commande mais pas dans sub : \" , len(s.difference(t))# \ts - t\tnew set with elements in s but not in t\n",
    "print \"elements dans sub mais pas dans commande : \" , len(t.difference(s))\n",
    "print \"\\n\"\n",
    "\n",
    "liste_nouveaux_sub_commande_juin_2015_2 = df_comm_juin.ClientUserId.unique()\n",
    "liste_nouveaux_sub_juin_2015_2 = nouveaux_sub_juin.ClientUserId.unique()\n",
    "\n",
    "s=set(liste_nouveaux_sub_commande_juin_2015_2)\n",
    "t=set(liste_nouveaux_sub_juin_2015_2)\n",
    "print \"taille de sub commande : \" , len(s)\n",
    "print \"taille de sub : \", len(t)\n",
    "print \"commande est il subset de sub ?\", s.issubset(t)#\ts <= t\ttest whether every element in s is in t\n",
    "print \"commande est il superset de sub ?\", s.issuperset(t)#\ts >= t\ttest whether every element in t is in s\n",
    "print \"taille de l'intersection : \", len(s.intersection(t))#\ts & t\tnew set with elements common to s and t\n",
    "print \"elements dans commande mais pas dans sub : \" , len(s.difference(t))# \ts - t\tnew set with elements in s but not in t\n",
    "print \"elements dans sub mais pas dans commande : \" , len(t.difference(s))\n",
    "\n",
    "#s.symmetric_difference(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Préparation Kick-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble des données prod42\n",
      "min sub creation 2014-04-23 15:53:59\n",
      "max sub creation 2015-07-14 23:05:09\n",
      "min last update 2014-07-21 00:00:19\n",
      "max last update 2015-07-14 23:47:55\n",
      "\n",
      "\n",
      "sites de l'équipe pour prod42\n",
      "min sub creation 2014-04-28 09:52:00\n",
      "max sub creation 2015-07-14 23:05:09\n",
      "min last update 2014-07-21 00:00:19\n",
      "max last update 2015-07-14 23:39:33\n"
     ]
    }
   ],
   "source": [
    "dfp = pd.read_pickle(path+\"\\s_dataframe_general_vel_PROD42.pkl\")\n",
    "dfp['SubscriptionCreated'] = pd.to_datetime(dfp['SubscriptionCreated'], format='%d/%m/%Y %H:%M:%S')\n",
    "dfp['SubscriptionLastUpdated'] = pd.to_datetime(dfp['SubscriptionLastUpdated'], format='%d/%m/%Y %H:%M:%S')\n",
    "dfp['ServiceExpiry'] = pd.to_datetime(dfp['ServiceExpiry'], format='%d/%m/%Y %H:%M:%S')\n",
    "print 'ensemble des données prod42 :'\n",
    "print \"min sub creation\" ,dfp.SubscriptionCreated.min()\n",
    "print \"max sub creation\" ,dfp.SubscriptionCreated.max()\n",
    "print 'min last update',dfp.SubscriptionLastUpdated.min()\n",
    "print 'max last update' ,dfp.SubscriptionLastUpdated.max()\n",
    "print '\\n'\n",
    "print \"sites de l'équipe pour prod42 :\"\n",
    "dfp = dfp.loc[dfp['ClientUserId'].isin(liste_client)]\n",
    "print \"min sub creation\" ,dfp.SubscriptionCreated.min()\n",
    "print \"max sub creation\" ,dfp.SubscriptionCreated.max()\n",
    "print 'min last update',dfp.SubscriptionLastUpdated.min()\n",
    "print 'max last update' ,dfp.SubscriptionLastUpdated.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble des données client :\n",
      "min fileid 2013-12-03\n",
      "max fileid 2015-06-30\n",
      "\n",
      "\n",
      "sites de l'équipe pour client :\n",
      "min fileid 2013-12-03\n",
      "max fileid 2015-06-30\n"
     ]
    }
   ],
   "source": [
    "dfc = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_clients.pkl\")\n",
    "dfc['date'] = dfc['fileid'].str.extract(('.+_(.+)\\.csv'))\n",
    "\n",
    "print 'ensemble des données client :'\n",
    "print \"min fileid\" ,dfc.date.min()[:4]+\"-\"+dfc.date.min()[4:6]+\"-\"+dfc.date.min()[6:]\n",
    "print \"max fileid\" ,dfc.date.max()[:4]+\"-\"+dfc.date.max()[4:6]+\"-\"+dfc.date.max()[6:]\n",
    "print '\\n'\n",
    "print \"sites de l'équipe pour client :\"\n",
    "dfc = dfc.loc[dfc['id_client'].isin(liste_client)]\n",
    "print \"min fileid\" ,dfc.date.min()[:4]+\"-\"+dfc.date.min()[4:6]+\"-\"+dfc.date.min()[6:]\n",
    "print \"max fileid\" ,dfc.date.max()[:4]+\"-\"+dfc.date.max()[4:6]+\"-\"+dfc.date.max()[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les sites de l'equipe <br/>\n",
    "date de premiere visite min 2013-12-02 00:00:00<br/>\n",
    "date de premiere visite max 2015-06-30 00:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble des données commande : \n",
      "min orderdate 2015-04-07 00:08:52\n",
      "max orderdate 2015-07-05 23:56:57\n",
      "\n",
      "\n",
      "sites de l'équipe pour commande : \n",
      "min fileid 2015-04-07 00:08:52\n",
      "max fileid 2015-07-05 23:56:57\n"
     ]
    }
   ],
   "source": [
    "#dfco= pd.read_pickle(path+\"\\s_dataframe_general_vel_commande.pkl\")\n",
    "dfco['OrderDate'] = pd.to_datetime(dfco['OrderDate'], format='%d/%m/%Y %H:%M:%S')\n",
    "print 'ensemble des données commande : '\n",
    "print \"min orderdate\", dfco.OrderDate.min()\n",
    "print 'max orderdate', dfco.OrderDate.max()\n",
    "print '\\n'\n",
    "print \"sites de l'équipe pour commande : \"\n",
    "dfco = dfco.loc[dfco['ClientUserId'].isin(liste_client)]\n",
    "print \"min fileid\" ,dfco.OrderDate.min()\n",
    "print \"max fileid\" ,dfco.OrderDate.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ensemble des données pages<br/>\n",
    "date min : 2013-09-09<br/>\n",
    "date max : 2015-07-08\n",
    "\n",
    "ensembe des données sessions<br/>\n",
    "date min : 2013-09-09<br/>\n",
    "date max : 2015-06-30<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble des données souscription :\n",
      "min sub creation 2014-09-20 21:02:27\n",
      "max sub creation 2015-07-05 21:53:22\n",
      "min last update 2015-04-07 00:01:32\n",
      "max last update 2015-07-05 23:56:58\n",
      "\n",
      "\n",
      "sites de l'équipe pour souscription :\n",
      "min sub creation 2014-09-20 21:02:27\n",
      "max sub creation 2015-07-05 21:46:22\n",
      "min last update 2015-04-07 00:01:32\n",
      "max last update 2015-07-05 23:56:58\n"
     ]
    }
   ],
   "source": [
    "dfp= pd.read_pickle(path+\"\\s_dataframe_general_vel_souscription.pkl\")\n",
    "dfp['SubscriptionCreated'] = pd.to_datetime(dfp['SubscriptionCreated'], format='%d/%m/%Y %H:%M:%S')\n",
    "dfp['SubscriptionLastUpdated'] = pd.to_datetime(dfp['SubscriptionLastUpdated'], format='%d/%m/%Y %H:%M:%S')\n",
    "dfp['ServiceExpiry'] = pd.to_datetime(dfp['ServiceExpiry'], format='%d/%m/%Y %H:%M:%S')\n",
    "print 'ensemble des données souscription :'\n",
    "print \"min sub creation\" ,dfp.SubscriptionCreated.min()\n",
    "print \"max sub creation\" ,dfp.SubscriptionCreated.max()\n",
    "print 'min last update',dfp.SubscriptionLastUpdated.min()\n",
    "print 'max last update' ,dfp.SubscriptionLastUpdated.max()\n",
    "print '\\n'\n",
    "print \"sites de l'équipe pour souscription :\"\n",
    "dfp = dfp.loc[dfp['ClientUserId'].isin(liste_client)]\n",
    "print \"min sub creation\" ,dfp.SubscriptionCreated.min()\n",
    "print \"max sub creation\" ,dfp.SubscriptionCreated.max()\n",
    "print 'min last update',dfp.SubscriptionLastUpdated.min()\n",
    "print 'max last update' ,dfp.SubscriptionLastUpdated.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
