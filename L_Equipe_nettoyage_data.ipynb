{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join,getsize,isdir\n",
    "import csv\n",
    "import numpy\n",
    "import cPickle as pickle\n",
    "import json\n",
    "from random import shuffle\n",
    "import warnings\n",
    "import time\n",
    "#import msgpack\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "import math\n",
    "import pylab\n",
    "import pandas_profiling\n",
    "import unicodedata\n",
    "\n",
    "#n'affiche pas les warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#affiche la totalité des colonnes du dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "#path = \"C:\\Users\\Data Science 5\\Desktop\\L_equipe\"\n",
    "path = \"W:\\L_equipe\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chargement des dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_client = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_clients.pkl\")\n",
    "#df_prod42 = pd.read_pickle(path+\"\\s_dataframe_general_vel_PROD42.pkl\")\n",
    "#df_client_clean = pd.read_pickle(path+\"\\s_dataframe_client_clean_final.pkl\")\n",
    "df_client = pd.read_pickle(r\"W:\\L_equipe\\df_client_true_site.pkl\")\n",
    "#dataframe = pd.read_pickle(r\"C:\\Users\\Data Science 5\\Desktop\\L_equipe\\df_client_true_site.pkl\")\n",
    "\n",
    "#df_compte = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_vel_compte.pkl\")\n",
    "#df_commande = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_vel_commande.pkl\")\n",
    "#df_prod42_clean = pd.read_pickle(path+\"\\s_dataframe_prod42_clean.pkl\")\n",
    "#df_compte_clean = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_compte_clean.pkl\")\n",
    "#df_commande_clean = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_commande_clean.pkl\")\n",
    "#df_pages_juin_p1 = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_pages_juin2015_partie1.pkl\")\n",
    "#df_pages_juin_p1 = pd.read_pickle(\"s_dataframe_pages_juin_partie1_clean.pkl\")\n",
    "#df_session_juin = pd.read_pickle(path+\"\\s_dataframe_general_xiti_sessions_juin2015.pkl\")\n",
    "#df_session_juin_clean = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_session_juin2015_clean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206.353442363 seconds\n"
     ]
    }
   ],
   "source": [
    "#on ne garde que les sites de l'équipe et les client qui sont dans la base Client\n",
    "#----------------------------------Sites de l'equipe ----------------------------------\n",
    "start_time = time.clock()\n",
    "df_client = pd.read_pickle(r\"W:\\L_equipe\\df_client_true_site.pkl\")\n",
    "\n",
    "def nettoyage_site(dataframe) :\n",
    "    dataframe = dataframe.loc[dataframe['id_site'].isin([492987,496306,496307,496838,502193,509043,539121,548647])]\n",
    "    liste_client = dataframe['id_client'].tolist()\n",
    "    return dataframe, liste_client \n",
    "\n",
    "df_client, liste_client = nettoyage_site(df_client)\n",
    "print time.clock() - start_time, 'seconds'\n",
    "#save\n",
    "#df_client.to_pickle(r\"C:\\Users\\Data Science 5\\Desktop\\L_equipe\\df_client_true_site.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#--------------------nettoyage nouveaux abonnés--------------------------\n",
    "\n",
    "df_prod42 = pd.read_pickle(path+\"\\dataframes\\s_dataframe_general_vel_PROD42.pkl\")\n",
    "\n",
    "def nettoyage_sub(dataframe, liste_client) :\n",
    "    #selection des abonnés de l'equipe\n",
    "    dataframe = dataframe.loc[dataframe['ClientUserId'].isin(liste_client)]\n",
    "    #formatage datetime \n",
    "    dataframe['SubscriptionCreated'] = pd.to_datetime(dataframe['SubscriptionCreated'], format='%d/%m/%Y %H:%M:%S')\n",
    "    dataframe['SubscriptionLastUpdated'] = pd.to_datetime(dataframe['SubscriptionLastUpdated'], format='%d/%m/%Y %H:%M:%S')\n",
    "    dataframe['ServiceExpiry'] = pd.to_datetime(dataframe['ServiceExpiry'], format='%d/%m/%Y %H:%M:%S')\n",
    "    \n",
    "    #on ne garde que les abonnés de l'année 2015\n",
    "    dataframe['anciennete_souscripteur'] = dataframe.groupby('ClientUserId').SubscriptionCreated.transform('min')\n",
    "    dataframe = dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,1,1)) & (dataframe.anciennete_souscripteur < date(2015,7,1)) ]\n",
    "    #de plus on ne prends pas en compte les mises a jour de juillet 2015\n",
    "    dataframe = dataframe.loc[(dataframe.SubscriptionLastUpdated > date(2015,1,1)) & (dataframe.SubscriptionLastUpdated < date(2015,7,1))]\n",
    "    \n",
    "    #correction fautes de frapes et autres modifs\n",
    "    dataframe.loc[(dataframe.ServiceTitle == 'SFR_Full_Extra_Suplementaire'), 'ServiceTitle']= 'SFR_Full_Extra_Supplementaire'\n",
    "    dataframe.loc[(dataframe.ServiceTitle == 'Fete_des_peres_2015_v2'), 'ServiceTitle']= 'Fete_des_peres_2015'\n",
    "    dataframe.loc[(dataframe.ServiceTitle == 'Running_heroes_1M_gratuit'), 'ServiceTitle']= 'Running_Heroes_1M_gratuit'\n",
    "    dataframe.loc[(dataframe.ServiceTitle == 'TEST_ROLAND_GARROS_'), 'ServiceTitle']= 'RG2015_1€_15jrs_puis_11.99€'\n",
    "    dataframe.loc[(dataframe.ServiceTitle == 'TEMPLATE_ROLAND_GARROS'), 'ServiceTitle']= 'RG2015_1€_15jrs_puis_11.99€'\n",
    "    dataframe.loc[(dataframe.ServiceTitle == 'CocaCola_Total_1mois_essai '), 'ServiceTitle']= 'CocaCola_Total_1mois_essai'\n",
    "    \n",
    "    #fusion du nettoyage des souscriptions avec les abonnés\n",
    "    df_sous = pd.read_excel(path+'\\souscription_detail.xlsx')\n",
    "    df_sous.drop(['ServiceTitle'], axis=1, inplace=True)\n",
    "    dataframe = pd.merge(dataframe, df_sous, how='inner', on='ServiceID')\n",
    "    \n",
    "    #group by\n",
    "    #dataframe['somme_prix'] = dataframe.groupby(['ClientUserId', 'SubscriptionId'])['ExplicitPrice'].transform('sum')\n",
    "    dataframe['SubscriptionCreated'] = dataframe.groupby(['ClientUserId','SubscriptionId'])[\"SubscriptionCreated\"].transform('min')\n",
    "    dataframe['SubscriptionLastUpdated'] = dataframe.groupby(['ClientUserId', 'SubscriptionId'])[\"SubscriptionLastUpdated\"].transform('max')\n",
    "    dataframe['ServiceExpiry'] = dataframe.groupby(['ClientUserId', 'SubscriptionId'])['ServiceExpiry'].transform('max')\n",
    "    \n",
    "    #feature generation\n",
    "    dataframe = dataframe.loc[((dataframe.ServiceGroupTitle == 'Abo EQP') | (dataframe.ServiceGroupTitle == \"Abo EQP via Tiers\"))]\n",
    "    dataframe.loc[((dataframe['ServiceGroupTitle'] == 'Abo EQP') & (dataframe.prix_mensuel != 0)), 'service'] = 'Abo_Equipe_payant' \n",
    "    dataframe.loc[((dataframe['ServiceGroupTitle'] == 'Abo EQP') & (dataframe.prix_mensuel == 0)), 'service'] = 'Abo_Equipe_gratuit'\n",
    "    dataframe.loc[((dataframe['ServiceGroupTitle'] == 'Abo EQP via Tiers') & (dataframe.prix_mensuel != 0)), 'service'] = 'Abo_via_tiers_payant' \n",
    "    dataframe.loc[((dataframe['ServiceGroupTitle'] == 'Abo EQP via Tiers') & (dataframe.prix_mensuel == 0)), 'service'] = 'Abo_via_tiers_gratuit' \n",
    "          \n",
    "    dataframe['provider'] = dataframe['UKI'].str.extract(('.*@(.+)\\..*'))\n",
    "    \n",
    "    \n",
    "    dataframe[\"sub_month\"] = numpy.nan\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,1,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,2,1)), 'sub_month'] = '1'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,2,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,3,1)), 'sub_month'] = '2'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,3,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,4,1)), 'sub_month'] = '3'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,4,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,5,1)), 'sub_month'] = '4'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,5,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,6,1)), 'sub_month'] = '5'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,6,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,7,1)), 'sub_month'] = '6'\n",
    "        \n",
    "    dataframe[\"anciennete_souscripteur\"] = (date(2015, 7, 1) - dataframe[\"anciennete_souscripteur\"])\n",
    "    dataframe['duree_abonnement_reelle'] = dataframe['ServiceExpiry'] - dataframe['SubscriptionCreated']\n",
    "    dataframe[\"derniere_maj\"] = (date(2015, 7, 1) - dataframe[\"SubscriptionLastUpdated\"])\n",
    "    dataframe.anciennete_souscripteur= pd.to_numeric(dataframe.anciennete_souscripteur)/(1000000000 * 60 * 60 * 24)\n",
    "    dataframe.derniere_maj = pd.to_numeric(dataframe.derniere_maj)/(1000000000 * 60 * 60 * 24)\n",
    "    dataframe.duree_abonnement_reelle = pd.to_numeric(dataframe.duree_abonnement_reelle)/(1000000000 * 60 * 60 * 24)\n",
    "    \n",
    "    dataframe['somme_mensualites'] = ((np.maximum(0,dataframe['duree_abonnement_reelle']\n",
    "                                                  -dataframe[\"duree_cadeau\"])/30.416)*dataframe[\"prix_mensuel\"]\n",
    "                                     + (dataframe[\"duree_cadeau\"]/30.416)*dataframe[\"prix_mensuel_cadeau\"])\n",
    "    \n",
    "    #sauvegarde d'un temp pour la data vis\n",
    "    dataframe.to_pickle(path+\"\\dataframes\\s_dataframe_temp_sub_2015.pkl\")\n",
    "    \n",
    "    #dummies \n",
    "    dataframe = pd.get_dummies(dataframe, columns = ['SubscriptionStatus'], prefix ='SubStatus' )\n",
    "    dataframe = pd.get_dummies(dataframe, columns = ['AutoRenew'], prefix ='AutoRenew' )\n",
    "    dataframe = pd.get_dummies(dataframe, columns = ['service'], prefix = 'service')\n",
    "    dataframe = pd.get_dummies(dataframe , columns=['sub_month'], prefix = 'sub_month')\n",
    "    \n",
    "    #group by pour traiter certaines dummies\n",
    "    dataframe.rename(columns=lambda x: x.replace(\" \", \"_\"), inplace=True)\n",
    "    dataframe['SubStatus_Active_Subscription'] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"SubStatus_Active_Subscription\"].transform('min')\n",
    "    dataframe['SubStatus_Cancelled_By_AutoRenew_Process'] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"SubStatus_Cancelled_By_AutoRenew_Process\"].transform('max')\n",
    "    dataframe['SubStatus_Cancelled_By_Customer_Support_Agent'] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"SubStatus_Cancelled_By_Customer_Support_Agent\"].transform('max')\n",
    "    dataframe['SubStatus_Cancelled_By_User'] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"SubStatus_Cancelled_By_User\"].transform('max')\n",
    "    dataframe['SubStatus_Expired_Subscription'] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"SubStatus_Expired_Subscription\"].transform('max')\n",
    "    dataframe['SubStatus_Failure_Retry_Mode'] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"SubStatus_Failure_Retry_Mode\"].transform('max')\n",
    "    dataframe['duree_abonnement_reelle'] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"duree_abonnement_reelle\"].transform('max')\n",
    "    dataframe[\"AutoRenew_0.0\"] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"AutoRenew_0.0\"].transform('max')\n",
    "    dataframe[\"AutoRenew_1.0\"] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"AutoRenew_1.0\"].transform('max')\n",
    "    dataframe[\"AutoRenew_2.0\"] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"AutoRenew_2.0\"].transform('max')\n",
    "    dataframe[\"AutoRenew_4.0\"] = dataframe.groupby(\n",
    "        ['ClientUserId','SubscriptionId'])[\"AutoRenew_4.0\"].transform('max')\n",
    "    \n",
    "    #drop\n",
    "    dataframe.drop(['ExplicitCurrency', 'PriceBandPaymentType', 'PriceBandAmount', 'PriceBandCurrency', 'IsTrial'\n",
    "                  ,'ExplicitPaymentType',\"ServiceGroup\", 'ServiceGroupDescription',\"UKI\"\n",
    "                      ,\"SubscriptionStatusID\",\"ServiceDescription\",\"ServiceExpiry\",'ServiceGroupTitle',\"SubscriptionCreated\" ,\"ExplicitPrice\"\n",
    "                   ,\"SubscriptionLastUpdated\", 'ServiceTitle', 'cadeau',], axis=1, inplace=True)\n",
    "    \n",
    "    dataframe = dataframe.drop_duplicates()\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "df_prod42_test = nettoyage_sub(df_prod42, liste_client)\n",
    "#print df_prod42_test.somme_mensualites.sum()\n",
    "#df_prod42_test.sort_values(by=['ClientUserId'])\n",
    "#sauvegarde \n",
    "df_prod42_test.to_pickle(path+\"\\dataframes\\s_dataframe_final_sub_2015.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection de la population des subscribers (à compléter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------nouveaux abonnés juin------------------------\n",
    "df_prod42_test = pd.read_pickle(path+\"\\dataframes\\s_dataframe_final_sub_2015.pkl\")\n",
    "#l_prod42 = df_prod42_test.ClientUserId.unique()\n",
    "\n",
    "#sub juin \n",
    "nouveaux_sub_juin = df_prod42_test.loc[df_prod42_test.sub_month_6 == 1]\n",
    "nouveaux_sub_juin = nouveaux_sub_juin.loc[nouveaux_sub_juin.somme_mensualites >0]\n",
    "nouveaux_sub_juin.drop(['sub_month_1', 'sub_month_2', 'sub_month_3'\n",
    "            , 'sub_month_4','sub_month_5',\"sub_month_6\"], axis=1, inplace=True)\n",
    "\n",
    "nouveaux_sub_juin.to_pickle(path+\"\\dataframes\\s_dataframe_final_sub_payant_juin_2015.pkl\")\n",
    "#\n",
    "#print nouveaux_sub_juin.shape #-> (2021, 28)\n",
    "#print len (nouveaux_sub_juin.ClientUserId.unique()) #-> 2011\n",
    "#\n",
    "#test1 = nouveaux_sub_juin.loc[((nouveaux_sub_juin.service_Abo_Equipe_payant ==1 ) | (nouveaux_sub_juin.service_Abo_via_tiers_payant == 1))]\n",
    "#print test1.shape #-> (946, 28)\n",
    "#print len(test1.ClientUserId.unique()) #-> 945\n",
    "#\n",
    "#test2 = nouveaux_sub_juin.loc[nouveaux_sub_juin.somme_mensualites >0]\n",
    "#print test2.shape #-> (439, 28)\n",
    "#print len(test2.ClientUserId.unique()) #-> 439\n",
    "## ***.to_pickle(path+\"\\s_final_groupby_dataframe_sub_juin_2015.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Clients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------nettoyage clients--------------------------\n",
    "\n",
    "dataframe = pd.read_pickle(r\"W:\\L_equipe\\df_client_true_site.pkl\")\n",
    "\n",
    "dataframe = dataframe.loc[dataframe['id_client'].isin(liste_client)]\n",
    "#formatage datetime \n",
    "dataframe['premiere_visite'] = pd.to_datetime(dataframe['premiere_visite'], format='%Y-%m-%d %H:%M:%S')\n",
    "dataframe['derniere_visite'] = pd.to_datetime(dataframe['derniere_visite'], format='%Y-%m-%d %H:%M:%S')\n",
    "#group by\n",
    "dataframe['somme_visites_totale'] = dataframe.groupby(['id_client']).visites.transform('sum')\n",
    "dataframe['somme_pages_vues_totale'] = dataframe.groupby([\"id_client\"])[\"pages_vues\"].transform('sum')\n",
    "dataframe['anciennete'] = dataframe.groupby(['id_client'])[\"premiere_visite\"].transform('min')\n",
    "dataframe['recence_de_visite'] = dataframe.groupby(['id_client'])[\"derniere_visite\"].transform('max')\n",
    "    \n",
    "#on choisit les clients qui on eu une acitivité entre  mars et mai 2015\n",
    "dataframe = dataframe.loc[(dataframe.anciennete < date(2015,6,1)) & (dataframe.recence_de_visite > date(2015,2,28))]\n",
    "\n",
    "dataframe['recence_de_visite'] =(date(2015, 7, 1) - dataframe.recence_de_visite)\n",
    "dataframe.recence_de_visite = pd.to_numeric(dataframe.recence_de_visite)/(1000000000 * 60 * 60 * 24)\n",
    "dataframe['anciennete'] =(date(2015, 7, 1) - dataframe.anciennete)\n",
    "dataframe.anciennete = pd.to_numeric(dataframe.anciennete)/(1000000000 * 60 * 60 * 24)\n",
    "\n",
    "dataframe['somme_visites_par_site'] = dataframe.groupby([\"id_client\",\"id_site\"])[\"visites\"].transform('sum')\n",
    "dataframe['pourcentage_visites_par_site'] = dataframe[\"somme_visites_par_site\"]/dataframe[\"somme_visites_totale\"]*100\n",
    "\n",
    "dataframe['somme_pages_vues_par_site'] = dataframe.groupby([\"id_client\",\"id_site\"])[\"pages_vues\"].transform('sum')\n",
    "dataframe['pourcentage_pages_vues_par_site'] = dataframe[\"somme_pages_vues_par_site\"]/dataframe[\"somme_pages_vues_totale\"]*100\n",
    "\n",
    "#dummies\n",
    "dataframe[\"id_site2\"] = dataframe.id_site\n",
    "dataframe = pd.get_dummies(dataframe, columns =['id_site'], prefix ='%visite_site')\n",
    "dataframe = pd.get_dummies(dataframe, columns =['id_site2'], prefix ='%pages_vues_site')\n",
    "\n",
    "#mutliplication des dummies pour optenir le pourcentage de visites par site\n",
    "dataframe[['%visite_site_492987','%visite_site_496306','%visite_site_496307','%visite_site_496838','%visite_site_502193'\n",
    ",'%visite_site_509043','%visite_site_539121','%visite_site_548647']] = dataframe[['%visite_site_492987','%visite_site_496306'\n",
    ",'%visite_site_496307','%visite_site_496838','%visite_site_502193','%visite_site_509043','%visite_site_539121'\n",
    ",'%visite_site_548647']].multiply(dataframe[\"pourcentage_visites_par_site\"], axis=\"index\")\n",
    "\n",
    "#mutliplication des dummies pour optenir le pourcentage de pages vues par site\n",
    "dataframe[['%pages_vues_site_492987','%pages_vues_site_496306','%pages_vues_site_496307','%pages_vues_site_496838'\n",
    ",'%pages_vues_site_502193','%pages_vues_site_509043','%pages_vues_site_539121'\n",
    ",'%pages_vues_site_548647']] = dataframe[['%pages_vues_site_492987','%pages_vues_site_496306','%pages_vues_site_496307'\n",
    ",'%pages_vues_site_496838','%pages_vues_site_502193','%pages_vues_site_509043','%pages_vues_site_539121'\n",
    ",'%pages_vues_site_548647']].multiply(dataframe[\"pourcentage_pages_vues_par_site\"], axis=\"index\")\n",
    "\n",
    "dataframe[['%visite_site_492987','%visite_site_496306','%visite_site_496307','%visite_site_496838','%visite_site_502193'\n",
    ",'%visite_site_509043','%visite_site_539121','%visite_site_548647']] = dataframe.groupby([\"id_client\"])['%visite_site_492987'\n",
    ",'%visite_site_496306','%visite_site_496307','%visite_site_496838','%visite_site_502193','%visite_site_509043'\n",
    ",'%visite_site_539121','%visite_site_548647'].transform('max')\n",
    "\n",
    "dataframe[['%pages_vues_site_492987','%pages_vues_site_496306','%pages_vues_site_496307','%pages_vues_site_496838'\n",
    ",'%pages_vues_site_502193','%pages_vues_site_509043','%pages_vues_site_539121'\n",
    ",'%pages_vues_site_548647']] = dataframe.groupby([\"id_client\"])['%pages_vues_site_492987','%pages_vues_site_496306'\n",
    ",'%pages_vues_site_496307','%pages_vues_site_496838','%pages_vues_site_502193','%pages_vues_site_509043'\n",
    ",'%pages_vues_site_539121','%pages_vues_site_548647'].transform(\"max\")\n",
    "\n",
    "\n",
    "dataframe.drop(['somme_visites_par_site', 'somme_pages_vues_par_site','premiere_visite', 'derniere_visite'\n",
    "                , 'fileid', 'visites', 'pages_vues', 'pourcentage_pages_vues_par_site','pourcentage_visites_par_site']\n",
    "               , axis=1, inplace=True)\n",
    "dataframe = dataframe.drop_duplicates()\n",
    "\n",
    "dataframe.to_pickle(path+\"\\dataframes\\s_dataframe_final_clients_mam_pas_forcmnt_juin.pkl\")\n",
    "\n",
    "#client ayant fait au moins une visite entre mars et mai 2015 et au moins une en juin 2015\n",
    "dataframe = dataframe.loc[dataframe.recence_de_visite < 31]\n",
    "dataframe.to_pickle(path+\"\\dataframes\\s_dataframe_final_clients.pkl\")\n",
    "\n",
    "#df_client_clean = pd.read_pickle(path+\"\\dataframes\\s_dataframes_final_clients.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean commmandes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe= pd.read_pickle(path+\"\\dataframes\\s_dataframe_general_vel_commande.pkl\")\n",
    "\n",
    "def nettoyage_commande(dataframe, liste_client) :\n",
    "    #selection des abonnés de l'equipe\n",
    "    dataframe = dataframe.loc[dataframe['ClientUserId'].isin(liste_client)]\n",
    "    #formatage datetime \n",
    "    dataframe['OrderDate'] = pd.to_datetime(dataframe['OrderDate'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    #On ne considère pas les achats et les souscriptions post juin 2015\n",
    "    dataframe = dataframe.loc[dataframe.OrderDate < date(2015,7,1)]\n",
    "    \n",
    "    dataframe = dataframe.loc[dataframe.OrderStatus == 'Completed']\n",
    "    #Pour bien faire le lien avec la table souscription on réccupère les nouveaux abonnés 2015\n",
    "    datasub = dataframe.loc[dataframe.ProductType == 'Service']\n",
    "    datasub['anciennete_souscripteur'] = datasub.groupby(['ClientUserId']).OrderDate.transform('min')\n",
    "    datasub = datasub.loc[((datasub.anciennete_souscripteur > date(2015,1,1)) & (datasub.OrderDate < date(2015,7,1))) ]\n",
    "    l_sub = datasub.ClientUserId.unique()\n",
    "    datasub = datasub[[\"ClientUserId\", \"anciennete_souscripteur\"]]\n",
    "    datasub = datasub.drop_duplicates()\n",
    "\n",
    "    dataframe = dataframe.merge(datasub,how='left', left_on='ClientUserId', right_on='ClientUserId')\n",
    "    \n",
    "    \n",
    "    #fusion du nettoyage des souscriptions avec les commandes\n",
    "    dataframe.rename(columns={'ServiceId' : \"ServiceID\"}, inplace=True)\n",
    "    df_sous = pd.read_excel(path+'\\souscription_detail.xlsx')\n",
    "    df_sous.drop(['ServiceTitle'], axis=1, inplace=True)\n",
    "    dataframe = pd.merge(dataframe, df_sous, how='left', on=\"ServiceID\")\n",
    "        \n",
    "    dataframe[\"sub_month\"] = numpy.nan\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,1,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,2,1)), 'sub_month'] = '1'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,2,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,3,1)), 'sub_month'] = '2'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,3,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,4,1)), 'sub_month'] = '3'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,4,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,5,1)), 'sub_month'] = '4'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,5,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,6,1)), 'sub_month'] = '5'\n",
    "    dataframe.loc[(dataframe.anciennete_souscripteur > date(2015,6,1)) \n",
    "                  & (dataframe.anciennete_souscripteur < date(2015,7,1)), 'sub_month'] = '6' \n",
    "    \n",
    "    #group by\n",
    "    \n",
    "    dataframe['recence_souscripteur'] = 0\n",
    "    dataframe['recence_souscripteur'].loc[dataframe.ProductType == 'Service'] =\\\n",
    "    dataframe.loc[dataframe.ProductType == 'Service'].groupby(['ClientUserId'])[\"OrderDate\"].transform('max')\n",
    "    dataframe[\"recence_souscripteur\"].loc[dataframe.ProductType == 'Service'] = (date(2015, 7, 1) - dataframe.recence_souscripteur)\n",
    "    dataframe.recence_souscripteur = pd.to_numeric(dataframe.recence_souscripteur)/(1000000000 * 60 * 60 * 24)\n",
    "    \n",
    "    dataframe.anciennete_souscripteur = (date(2015, 7, 1) - dataframe.anciennete_souscripteur)\n",
    "    dataframe.anciennete_souscripteur = pd.to_numeric(dataframe.anciennete_souscripteur)/(1000000000 * 60 * 60 * 24)\n",
    "    \n",
    "    dataframe[\"recence_article\"] = 0\n",
    "    dataframe['recence_article'].loc[dataframe.ProductType == 'Digital'] =\\\n",
    "    dataframe.loc[dataframe.ProductType == 'Digital'].groupby(['ClientUserId'])[\"OrderDate\"].transform('max')\n",
    "    dataframe[\"recence_article\"].loc[dataframe.ProductType == 'Digital'] = (date(2015, 7, 1) - dataframe.recence_article)\n",
    "    dataframe.recence_article = pd.to_numeric(dataframe.recence_article)/(1000000000 * 60 * 60 * 24)\n",
    "\n",
    "    dataframe[\"anciennete_article\"] = 0\n",
    "    dataframe['anciennete_article'].loc[dataframe.ProductType == \"Digital\"] =\\\n",
    "    dataframe.loc[dataframe.ProductType == \"Digital\"].groupby(['ClientUserId'])[\"OrderDate\"].transform('min')\n",
    "    dataframe.anciennete_article.loc[dataframe.ProductType == \"Digital\"] = (date(2015, 7, 1) - dataframe.anciennete_article)\n",
    "    dataframe.anciennete_article = pd.to_numeric(dataframe.anciennete_article)/(1000000000 * 60 * 60 * 24)\n",
    "    \n",
    "    dataframe[\"somme_paiement_par_abonnement\"] =0\n",
    "    dataframe[\"somme_paiement_par_abonnement\"].loc[dataframe.ProductType == 'Service'] = \\\n",
    "    dataframe.loc[dataframe.ProductType == 'Service'].groupby([\"ClientUserId\",\"ServiceID\"]).GrossAmount.transform('sum')\n",
    "    \n",
    "    dataframe['nb_mensualites'] = 0\n",
    "    dataframe['nb_mensualites'].loc[((dataframe.ProductType == 'Service') & (dataframe.duree_abonnement_theorique ==0 ))] =\\\n",
    "    dataframe.loc[dataframe.ProductType == 'Service'].groupby([\"ClientUserId\",\"ServiceID\"]).BasketNumber.transform('count')\n",
    "     \n",
    "\n",
    "    dataframe[\"somme_mensualites\"] = 0\n",
    "    dataframe[\"somme_mensualites\"].loc[((dataframe.ProductType == 'Service') & (dataframe.duree_abonnement_theorique ==0 ))] =\\\n",
    "    dataframe.loc[((dataframe.ProductType == 'Service') & (dataframe.duree_abonnement_theorique ==0 ))]\\\n",
    "    .groupby([\"ClientUserId\"]).BasketNumber.transform('count')\n",
    "    \n",
    "    \n",
    "    dataframe[\"mensualite_moyenne\"] = 0\n",
    "    dataframe[\"mensualite_moyenne\"].loc[((dataframe.ProductType == 'Service') & (dataframe.duree_abonnement_theorique ==0 ))] =\\\n",
    "    dataframe.loc[((dataframe.ProductType == 'Service') & (dataframe.duree_abonnement_theorique ==0 ))]\\\n",
    "    .groupby(['ClientUserId']).GrossAmount.transform(\"sum\")\n",
    "    dataframe[\"mensualite_moyenne\"] = dataframe.mensualite_moyenne / dataframe.somme_mensualites\n",
    "    #\n",
    "    ##pour les abos fermes :\n",
    "    dataframe.loc[((dataframe.somme_mensualites == 0) & (dataframe.duree_abonnement_theorique > 0)), \"somme_mensualites\"] =\\\n",
    "    dataframe.loc[((dataframe.somme_mensualites == 0) & (dataframe.duree_abonnement_theorique > 0))].duree_abonnement_theorique/30\n",
    "    \n",
    "    dataframe.loc[((dataframe.somme_mensualites == 0) & (dataframe.duree_abonnement_theorique > 0)), \"mensualite_moyenne\"] =\\\n",
    "    dataframe.loc[((dataframe.somme_mensualites == 0) & (dataframe.duree_abonnement_theorique > 0))].GrossAmount/\\\n",
    "    dataframe.loc[((dataframe.somme_mensualites == 0) & (dataframe.duree_abonnement_theorique > 0))].somme_mensualites\n",
    "    \n",
    "    dataframe['somme_paiement_totale'] = dataframe.groupby([\"ClientUserId\"]).GrossAmount.transform('sum')\n",
    "    \n",
    "    dataframe[\"somme_paiement_article\"] =0\n",
    "    dataframe[\"somme_paiement_article\"].loc[dataframe.ProductType == 'Digital'] = \\\n",
    "    dataframe.loc[dataframe.ProductType == 'Digital'].groupby([\"ClientUserId\"]).GrossAmount.transform('sum')\n",
    "        \n",
    "    #features\n",
    "    dataframe.ServiceID.fillna(0, inplace=True)\n",
    "    #dataframe['theme_article'] = dataframe['ARTICLE_URL'].str.extract(('.*www.lequipe.fr/([a-zA-Z]+)/.*'))\n",
    "    #dataframe.loc[dataframe.somme_paiement_par_abonnement == 0, 'commande_payante']=1\n",
    "    #dataframe.loc[dataframe.somme_paiement_par_abonnement != 0, \"commande_payante\"]=0\n",
    "    \n",
    "    \n",
    "    #dummies\n",
    "    dataframe = pd.get_dummies(dataframe, columns = ['ProductType'], prefix ='prod_type')\n",
    "    dataframe = pd.get_dummies(dataframe , columns=['sub_month'], prefix = 'sub_month')\n",
    "    \n",
    "    #dataframe['theme_article'] = dataframe['ARTICLE_URL'].str.extract(('.*www.lequipe.fr/([a-zA-Z]+)/.*'))\n",
    "    #dataframe = pd.get_dummies(dataframe, columns=['theme_article'], prefix='theme_article')\n",
    "    \n",
    "    #duplication des infos utiles pour split abonnements et articles\n",
    "    \n",
    "    \n",
    "    dataframe['nb_articles'] = dataframe.prod_type_Digital\n",
    "    dataframe['nb_articles'] = dataframe.groupby(['ClientUserId']).nb_articles.transform(\"sum\")\n",
    "   \n",
    "    #drop\n",
    "    dataframe.drop([\"AffiliateId\",\"BasketNumber\",\"Country\",\"County\",\"Currency\",\"District\",\"NetAmount\",\"HouseFlatNumber\",\n",
    "        \"HouseName\",\"Message\",\"Name\",\"OfferId\",\"OrderStatus\",\"ParameterOrderId\",\"PaymentMethod\",\"PostCode\",\"ProductDataSource\",\n",
    "        \"Street\",\"SupplierID\",\"ThirdPartyRef\",\"Titre\",\"TotalTaxPercentage\",\"TownCity\",\"VATAmount\",\"detail_mode_paiement\",\"gift_id\",\n",
    "        \"identifiant_commercial\",\"identifiant_salon\",\"lien\",\"provenance\",\"type\",'ActivationCode',\n",
    "        'DeliveryComments', 'subscriptionId','ContentItemId', \"ARTICLE_TITLE\", \"ARTICLE_URL\"\n",
    "       ,'GrossAmount', 'OrderID', 'AccountID', \"MPPGiftCode\",\"Description\", 'MPPGiftID', 'OrderDate', 'somme_paiement_par_abonnement',\n",
    "        'nb_mensualites', 'ServiceID'], axis=1, inplace=True)\n",
    "    \n",
    "    dataframe = dataframe.drop_duplicates()\n",
    "    dataframe[\"nb_abonnements\"] = dataframe.groupby([\"ClientUserId\"]).prod_type_Service.transform(\"sum\")\n",
    "    \n",
    "    #dataframe[['anciennete_article','anciennete_souscripteur', \"somme_mensualites\", 'mensualite_moyenne'\n",
    "    #          , 'somme_paiement_totale', 'somme_paiement_article', 'prod_type_Digital', 'prod_type_Service'\n",
    "    #          , 'sub_month_4','sub_month_5','sub_month_6', 'nb_abonnements'\n",
    "    #          , 'nb_articles']] = dataframe.groupby([\"ClientUserId\"])['anciennete_article','anciennete_souscripteur'\n",
    "    #          , \"somme_mensualites\", 'mensualite_moyenne', 'somme_paiement_totale', 'somme_paiement_article'\n",
    "    #          , 'prod_type_Digital', 'prod_type_Service', 'sub_month_4','sub_month_5','sub_month_6'\n",
    "    #          , 'nb_abonnements', 'nb_articles'].transform('max')\n",
    "    #\n",
    "    dataframe[['recence_article', 'recence_souscripteur']] = dataframe.groupby(['ClientUserId'])['recence_article'\n",
    "                    , 'recence_souscripteur'].transform('min')\n",
    "    dataframe = dataframe.drop_duplicates()\n",
    "    dataframe.fillna(0, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "df_commande = nettoyage_commande(dataframe, liste_client)\n",
    "df_commande.to_pickle(path+\"\\dataframes\\s_dataframe_final_commandes.pkl\")\n",
    "\n",
    "\n",
    "#df_commande.sort_values(by='ClientUserId')\n",
    "#df_commande.nb_abonnements.value_counts(dropna=False)\n",
    "#pandas_profiling.ProfileReport(df_commande)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean pages premiere partie (1iere étape terminée, deuxieme étape en cours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#création de la liste de la popolation a sélectionner dans page et sessions\n",
    "df = pd.read_pickle(path+\"\\dataframes\\s_dataframe_final_clients.pkl\")\n",
    "df1 = pd.read_pickle(path+\"\\dataframes\\s_dataframe_final_sub_payant_juin_2015.pkl\")\n",
    "liste_pop = df.id_client.unique().tolist()\n",
    "liste_sub_juin = df1.ClientUserId.unique().tolist()\n",
    "set_pop=set(liste_pop)\n",
    "set_sub_juin = set(liste_sub_juin)\n",
    "set_pop.update(set_sub_juin)  #liste d'id unique contenant notre population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#premiere étape clean chaque fichiers page et fichier session avec le cut sur notre population \n",
    "path = \"W:\\L_equipe\\dataframes\\\\\"\n",
    "files  = []\n",
    "files.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                        if f.startswith('s_dataframe_general_xiti_pages') if getsize(join(path,f))>0])\n",
    "\n",
    "for f in files :\n",
    "    reg = re.match(('.*pages_(.*)'), f)\n",
    "    dataframe = pd.read_pickle(f)\n",
    "    dataframe = dataframe.loc[dataframe.id_site.isin([492987,496306,496307,496838,502193,509043,539121,548647])]\n",
    "    dataframe = dataframe.loc[dataframe.id_client.isin(set_pop)]\n",
    "    dataframe.to_pickle(path+\"s_dataframe_temp_pages_payant_\"+reg.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deuxieme étape (à faire car le cut n'est pas suffisant) on group by et on refera une étape de groupby par la suite\n",
    "path = \"W:\\L_equipe\\dataframes\\\\\"\n",
    "files  = []\n",
    "files.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                        if f.startswith('s_dataframe_temp_pages_population') if getsize(join(path,f))>0])\n",
    "\n",
    "for f in files :\n",
    "    reg = re.match(('.*pages_population_(.*)\\.pkl'), f)\n",
    "    dataframe = pd.read_pickle(f)\n",
    "    \n",
    "    dataframe = pd.get_dummies(dataframe, columns=[\"id_site\"], prefix=\"id_site\")\n",
    "    #normalisation de niveau_2\n",
    "    dataframe.niveau_2 = dataframe.niveau_2.str.lower()\n",
    "    dataframe.niveau_2 = dataframe.niveau_2.str.normalize('NFKD')\n",
    "    dataframe.niveau_2 = dataframe.niveau_2.str.encode('ASCII', 'ignore')\n",
    "    type_sports = pd.read_csv(r\"W:\\L_Equipe\\types_sports.csv\",sep=\";\")\n",
    "    dataframe = dataframe.merge(type_sports,how='left', left_on='niveau_2', right_on='niveau_2')\n",
    "    #dummies\n",
    "    dataframe = pd.get_dummies(dataframe, columns= [\"theme\"], prefix='theme')\n",
    "    #groupby\n",
    "    dataframe[\"nb_session\"] = dataframe.groupby([\"id_client\"]).id_session.transform(\"count\")\n",
    "    \n",
    "    dataframe[\"nb_pages_par_session\"] = dataframe.groupby([\"fileid\", \"id_client\",\"id_session\"]).position_de_la_page.transform(\"max\")\n",
    "    dataframe['nb_pages_moyen'] = dataframe.groupby([\"id_client\"]).nb_pages_par_session.transform('mean')\n",
    "    dataframe['nb_pages_min'] = dataframe.groupby([\"id_client\"]).nb_pages_par_session.transform('min')\n",
    "    dataframe['nb_pages_max'] = dataframe.groupby([\"id_client\"]).nb_pages_par_session.transform('mean')\n",
    "    dataframe['nb_pages_total'] = dataframe.groupby([\"id_client\"]).nb_pages_par_session.transform('sum')\n",
    "    \n",
    "    dataframe[\"duree_session\"] = dataframe.groupby([\"fileid\", \"id_client\",\"id_session\"]).temps_passe_sur_la_page.transform(\"sum\")\n",
    "    dataframe[\"duree_session_moyenne\"] = dataframe.groupby([\"id_client\"]).duree_session.transform('mean')\n",
    "    dataframe[\"duree_session_max\"] = dataframe.groupby([\"id_client\"]).duree_session.transform('max')\n",
    "    dataframe[\"duree_session_min\"] = dataframe.groupby([\"id_client\"]).duree_session.transform('min')\n",
    "    \n",
    "    sites = [col for col in list(dataframe) if col.startswith(\"id_site\")]\n",
    "\n",
    "    themes = [col for col in list(dataframe) if col.startswith('theme')]\n",
    "    \n",
    "    dataframe[sites] = dataframe.groupby([\"id_client\"])[sites].transform(\"max\")\n",
    "    dataframe[themes] = dataframe.groupby([\"id_client\"])[themes].transform(\"max\")\n",
    "    \n",
    "    dataframe.drop([\"url\", \"fileid\",\"position_de_la_page\",\"temps_passe_sur_la_page\", \"niveau_2\",\"id_session\"\n",
    "                    ,\"nb_pages_par_session\",\"duree_session\",\"index\"],axis=1, inplace=True)\n",
    "    \n",
    "    dataframe = dataframe.drop_duplicates()\n",
    "    dataframe = dataframe.rename(columns = lambda x : reg.group(1) +'_'+ x)\n",
    "    dataframe.to_pickle(path+\"\\s_dataframe_temp2_pages_population_\"+reg.group(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dataframe= pd.read_pickle(path+\"\\dataframes\\s_dataframe_general_xiti_sessions_mars_juin2015.pkl\")\n",
    "path = \"W:\\L_equipe\\dataframes\\\\\"\n",
    "files  = []\n",
    "files.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                        if f.startswith('s_dataframe_general_xiti_sessions') if getsize(join(path,f))>0])\n",
    "\n",
    "for f in files :\n",
    "    \n",
    "\n",
    "#def clean_session(dataframe, liste_client) :\n",
    "#selection des abonnés de l'equipe\n",
    "dataframe = dataframe.loc[dataframe['id_client'].isin(liste_client)]\n",
    "#suppression des lignes ou fin de session = u'0001-01-01 00:00:00'\n",
    "dataframe = dataframe.loc[dataframe.fin_de_session != u'0001-01-01 00:00:00']\n",
    "#formatage datetime \n",
    "dataframe['debut_de_session'] = pd.to_datetime(dataframe['debut_de_session'], format='%Y-%m-%d %H:%M:%S')\n",
    "dataframe['fin_de_session'] = pd.to_datetime(dataframe['fin_de_session'], format='%Y-%m-%d %H:%M:%S')\n",
    "dataframe['duree_session'] = dataframe.fin_de_session - dataframe.debut_de_session\n",
    "\n",
    "dataframe.loc[((dataframe.debut_de_session.dt.hour >= 0) & (dataframe.debut_de_session.dt.hour <= 5)), \"periode\" ] = \"nuit\"\n",
    "dataframe.loc[((dataframe.debut_de_session.dt.hour >= 6) & (dataframe.debut_de_session.dt.hour <=8)), \"periode\" ] = \"aube\"\n",
    "dataframe.loc[((dataframe.debut_de_session.dt.hour >= 9) & (dataframe.debut_de_session.dt.hour <12)), \"periode\" ] = \"matinee\"\n",
    "dataframe.loc[((dataframe.debut_de_session.dt.hour >= 12) & (dataframe.debut_de_session.dt.hour < 14)), \"periode\" ] = \"midi\"\n",
    "dataframe.loc[((dataframe.debut_de_session.dt.hour >= 14) & (dataframe.debut_de_session.dt.hour < 17)), \"periode\"] = \"apres_midi\"\n",
    "dataframe.loc[((dataframe.debut_de_session.dt.hour >=17 )& (dataframe.debut_de_session.dt.hour <20)), 'periode'] = 'fin_apres_midi'\n",
    "dataframe.loc[((dataframe.debut_de_session.dt.hour >= 20 ) & (dataframe.debut_de_session.dt.hour < 24)), \"periode\"] = \"soiree\"\n",
    "                \n",
    "dataframe.loc[dataframe['localisation_3eme_niveau'] == 'Paris', 'Paris'] = 1\n",
    "dataframe.loc[dataframe['localisation_3eme_niveau'] != 'Paris', 'Paris'] = 0\n",
    "\n",
    "dataframe.loc[dataframe['localisation_2eme_niveau'] == 'Ile-de-France', 'RP'] = 1\n",
    "dataframe.loc[dataframe['localisation_2eme_niveau'] != 'Ile-de-France', 'RP'] = 0\n",
    "\n",
    "dataframe.loc[dataframe['localisation_1er_niveau'] == 'France', 'France'] = 1\n",
    "dataframe.loc[dataframe['localisation_1er_niveau'] != 'France', 'France'] = 0\n",
    "\n",
    "dataframe.loc[((dataframe['debut_de_session'] <= date(2015, 6, 7)) &\n",
    "              (dataframe.debut_de_session > date(2015,5,31))), 'juin_semaine_1'] = 1\n",
    "dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 6, 14))& \n",
    "              (dataframe['debut_de_session'] > date(2015, 6, 7)), 'juin_semaine_2'] = 1\n",
    "dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 6, 21) ) & \n",
    "              (dataframe['debut_de_session'] > date(2015, 6, 14)), 'juin_semaine_3'] = 1\n",
    "dataframe.loc[(dataframe['debut_de_session'] > date(2015, 6, 21)) & \n",
    "              (dataframe.debut_de_session < date(2015,7,1)), 'juin_semaine_4'] = 1\n",
    "\n",
    "dataframe.loc[((dataframe['debut_de_session'] <= date(2015, 5,10)) &\n",
    "              (dataframe.debut_de_session > date(2015,4,30))), 'mai_semaine_1'] = 1\n",
    "dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 5, 17))& \n",
    "              (dataframe['debut_de_session'] > date(2015, 5, 10)), 'mai_semaine_2'] = 1\n",
    "dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 5, 24) ) & \n",
    "              (dataframe['debut_de_session'] > date(2015, 5, 17)), 'mai_semaine_3'] = 1\n",
    "dataframe.loc[(dataframe['debut_de_session'] > date(2015, 5, 24)) & \n",
    "              (dataframe.debut_de_session < date(2015,6,1)), 'mai_semaine_4'] = 1\n",
    "                  \n",
    "dataframe.loc[((dataframe['debut_de_session'] <= date(2015, 4, 5)) &\n",
    "              (dataframe.debut_de_session > date(2015,3,31))), 'avril_semaine_1'] = 1\n",
    "dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 4, 12))& \n",
    "              (dataframe['debut_de_session'] > date(2015, 4, 5)), 'avril_semaine_2'] = 1\n",
    "dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 4, 19) ) & \n",
    "              (dataframe['debut_de_session'] > date(2015, 4, 12)), 'avril_semaine_3'] = 1\n",
    "dataframe.loc[(dataframe['debut_de_session'] > date(2015, 4, 19)) & \n",
    "              (dataframe.debut_de_session < date(2015,5,1)), 'avril_semaine_4'] = 1\n",
    "               \n",
    "dataframe.loc[((dataframe['debut_de_session'] <= date(2015, 3, 8)) &\n",
    "              (dataframe.debut_de_session > date(2015,2,28))), 'mars_semaine_1'] = 1\n",
    "dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 3, 15))& \n",
    "              (dataframe['debut_de_session'] > date(2015, 3, 8)), 'mars_semaine_2'] = 1\n",
    "dataframe.loc[(dataframe['debut_de_session'] <= date(2015, 3, 22) ) & \n",
    "              (dataframe['debut_de_session'] > date(2015, 3, 15)), 'mars_semaine_3'] = 1\n",
    "dataframe.loc[(dataframe['debut_de_session'] > date(2015, 3, 22)) & \n",
    "              (dataframe.debut_de_session < date(2015,4,1)), 'mars_semaine_4'] = 1\n",
    "\n",
    "liste_weekend = [date(2015,3,1),date(2015,3,7),date(2015,3,8),date(2015,3,14),date(2015,3,15), date(2015,3,21),date(2015,3,22),date(2015,3,28),\n",
    "                 date(2015,3,29),date(2015,4,4),date(2015,4,5),date(2015,4,11),date(2015,4,12),date(2015,4,18),date(2015,4,19),date(2015,4,25),\n",
    "                 date(2015,4,26),date(2015,5,2),date(2015,5,3),date(2015,5,9),date(2015,5,10),date(2015,5,16),date(2015,5,17),date(2015,5,23),\n",
    "                 date(2015,5,24),date(2015,5,30),date(2015,5,31),date(2015,6,6),date(2015,6,7),date(2015,6,13),date(2015,6,14),date(2015,6,20),\n",
    "                 date(2015,6,21),date(2015,6,27),date(2015,6,28)]\n",
    "\n",
    "dataframe[\"type_jour\"] = \"semaine\"\n",
    "dataframe.loc[dataframe.debut_de_session.dt.date.isin(liste_weekend), \"type_jour\"] = \"weekend\"\n",
    "\n",
    "#liste_semaines = [col for col in list(dataframe) if col.find(\"semaine\")]\n",
    "#dataframe[liste_semaines] = dataframe.groupby([\"id_client\"])[liste_semaines].transform(\"max\")\n",
    "\n",
    "dataframe[\"mars_semaine_1\"] = dataframe.groupby([\"id_client\"])[\"mars_semaine_1\"].transform(\"max\")\n",
    "dataframe[\"mars_semaine_2\"] = dataframe.groupby([\"id_client\"])[\"mars_semaine_2\"].transform(\"max\")\n",
    "dataframe[\"mars_semaine_3\"] = dataframe.groupby([\"id_client\"])[\"mars_semaine_3\"].transform(\"max\")\n",
    "dataframe[\"mars_semaine_4\"] = dataframe.groupby([\"id_client\"])[\"mars_semaine_4\"].transform(\"max\")\n",
    "dataframe[\"avril_semaine_1\"] = dataframe.groupby([\"id_client\"])[\"avril_semaine_1\"].transform(\"max\")\n",
    "dataframe[\"avril_semaine_2\"] = dataframe.groupby([\"id_client\"])[\"avril_semaine_2\"].transform(\"max\")\n",
    "dataframe[\"avril_semaine_3\"] = dataframe.groupby([\"id_client\"])[\"avril_semaine_3\"].transform(\"max\")\n",
    "dataframe[\"avril_semaine_4\"] = dataframe.groupby([\"id_client\"])[\"avril_semaine_4\"].transform(\"max\")\n",
    "dataframe[\"mai_semaine_1\"] = dataframe.groupby([\"id_client\"])[\"mai_semaine_1\"].transform(\"max\")\n",
    "dataframe[\"mai_semaine_2\"] = dataframe.groupby([\"id_client\"])[\"mai_semaine_2\"].transform(\"max\")\n",
    "dataframe[\"mai_semaine_3\"] = dataframe.groupby([\"id_client\"])[\"mai_semaine_3\"].transform(\"max\")\n",
    "dataframe[\"mai_semaine_4\"] = dataframe.groupby([\"id_client\"])[\"mai_semaine_4\"].transform(\"max\")\n",
    "dataframe[\"juin_semaine_1\"] = dataframe.groupby([\"id_client\"])[\"juin_semaine_1\"].transform(\"max\")\n",
    "dataframe[\"juin_semaine_2\"] = dataframe.groupby([\"id_client\"])[\"juin_semaine_2\"].transform(\"max\")\n",
    "dataframe[\"juin_semaine_3\"] = dataframe.groupby([\"id_client\"])[\"juin_semaine_3\"].transform(\"max\")\n",
    "dataframe[\"juin_semaine_4\"] = dataframe.groupby([\"id_client\"])[\"juin_semaine_4\"].transform(\"max\")\n",
    "\n",
    "\n",
    "dataframe[\"somme_session\"] = dataframe.groupby([\"id_client\"]).id_session.transform('count')\n",
    "dataframe = pd.get_dummies(dataframe, columns=[\"type_jour\"])\n",
    "dataframe[\"somme_session_semaine\"] = dataframe.groupby([\"id_client\"]).type_jour_semaine.transform('sum')\n",
    "dataframe[\"somme_session_weekend\"] = dataframe.groupby([\"id_client\"]).type_jour_weekend.transform('sum')\n",
    "#dataframe[\"duree_session_moyenne\"] = dataframe.groupby([\"id_client\"]).duree_session.transform(\"mean\")\n",
    "dataframe[\"duree_session_min\"] = dataframe.groupby([\"id_client\"]).duree_session.transform(\"min\")\n",
    "dataframe[\"duree_session_max\"] = dataframe.groupby([\"id_client\"]).duree_session.transform(\"max\")\n",
    "dataframe[\"duree_session_totale\"] = dataframe.groupby([\"id_client\"]).duree_session.transform(\"sum\")\n",
    "\n",
    "dataframe = pd.get_dummies(dataframe, columns=[\"id_site\"], prefix = \"site\")\n",
    "#liste_site = [\"site_492987\",\"site_496306\",\"site_496307\",\"site_496838\",\"site_502193\",\"site_509043\",\"site_539121\",\"site_548647\"]\n",
    "dataframe[\"site_492987\"] = dataframe.groupby([\"id_client\"])[\"site_492987\"].transform(\"max\")\n",
    "dataframe[\"site_496306\"] = dataframe.groupby([\"id_client\"])[\"site_496306\"].transform(\"max\")\n",
    "dataframe[\"site_496307\"] = dataframe.groupby([\"id_client\"])[\"site_496307\"].transform(\"max\")\n",
    "dataframe[\"site_496838\"] = dataframe.groupby([\"id_client\"])[\"site_496838\"].transform(\"max\")\n",
    "dataframe[\"site_502193\"] = dataframe.groupby([\"id_client\"])[\"site_502193\"].transform(\"max\")\n",
    "dataframe[\"site_509043\"] = dataframe.groupby([\"id_client\"])[\"site_509043\"].transform(\"max\")\n",
    "dataframe[\"site_539121\"] = dataframe.groupby([\"id_client\"])[\"site_539121\"].transform(\"max\")\n",
    "dataframe[\"site_548647\"] = dataframe.groupby([\"id_client\"])[\"site_548647\"].transform(\"max\")\n",
    "\n",
    "\n",
    "dataframe[\"somme_pages_vues\"] = dataframe.groupby([\"id_client\"]).pages_vues.transform(\"sum\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#OS managing\n",
    "dataframe[\"os2\"] = \"autres\"\n",
    "dataframe.os = dataframe.os.str.lower()\n",
    "dataframe.loc[dataframe.os.str.match(\".*ios.*\"), \"os2\"] = \"iphone\"\n",
    "dataframe.loc[dataframe.os.str.match(\".*ipad.*\"), \"os2\"] = \"ipad\"\n",
    "dataframe.loc[((dataframe.os.str.match(\".*mac os.*\")) | (dataframe.os2.str.match(\".*os x.*\")))  , \"os2\"] = \"mac\"\n",
    "dataframe.loc[dataframe.os.str.match(\".*windows.*\"), \"os2\"] = \"windows\"\n",
    "dataframe.loc[dataframe.os.str.match(\".*windows phone.*\"), \"os2\"] = \"windows phone\"\n",
    "dataframe.loc[dataframe.os.str.match(\".*android.*\"), \"os2\"] = \"android\"\n",
    "dataframe.loc[dataframe.os.str.match(\".*linux.*\"), \"os2\"] = \"linux\"\n",
    "dataframe.loc[dataframe.os.str.match(\".*chrome.*\"), \"os2\"] = \"chrome os\"\n",
    "\n",
    "#dataframe = pd.get_dummies(dataframe, columns=[\"os2\"])\n",
    "#dataframe = pd.get_dummies(dataframe, columns= [\"periode\"])\n",
    "\n",
    "dataframe.drop([\"fileid\",\"id_campagne\",\"id_session\", \"debut_de_session\",\"fin_de_session\", \"pages_vues\", 'localisation_1er_niveau',\n",
    "                'localisation_2eme_niveau', 'localisation_3eme_niveau', \"terminal\", \"os\"],axis=1, inplace=True)\n",
    "\n",
    "dataframe = dataframe.drop_duplicates()\n",
    "#return dataframe\n",
    "\n",
    "#df_session = clean_session(dataframe, liste_client)\n",
    "#df_session.to_pickle(path+\"\\dataframes\\s_dataframe_final_sessions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe = pd.get_dummies(dataframe, columns= [\"periode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "cannot allocate memory for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-205c0d9bc11b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\util\\decorators.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36mdrop_duplicates\u001b[1;34m(self, subset, keep, inplace)\u001b[0m\n\u001b[0;32m   3050\u001b[0m         \u001b[0mdeduplicated\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3051\u001b[0m         \"\"\"\n\u001b[1;32m-> 3052\u001b[1;33m         \u001b[0mduplicated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3054\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\util\\decorators.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36mduplicated\u001b[1;34m(self, subset, keep)\u001b[0m\n\u001b[0;32m   3102\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3104\u001b[1;33m         \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_group_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxnull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3105\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mduplicated_int64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\core\\groupby.pyc\u001b[0m in \u001b[0;36mget_group_index\u001b[1;34m(labels, shape, sort, xnull)\u001b[0m\n\u001b[0;32m   4038\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_lift\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4039\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4040\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\core\\groupby.pyc\u001b[0m in \u001b[0;36mloop\u001b[1;34m(labels, shape)\u001b[0m\n\u001b[0;32m   4029\u001b[0m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnlev\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4031\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4033\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmaybe_lift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pormote nan values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\core\\groupby.pyc\u001b[0m in \u001b[0;36mloop\u001b[1;34m(labels, shape)\u001b[0m\n\u001b[0;32m   4029\u001b[0m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnlev\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4031\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4033\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmaybe_lift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pormote nan values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\core\\groupby.pyc\u001b[0m in \u001b[0;36mloop\u001b[1;34m(labels, shape)\u001b[0m\n\u001b[0;32m   4024\u001b[0m         \u001b[1;31m# compress what has been done so far in order to avoid overflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4025\u001b[0m         \u001b[1;31m# to retain lexical ranks, obs_ids should be sorted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4026\u001b[1;33m         \u001b[0mcomp_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_compress_group_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4028\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcomp_ids\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnlev\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\core\\groupby.pyc\u001b[0m in \u001b[0;36m_compress_group_index\u001b[1;34m(group_index, sort)\u001b[0m\n\u001b[0;32m   4270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4271\u001b[0m     \u001b[1;31m# note, group labels come out ascending (ie, 1,2,3 etc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4272\u001b[1;33m     \u001b[0mcomp_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_group_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_labels_groupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4274\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msort\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_group_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_labels_groupby (pandas\\hashtable.c:8733)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64Vector.resize (pandas\\hashtable.c:3438)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: cannot allocate memory for array"
     ]
    }
   ],
   "source": [
    "dataframe = dataframe.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16871348"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "33742697/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "error return without exception set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c2a4e81dc6ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16871348\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\dataframes\\s_dataframe_temp_session1.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36mto_pickle\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \"\"\"\n\u001b[0;32m   1171\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1172\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_clipboard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexcel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amaury\\Anaconda2\\lib\\site-packages\\pandas\\io\\pickle.pyc\u001b[0m in \u001b[0;36mto_pickle\u001b[1;34m(obj, path)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: error return without exception set"
     ]
    }
   ],
   "source": [
    "dataframe.head(16871348).to_pickle(path+\"\\dataframes\\s_dataframe_temp_session1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final groupby pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"W:\\L_equipe\\dataframes\\\\\"\n",
    "files  = []\n",
    "files.extend([join(path,f) for f in listdir(path) if isfile(join(path, f)) \n",
    "                        if f.startswith('s_dataframe_temp2_pages_population') if getsize(join(path,f))>0])\n",
    "\n",
    "dataframe = pd.concat((pd.read_pickle(f) for f in files))\n",
    "dataframe\n",
    "\n",
    "#dataframe[\"nb_session\"] = dataframe.groupby([\"id_client\"]).nb_session.transform(\"sum\")\n",
    "#dataframe['nb_pages_min'] = dataframe.groupby([\"id_client\"]).nb_pages_par_session.transform('min')\n",
    "#dataframe['nb_pages_max'] = dataframe.groupby([\"id_client\"]).nb_pages_par_session.transform('mean')\n",
    "#dataframe['nb_pages_total'] = dataframe.groupby([\"id_client\"]).nb_pages_par_session.transform('sum')\n",
    "#dataframe[\"duree_session_max\"] = dataframe.groupby([\"id_client\"]).duree_session.transform('max')\n",
    "#dataframe[\"duree_session_min\"] = dataframe.groupby([\"id_client\"]).duree_session.transform('min')\n",
    "#\n",
    "#sites = [col for col in list(dataframe) if col.startswith(\"id_site\")]\n",
    "#\n",
    "#themes = [col for col in list(dataframe) if col.startswith('theme')]\n",
    "#\n",
    "#dataframe[sites] = dataframe.groupby([\"id_client\"])[sites].transform(\"max\")\n",
    "#dataframe[themes] = dataframe.groupby([\"id_client\"])[themes].transform(\"max\")\n",
    "#dataframe.to_pickle(path+\"\\s_dataframe_final_page.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes, tests, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean datetime\n",
    "#df_commande_clean['OrderDate'] = pd.to_datetime(df_commande_clean['OrderDate'])\n",
    "#df_compte_clean['LastUpdated'] = pd.to_datetime(df_compte_clean['LastUpdated'])\n",
    "#df_compte_clean['CreateDate'] = pd.to_datetime(df_compte_clean['CreateDate'])\n",
    "\n",
    "\n",
    "df_session_juin['debut_de_session'] = pd.to_datetime(df_session_juin['debut_de_session'])\n",
    "df_session_juin['fin_de_session'] = pd.to_datetime(df_session_juin['fin_de_session'])\n",
    "\n",
    "\n",
    "#df_client_clean.premiere_v = pd.to_datetime(df_client_clean['premiere_v'])\n",
    "#df_client_clean.derniere_v = pd.to_datetime(df_client_clean['derniere_v'])\n",
    "\n",
    "#\n",
    "df_session_juin[\"temps_session\"] =  df_session_juin[\"fin_de_session\"] - df_session_juin[\"debut_de_session\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#création de différents indicateurs\n",
    "df_session_juin.loc[df_session_juin['localisation_3eme_niveau'] == 'Paris', 'Paris'] = 1\n",
    "df_session_juin.loc[df_session_juin['localisation_3eme_niveau'] != 'Paris', 'Paris'] = 0\n",
    "\n",
    "df_session_juin.loc[df_session_juin['localisation_2eme_niveau'] == 'Ile-de-France', 'RP'] = 1\n",
    "df_session_juin.loc[df_session_juin['localisation_2eme_niveau'] != 'Ile-de-France', 'RP'] = 0\n",
    "\n",
    "df_session_juin.loc[df_session_juin['localisation_1er_niveau'] == 'France', 'France'] = 1\n",
    "df_session_juin.loc[df_session_juin['localisation_1er_niveau'] != 'France', 'France'] = 0\n",
    "\n",
    "\n",
    "df_session_juin.loc[(df_session_juin['debut_de_session'] < date(2015, 6, 7)), 'juin_semaine_1'] = 1\n",
    "#df_session_juin_clean.loc[(df_session_juin_clean['debut_de_session'] > date(2015, 6, 7)), 'juin_semaine_1'] = 0\n",
    "\n",
    "df_session_juin.loc[(df_session_juin['debut_de_session'] < date(2015, 6, 14) ) \n",
    "                          & (df_session_juin['debut_de_session'] > date(2015, 6, 6)), 'juin_semaine_2'] = 1\n",
    "#df_session_juin_clean.loc[(df_session_juin_clean['debut_de_session'] < date(2015, 6, 7) ) \n",
    "#                          & (df_session_juin_clean['debut_de_session'] > date(2015, 5, 31)), 'juin_semaine_1'] = 1\n",
    "\n",
    "df_session_juin.loc[(df_session_juin['debut_de_session'] < date(2015, 6, 21) ) \n",
    "                          & (df_session_juin['debut_de_session'] > date(2015, 6, 13)), 'juin_semaine_3'] = 1\n",
    "\n",
    "df_session_juin.loc[(df_session_juin['debut_de_session'] > date(2015, 6, 20) ), 'juin_semaine_4'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_session_juin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#travail sur les dates des clients : \n",
    "df_client_clean[\"anciennete\"] = (date(2015, 6, 30) - df_client_clean[\"premiere_v\"])\n",
    "df_client_clean[\"derniere_visite\"] = (date(2015, 6, 30) - df_client_clean[\"derniere_v\"])\n",
    "df_client_clean.anciennete = pd.to_numeric(df_client_clean.anciennete)/(1000000000 * 60 * 60 * 24)\n",
    "df_client_clean.derniere_visite = pd.to_numeric(df_client_clean.derniere_visite)/(1000000000 * 60 * 60 * 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "#df_session_juin[\"debut_de_session\"].groupby(df_session_juin[\"debut_de_session\"].dt.hour).count().plot(kind=\"bar\")\n",
    "df_session_juin[\"debut_de_session\"].groupby(df_session_juin[\"debut_de_session\"].dt.hour).count().plot(kind=\"bar\")\n",
    "plt.savefig(\"debut_de_session_juin2015.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_commande.rename(columns = {'Somme_NetAmmount':'Somme_NetAmount'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (df_pages_juin_p1.theme_page.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clients\n",
    "#commande\n",
    "#df_commande_clean_d = pd.get_dummies(df_commande_clean, columns =['theme_article'], prefix ='theme' )\n",
    "#df_client_clean = pd.get_dummies(df_client_clean, columns =['id_site'], prefix ='site' )\n",
    "\n",
    "\n",
    "\n",
    "#df_prod42_clean = pd.get_dummies(df_prod42_clean, columns =['SubscriptionStatus'], prefix ='SubStatus' )\n",
    "df_prod42_clean = pd.get_dummies(df_prod42_clean, columns =['AutoRenew'], prefix ='AutoRenew' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_session_juin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(df_prod42.SubscriptionId.unique())\n",
    "#print len( df_prod42_clean.loc[df_prod42_clean[''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_session_juin.to_pickle(path+\"\\s_dataframe_sessions_juin_clean.pkl\")\n",
    "\n",
    "#si on veut sauver les dataframe en fichier excel :\n",
    "#writer = pd.ExcelWriter('df_sessions_juin.xlsx', engine='xlsxwriter')\n",
    "#df_session_juin.head(2).to_excel(writer, sheet_name='Sheet1')\n",
    "#writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différence des abonnés dans comande et dans sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=set(liste_nouveaux_sub_commande_2015)\n",
    "t=set(liste_nouveaux_sub_2015)\n",
    "print \"taille de sub commande\" , len(s)\n",
    "print \"taille de sub\", len(t)\n",
    "print \"commande est il subset de sub ?\", s.issubset(t)#\ts <= t\ttest whether every element in s is in t\n",
    "print \"commande est il superset de sub ?\", s.issuperset(t)#\ts >= t\ttest whether every element in t is in s\n",
    "print \"taille de l'intersection\", len(s.intersection(t))#\ts & t\tnew set with elements common to s and t\n",
    "print \"elements dans commande mais pas dans sub\" , len(s.difference(t))# \ts - t\tnew set with elements in s but not in t\n",
    "print \"elements dans sub mais pas dans commande\" , len(t.difference(s))\n",
    "#s.symmetric_difference(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liste_nouveaux_sub_commande_2015_2 = df_commande.ClientUserId.unique()\n",
    "liste_nouveaux_sub_2015_2 = df_prod42_test.ClientUserId.unique()\n",
    "s=set(liste_nouveaux_sub_commande_2015_2)\n",
    "t=set(liste_nouveaux_sub_2015_2)\n",
    "print \"taille de sub commande : \" , len(s)\n",
    "print \"taille de sub : \", len(t)\n",
    "print \"commande est il subset de sub ?\", s.issubset(t)#\ts <= t\ttest whether every element in s is in t\n",
    "print \"commande est il superset de sub ?\", s.issuperset(t)#\ts >= t\ttest whether every element in t is in s\n",
    "print \"taille de l'intersection : \", len(s.intersection(t))#\ts & t\tnew set with elements common to s and t\n",
    "print \"elements dans commande mais pas dans sub : \" , len(s.difference(t))# \ts - t\tnew set with elements in s but not in t\n",
    "print \"elements dans sub mais pas dans commande : \" , len(t.difference(s))\n",
    "print \"\\n\"\n",
    "\n",
    "liste_nouveaux_sub_commande_juin_2015_2 = df_comm_juin.ClientUserId.unique()\n",
    "liste_nouveaux_sub_juin_2015_2 = nouveaux_sub_juin.ClientUserId.unique()\n",
    "\n",
    "s=set(liste_nouveaux_sub_commande_juin_2015_2)\n",
    "t=set(liste_nouveaux_sub_juin_2015_2)\n",
    "print \"taille de sub commande : \" , len(s)\n",
    "print \"taille de sub : \", len(t)\n",
    "print \"commande est il subset de sub ?\", s.issubset(t)#\ts <= t\ttest whether every element in s is in t\n",
    "print \"commande est il superset de sub ?\", s.issuperset(t)#\ts >= t\ttest whether every element in t is in s\n",
    "print \"taille de l'intersection : \", len(s.intersection(t))#\ts & t\tnew set with elements common to s and t\n",
    "print \"elements dans commande mais pas dans sub : \" , len(s.difference(t))# \ts - t\tnew set with elements in s but not in t\n",
    "print \"elements dans sub mais pas dans commande : \" , len(t.difference(s))\n",
    "\n",
    "#s.symmetric_difference(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Préparation Kick-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble des données prod42\n",
      "min sub creation 2014-04-23 15:53:59\n",
      "max sub creation 2015-07-14 23:05:09\n",
      "min last update 2014-07-21 00:00:19\n",
      "max last update 2015-07-14 23:47:55\n",
      "\n",
      "\n",
      "sites de l'équipe pour prod42\n",
      "min sub creation 2014-04-28 09:52:00\n",
      "max sub creation 2015-07-14 23:05:09\n",
      "min last update 2014-07-21 00:00:19\n",
      "max last update 2015-07-14 23:39:33\n"
     ]
    }
   ],
   "source": [
    "dfp = pd.read_pickle(path+\"\\s_dataframe_general_vel_PROD42.pkl\")\n",
    "dfp['SubscriptionCreated'] = pd.to_datetime(dfp['SubscriptionCreated'], format='%d/%m/%Y %H:%M:%S')\n",
    "dfp['SubscriptionLastUpdated'] = pd.to_datetime(dfp['SubscriptionLastUpdated'], format='%d/%m/%Y %H:%M:%S')\n",
    "dfp['ServiceExpiry'] = pd.to_datetime(dfp['ServiceExpiry'], format='%d/%m/%Y %H:%M:%S')\n",
    "print 'ensemble des données prod42 :'\n",
    "print \"min sub creation\" ,dfp.SubscriptionCreated.min()\n",
    "print \"max sub creation\" ,dfp.SubscriptionCreated.max()\n",
    "print 'min last update',dfp.SubscriptionLastUpdated.min()\n",
    "print 'max last update' ,dfp.SubscriptionLastUpdated.max()\n",
    "print '\\n'\n",
    "print \"sites de l'équipe pour prod42 :\"\n",
    "dfp = dfp.loc[dfp['ClientUserId'].isin(liste_client)]\n",
    "print \"min sub creation\" ,dfp.SubscriptionCreated.min()\n",
    "print \"max sub creation\" ,dfp.SubscriptionCreated.max()\n",
    "print 'min last update',dfp.SubscriptionLastUpdated.min()\n",
    "print 'max last update' ,dfp.SubscriptionLastUpdated.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble des données client :\n",
      "min fileid 2013-12-03\n",
      "max fileid 2015-06-30\n",
      "\n",
      "\n",
      "sites de l'équipe pour client :\n",
      "min fileid 2013-12-03\n",
      "max fileid 2015-06-30\n"
     ]
    }
   ],
   "source": [
    "dfc = pd.read_pickle(r\"W:\\L_equipe\\s_dataframe_general_xiti_clients.pkl\")\n",
    "dfc['date'] = dfc['fileid'].str.extract(('.+_(.+)\\.csv'))\n",
    "\n",
    "print 'ensemble des données client :'\n",
    "print \"min fileid\" ,dfc.date.min()[:4]+\"-\"+dfc.date.min()[4:6]+\"-\"+dfc.date.min()[6:]\n",
    "print \"max fileid\" ,dfc.date.max()[:4]+\"-\"+dfc.date.max()[4:6]+\"-\"+dfc.date.max()[6:]\n",
    "print '\\n'\n",
    "print \"sites de l'équipe pour client :\"\n",
    "dfc = dfc.loc[dfc['id_client'].isin(liste_client)]\n",
    "print \"min fileid\" ,dfc.date.min()[:4]+\"-\"+dfc.date.min()[4:6]+\"-\"+dfc.date.min()[6:]\n",
    "print \"max fileid\" ,dfc.date.max()[:4]+\"-\"+dfc.date.max()[4:6]+\"-\"+dfc.date.max()[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les sites de l'equipe <br/>\n",
    "date de premiere visite min 2013-12-02 00:00:00<br/>\n",
    "date de premiere visite max 2015-06-30 00:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble des données commande : \n",
      "min orderdate 2015-04-07 00:08:52\n",
      "max orderdate 2015-07-05 23:56:57\n",
      "\n",
      "\n",
      "sites de l'équipe pour commande : \n",
      "min fileid 2015-04-07 00:08:52\n",
      "max fileid 2015-07-05 23:56:57\n"
     ]
    }
   ],
   "source": [
    "#dfco= pd.read_pickle(path+\"\\s_dataframe_general_vel_commande.pkl\")\n",
    "dfco['OrderDate'] = pd.to_datetime(dfco['OrderDate'], format='%d/%m/%Y %H:%M:%S')\n",
    "print 'ensemble des données commande : '\n",
    "print \"min orderdate\", dfco.OrderDate.min()\n",
    "print 'max orderdate', dfco.OrderDate.max()\n",
    "print '\\n'\n",
    "print \"sites de l'équipe pour commande : \"\n",
    "dfco = dfco.loc[dfco['ClientUserId'].isin(liste_client)]\n",
    "print \"min fileid\" ,dfco.OrderDate.min()\n",
    "print \"max fileid\" ,dfco.OrderDate.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ensemble des données pages<br/>\n",
    "date min : 2013-09-09<br/>\n",
    "date max : 2015-07-08\n",
    "\n",
    "ensembe des données sessions<br/>\n",
    "date min : 2013-09-09<br/>\n",
    "date max : 2015-06-30<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble des données souscription :\n",
      "min sub creation 2014-09-20 21:02:27\n",
      "max sub creation 2015-07-05 21:53:22\n",
      "min last update 2015-04-07 00:01:32\n",
      "max last update 2015-07-05 23:56:58\n",
      "\n",
      "\n",
      "sites de l'équipe pour souscription :\n",
      "min sub creation 2014-09-20 21:02:27\n",
      "max sub creation 2015-07-05 21:46:22\n",
      "min last update 2015-04-07 00:01:32\n",
      "max last update 2015-07-05 23:56:58\n"
     ]
    }
   ],
   "source": [
    "dfp= pd.read_pickle(path+\"\\s_dataframe_general_vel_souscription.pkl\")\n",
    "dfp['SubscriptionCreated'] = pd.to_datetime(dfp['SubscriptionCreated'], format='%d/%m/%Y %H:%M:%S')\n",
    "dfp['SubscriptionLastUpdated'] = pd.to_datetime(dfp['SubscriptionLastUpdated'], format='%d/%m/%Y %H:%M:%S')\n",
    "dfp['ServiceExpiry'] = pd.to_datetime(dfp['ServiceExpiry'], format='%d/%m/%Y %H:%M:%S')\n",
    "print 'ensemble des données souscription :'\n",
    "print \"min sub creation\" ,dfp.SubscriptionCreated.min()\n",
    "print \"max sub creation\" ,dfp.SubscriptionCreated.max()\n",
    "print 'min last update',dfp.SubscriptionLastUpdated.min()\n",
    "print 'max last update' ,dfp.SubscriptionLastUpdated.max()\n",
    "print '\\n'\n",
    "print \"sites de l'équipe pour souscription :\"\n",
    "dfp = dfp.loc[dfp['ClientUserId'].isin(liste_client)]\n",
    "print \"min sub creation\" ,dfp.SubscriptionCreated.min()\n",
    "print \"max sub creation\" ,dfp.SubscriptionCreated.max()\n",
    "print 'min last update',dfp.SubscriptionLastUpdated.min()\n",
    "print 'max last update' ,dfp.SubscriptionLastUpdated.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
